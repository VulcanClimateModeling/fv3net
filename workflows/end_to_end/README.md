# End to End Workflow

February 2020

This workflow serves to orchestrate the current set of VCM-ML workflows into one, 
allowing for going from a completed SHiELD run to a prognostic coarse run with ML
parameterization in a reasonable amount of time and with minimal active oversight.

The follwing steps are currently integrated into the workflow for use in experiments:

- coarsening restarts
- coarsening diagnostics 
- one step runs, i.e., short runs from coarsened restart files
- creating training data
- training an sklearn model
- testing an sklearn model
- a prognostic run of the coarse model using the trained sklearn parameterization

The workflow starting point is flexible, i.e., with any of the steps above, as is
its endpoint. If starting at a SHiELD run coarsened to C384, it is required that
the SHiELD C384 restart files and diagnostics are available (locally or remotely).
If starting at a later point, it is assumed that the outputs of all previous steps
are available. 


## Usage

Call the submit script from the top-level directory of the fv3net repo:


`./workflows/end_to_end/submit_workflow.sh {EXPERIMENT_CONFIG_YAML}`

The config YAML file is the focal point of using the workflow (the submission script
should not need to be modified). An indivudal experiment is run by calling the script
with a particular  configuration. Selection of steps to run, their individual 
configurations, and overall experiment parameters is done in the experiment
configuration YAML.


### Experiment configuration YAML syntax

Here is an example portion of a YAML file used to configure the workflow and run an experiment:

```
storage_proto: gs
storage_root: vcm-ml-data/orchestration-testing
experiment:
  name: test-experiment
  unique_id: True
  steps_to_run:
    - coarsen_restarts
    - coarsen_diagnostics
    - one_step_run
    - create_training_data
    - train_sklearn_model
    - test_sklearn_model
    - prognostic_run
  steps_config:
    coarsen_restarts:
      command: workflows/coarsen_restarts/orchestrator_job.sh
      inputs:
        data_to_coarsen:
          location: gs://vcm-ml-data/orchestration-testing/shield-C384-restarts-2019-12-04
      extra_args:
        source-resolution: 384
        target-resolution: 48
    one_step_run:
      command: python ./workflows/one_step_jobs/orchestrate_submit_jobs.py
      inputs:
        restart_data:
          from: coarsen_restarts
      extra_args: 
        experiment_yaml: one_step_yaml
        experiment_label: test-orchestration-group
        --n-steps: 50
```

#### Top-level arguments:

- **storage_proto**: protocol of the filesystem; can be either 'gs' or 'file'
- **storage_root**: root directory of storage for the experiment

#### Experiment level arguments:

- **name**: name of experiment
- **unique_id**: a UUID is appended to experiment name if True; if false, the `name` field is used directly. True is reccommended for safety as it will not be likely to clobber old experiments, whereas False allows for specifying an existing experiment to rerun or start from an intermediate step.
- **steps_to_run**: a YAML list of workflow steps to execute for this experiment

#### Step level arguments:

For each step in `steps_to_run`, its configuration is set by its equivalently named block in `step_config`. Step configuration can still be defined here but it will not be executed if it is excluded from `steps_to_run`. *Note:* the order of parameters correspond to the positional commandline arguments appended to the job `command`.  Parameters are translated to arguments in the order of _inputs_, _output_, _methodargs_.

- **command**: command to execute the step, e.g., a python or bash command and script that executes the step (without its arguments)
- **inputs**: a list of required input data sources from either previous steps or pre-existing sources; for each input data type, _one_ (but not both) of the following must be specified:
  - **from**: name of the previous step which produces the input data for this step, e.g., `coarsen_restarts` is an input to `one_step_run`. _Note:_ The step referenced in `from` does not have to be included in the `steps_to_run`. Whatever the output location is from that step will be used.  However, to make things explicit you should specify the input using `location` instead of `from`. 
  - **location**: explicit path to required input data for this step, which has been generated prior to the current experiment
- **output_location**: (Optional) explicit path to store output from this step.  This parameter is autogenerated if not provided and is used as the source for any input `from` refs.  
- **extra_args**: extra key-value pair paremeters required for the workflow step.  These are commonly used to reference step-specific configuration files or non-data related step parameters; each value is provided as a command-line argument appended to `command` after I/O arguments.  You can also specify python optional command-line arguments here by prefixing the optional variable with "--".  These optional arguments will be appended to the end of the command as --{param_keyname} {param_value}.
    

### Data output locations

If no `output_location` is specified for a step,  it will be output via the following structure:

```{storage_proto}://{storage_root}/{experiment_name}/{step_name}```

where `experiment_name` is the name plus the UUID (if added), and the `step_name` is defined as the name of the workflow step with the first 3 extra_args key/values appended to it.


## Creating new workflow steps

To write a new step in the workflow, create a CLI for the step that follows the following format:

```{COMMAND} {INPUT_1} {INPUT_2} ... {OUTPUT_PATH} {EXTRA_ARG_1} {EXTRA_ARG_2} {EXTRA_ARG_3} ... {--OPTIONAL_ARG} {OPTIONAL_ARG_VALUE} ...```

then add the step to the config YAML file, in both the `steps_to_run` list and the `steps_config` dict. At a minimum, the `command` and `inputs` values must be specified for the step configuration. Additionally, the step must be listed in `steps_to_run` in the order in which it is necessary, i.e., after any steps upon which it depends for input, and before any steps that depend on it for output. 