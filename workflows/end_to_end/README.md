## End to End Workflow

February 2020

This workflow serves to orchestrate the current set of VCM-ML workflows into one, 
allowing for going from a completed SHiELD run to a prognostic coarse run with ML
parameterization in a reasonable amount of time and with minimal active oversight.

The follwing steps are currently integrated into the workflow for use in experiments:

- coarsening restarts
- coarsening diagnostics 
- one step runs, i.e., short runs from coarsened restart files
- creating training data
- training an sklearn model
- testing an sklearn model
- a prognostic run of the coarse model using the trained sklearn parameterization

The workflow starting point is flexible, i.e., with any of the steps above, as is
its endpoint. If starting at a SHiELD run coarsened to C384, it is required that
the SHiELD C384 restart files and diagnostics are available (locally or remotely).
If starting at a later point, it is assumed that the outputs of all previous steps
are available. 


### Usage

Call the submit script from the top-level directory of the fv3net repo:


`./workflows/end_to_end/submit_workflow.sh {EXPERIMENT_CONFIG_YAML}`

The config YAML file is the focal point of using the workflow (the submission script
should not need to be modified). An indivudal experiment is run by calling the script
with a particular  configuration. Selection of steps to run, their individual 
configurations, and overall experiment parameters is done in the experiment
configuration YAML.


### Experiment configuration YAML syntax

Here is an example portion of a YAML file used to configure the workflow and run an experiment:

```
storage_proto: gs
storage_root: vcm-ml-data/orchestration-testing
experiment:
  name: test-experiment
  unique_id: True
  steps_to_run:
    - coarsen_restarts
    - coarsen_diagnostics
    - one_step_run
    - create_training_data
    - train_sklearn_model
    - test_sklearn_model
    - prognostic_run
  experiment_vars:
    one_step_yaml: workflows/one_step_jobs/all-physics-off.yml
  steps:
    coarsen_restarts:
      command: workflows/coarsen_restarts/orchestrator_job.sh
      inputs:
        data_to_coarsen:
          location: gs://vcm-ml-data/orchestration-testing/shield-C384-restarts-2019-12-04
      method:
        source-resolution: 384
        target-resolution: 48
    one_step_run:
      command: python ./workflows/one_step_jobs/orchestrate_submit_jobs.py
      inputs:
        restart_data:
          from: coarsen_restarts
          location: 
      method: 
        experiment_yaml: one_step_yaml
        experiment_label: test-orchestration-group
      config_transforms:
        add_unique_id: 
          - experiment_label
        use_top_level: 
          - experiment_yaml
```

#### Top-level arguments:

- storage_proto: protocol of the filesystem; can be either 'gs' or 'file'
- storage_root: root directory of storage for the experiment

#### Experiment level arguments:

- **name**: name of experiment
- **unique_id**: a UUID is appended to experiment name if True; if false, the `name` field is used directly. True is reccommended for safety as it will not be likely to clobber old experiments, whereas False allows for specifying an existing experiment to rerun or start from an intermediate step.
- **steps_to_run**: a YAML list of workflow steps to execute for this experiment
- **experiment_vars**: a parameter that applies to more than one step in the experiment can be set here; see `use_top_level` under step configuration for details

#### Step level arguments:

For each step in `steps_to_run`, its configuration is set by its equivalently named block in `step_config`. Step configuration can still be defined here but it will not be executed if it is excluded from `steps_to_run`. *Note:* the order of parameters correspond to the positional commandline arguments appended to the job `command`.  Parameters are translated to arguments in the order of _inputs_, _output_, _methodargs_.

- **command**: command to execute the step, e.g., a python or bash command and script that executes the step (without its arguments)
- **inputs**: a list of input data types required by the step; for each input data type, _one_ (but not both) of the following must be specified:
    - **from**: name of the previous step which produces the input data for this step, e.g., `coarsen_restarts` is an input to `one_step_run`. _Note:_ The step referenced in `from` does not have to be included in the `steps_to_run`. Whatever the output location is from that step will be used.  However, to make things explicit you should specify the input using `location` instead of `from`. 
    - **location**: explicit path to required input data for this step, which has been generated prior to the current experiment
- **output_location**: (Optional) explicit path to store output from this step.  This parameter is autogenerated if not provided and is used as the source for any input `from` refs.  
- **method**: configurable key-value pairs specific to the workflow step; each value is provided as a command-line argument appended to `command` after I/O arguments.
- **config_transforms**: additional parameter value translation functionality
    - **add_unique_id**: a list of step method keys for which the experiment-level UUID should be appended to the method value; useful for appending the experiment UUID to the `experiment label` tag appended to Kubernetes jobs
    - **use_top_level**: a list of step method keys for which the method value should be remapped using `experiment_vars` defined above; this allows for setting multiple steps' methods to the same value, e.g., using the same one-step YAML config file for both the one-step and prognostic jobs.
    

### Data output locations

If no `output_location` is specified for a step,  it will be output via the following structure:

```{storage_proto}://{storage_root}/{experiment_name}/{step_name}```

where `experiment_name` is the name plus the UUID (if added), and the `step_name` is defined as the name of the workflow step with the first 3 method key/values appended to it. 