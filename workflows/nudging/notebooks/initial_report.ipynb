{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start off by importing the libraries we need and loading two types of data - snapshots of model state at the end of the timestep (including from the coarsened high-res data), and tendencies. Nudging tendencies are output directly by the runfiles while the model is running. Physics tendencies are derived by us here from snapshots before and after physics is called.\n",
    "\n",
    "**Note: Data can take a long time to load (~tens of minutes)**\n",
    "Snapshots are mostly lazy-loaded, but contain many files, so the initial load can also take some time. Grid variables are eagerly loaded and take a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yaml\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import zarr\n",
    "import vcm\n",
    "import os\n",
    "import fv3util\n",
    "import fv3viz as viz\n",
    "import cartopy.crs\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = yaml.safe_load(open('../fv3config_base.yml', 'r'))\n",
    "DATA_DIR_TEMPLATE = \"gs://vcm-ml-data/mcgibbon-testing/outdir-{}h\"\n",
    "TIMESCALES = [1, 3, 6, 12, 24]\n",
    "# stages = ['before_dynamics', 'after_dynamics', 'after_physics', 'after_nudging', 'nudging_tendencies', 'reference']\n",
    "MODEL_STAGE = \"after_nudging\"\n",
    "GRID_FILENAME_TEMPLATE = os.path.join(DATA_DIR_TEMPLATE.format(12), 'atmos_8xdaily.tile{}.nc')\n",
    "RESTARTS_PATH = config['nudging']['restarts_path']\n",
    "IO_LAYOUT = (1, 1)  # probably only runs for (1, 1)\n",
    "PLOT_TRANSFORM = cartopy.crs.PlateCarree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load snapshot data\n",
    "\n",
    "def get_timescale_label(timescale):\n",
    "    if isinstance(timescale, int):\n",
    "        return f\"nudge-{timescale:02d}h\"\n",
    "    else:\n",
    "        return f\"nudge-{timescale:05.2f}\"\n",
    "\n",
    "snapshot = {}\n",
    "fs, _, _ = fsspec.core.get_fs_token_paths(DATA_DIR_TEMPLATE)\n",
    "for timescale in TIMESCALES:\n",
    "    zarr_location = os.path.join(DATA_DIR_TEMPLATE.format(timescale), f\"{MODEL_STAGE}.zarr\")\n",
    "    if not fs.isdir(zarr_location):\n",
    "        raise ValueError(f\"location {zarr_location} does not exist\")\n",
    "    snapshot[get_timescale_label(timescale)] = xr.open_zarr(zarr.storage.LRUStoreCache(fs.get_mapper(zarr_location), max_size=None))\n",
    "zarr_location = os.path.join(DATA_DIR_TEMPLATE.format(timescale), f\"reference.zarr\")\n",
    "if not fs.isdir(zarr_location):\n",
    "    raise ValueError(f\"location {zarr_location} does not exist\")\n",
    "snapshot[\"reference\"] = xr.open_zarr(zarr.storage.LRUStoreCache(fs.get_mapper(zarr_location), max_size=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load physics and nudging tendencies\n",
    "\n",
    "NUDGING_SUFFIX = \"_tendency_due_to_nudging\"\n",
    "TIMESTEP = timedelta(minutes=15)\n",
    "nudging_tendencies = {}\n",
    "physics_tendencies = {}\n",
    "for timescale in TIMESCALES:\n",
    "    nudging = xr.open_zarr(\n",
    "        zarr.storage.LRUStoreCache(\n",
    "            fs.get_mapper(\n",
    "                os.path.join(DATA_DIR_TEMPLATE.format(timescale), \"nudging_tendencies.zarr\")\n",
    "            ),\n",
    "            max_size=None\n",
    "        )\n",
    "    )\n",
    "    rename_dict = {}\n",
    "    for varname in nudging:\n",
    "        if varname.endswith(NUDGING_SUFFIX):\n",
    "            rename_dict[varname] = varname[:-len(NUDGING_SUFFIX)]\n",
    "    nudging = nudging.rename(rename_dict)\n",
    "    nudging_tendencies[get_timescale_label(timescale)] = nudging\n",
    "    \n",
    "    before_physics = xr.open_zarr(\n",
    "        zarr.storage.LRUStoreCache(\n",
    "            fs.get_mapper(\n",
    "                os.path.join(DATA_DIR_TEMPLATE.format(timescale), \"after_dynamics.zarr\")\n",
    "            ),\n",
    "            max_size=None\n",
    "        )\n",
    "    )\n",
    "    after_physics = xr.open_zarr(\n",
    "        zarr.storage.LRUStoreCache(\n",
    "            fs.get_mapper(\n",
    "                os.path.join(DATA_DIR_TEMPLATE.format(timescale), \"after_physics.zarr\")\n",
    "            ),\n",
    "            max_size=None\n",
    "        )\n",
    "    )\n",
    "    physics = xr.Dataset()\n",
    "    for varname in nudging:\n",
    "        if varname != 'time':\n",
    "            physics[varname] = (after_physics[varname] - before_physics[varname]) / TIMESTEP.total_seconds()\n",
    "    physics_tendencies[get_timescale_label(timescale)] = physics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid variables (latb in particular) are currently hard to acquire from the running model,\n",
    "# but easy to grab from the diagnostic files\n",
    "\n",
    "def load_grid():\n",
    "    ntime, ntile, nz, ny, nx = snapshot['reference']['air_temperature'].shape\n",
    "    lat = np.empty([ntile, ny, nx])\n",
    "    latb = np.empty([ntile, ny+1, nx+1])\n",
    "    lon = np.empty([ntile, ny, nx])\n",
    "    lonb = np.empty([ntile, ny+1, nx+1])\n",
    "    for tile in range(6):\n",
    "        grid_filename = GRID_FILENAME_TEMPLATE.format(tile+1)\n",
    "        with fsspec.open(grid_filename, mode='rb') as f:\n",
    "            ds = xr.open_dataset(f)\n",
    "            for name in ('lat', 'latb', 'lon', 'lonb'):\n",
    "                locals()[name][tile, :, :] = ds[name].values\n",
    "    return lat, lon, latb, lonb\n",
    "\n",
    "lat, lon, latb, lonb = load_grid()\n",
    "p = np.cumsum(np.mean(snapshot['reference']['pressure_thickness_of_atmospheric_layer'][0, :, :, :, :].values, axis=(0, 2, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at maps of 500hPa vertical wind, we see a lot of tropical grid-scale convection which gets smoothed out/removed by the coarse model. Frontal regions are also significantly smoothed. Frontal convection is not clearly decreased overall - inspecting the difference map generally shows dipole (shifting) and tripole (damping/spreading) patterns. Regions of convection and subsidence are generally in the right areas, even at fairly long damping timescales (12h) after about 3 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can tweak these variables to look at different timesteps, vertical levels, and variables.\n",
    "\n",
    "i_time = 5*24*4  # days * 24 hours * 4 timesteps per hour\n",
    "p_target_hpa = 500\n",
    "varname = \"vertical_wind\"\n",
    "cmap_base = \"viridis\"\n",
    "vmin_base = -0.1  # vmin and vmax for variable itself\n",
    "vmax_base = 0.1\n",
    "cmap_error = \"RdBu\"\n",
    "vmin_error = -0.1  # vmin/vmax for error with respect to reference\n",
    "vmax_error = 0.1\n",
    "\n",
    "\n",
    "compare_label = get_timescale_label(1)\n",
    "iz = np.argmin(np.abs(p - p_target_hpa*100))\n",
    "figsize=(20, 25)\n",
    "\n",
    "fig, ax = plt.subplots(len(snapshot.keys()), 2, subplot_kw={'projection': PLOT_TRANSFORM}, figsize=figsize)\n",
    "\n",
    "fig.suptitle(varname)\n",
    "\n",
    "for i, key in enumerate(sorted(list(snapshot.keys()))):\n",
    "    im = viz.pcolormesh_cube(\n",
    "        latb, lonb, snapshot[key][varname][i_time, :, iz].values, ax=ax[i, 0], vmin=vmin_base, vmax=vmax_base, cmap=cmap_base\n",
    "    )\n",
    "    ax[i, 0].set_title(key)\n",
    "    ax[i, 0].set_global()\n",
    "    ax[i, 0].coastlines()\n",
    "    plt.colorbar(im, ax=ax[i, 0])\n",
    "    im = viz.pcolormesh_cube(\n",
    "        latb, lonb, snapshot[key][varname][i_time, :, iz].values - snapshot['reference'][varname][i_time, :, iz].values, ax=ax[i, 1], vmin=vmin_error, vmax=vmax_error, cmap=cmap_error\n",
    "    )\n",
    "    ax[i, 1].set_title(f\"{key} - reference\")\n",
    "    ax[i, 1].set_global()\n",
    "    ax[i, 1].coastlines()\n",
    "    plt.colorbar(im, ax=ax[i, 1])\n",
    "\n",
    "plt.tight_layout(rect=(0, 0, 1, 0.95))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison to 500 hPa vertical wind, 850 hPa temperature errors are much more sensitive to the strength of nudging. This makes a little sense because air temperature is directly nudged, while vertical wind is affected by nudging horizontal winds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can tweak these variables to look at different timesteps, vertical levels, and variables.\n",
    "\n",
    "i_time = 5*24*4  # days * 24 hours * 4 timesteps per hour\n",
    "p_target_hpa = 850\n",
    "varname = \"air_temperature\"\n",
    "cmap_base = \"viridis\"\n",
    "vmin_base = 220  # vmin and vmax for variable itself\n",
    "vmax_base = 300\n",
    "cmap_error = \"RdBu\"\n",
    "vmin_error = -3  # vmin/vmax for error with respect to reference\n",
    "vmax_error = 3\n",
    "\n",
    "\n",
    "compare_label = get_timescale_label(1)\n",
    "iz = np.argmin(np.abs(p - p_target_hpa*100))\n",
    "figsize=(20, 25)\n",
    "\n",
    "fig, ax = plt.subplots(len(snapshot.keys()), 2, subplot_kw={'projection': PLOT_TRANSFORM}, figsize=figsize)\n",
    "\n",
    "fig.suptitle(varname)\n",
    "\n",
    "for i, key in enumerate(sorted(list(snapshot.keys()))):\n",
    "    im = viz.pcolormesh_cube(\n",
    "        latb, lonb, snapshot[key][varname][i_time, :, iz].values, ax=ax[i, 0], vmin=vmin_base, vmax=vmax_base, cmap=cmap_base\n",
    "    )\n",
    "    ax[i, 0].set_title(key)\n",
    "    ax[i, 0].set_global()\n",
    "    ax[i, 0].coastlines()\n",
    "    plt.colorbar(im, ax=ax[i, 0])\n",
    "    im = vcm.pcolormesh_cube(\n",
    "        latb, lonb, snapshot[key][varname][i_time, :, iz].values - snapshot['reference'][varname][i_time, :, iz].values, ax=ax[i, 1], vmin=vmin_error, vmax=vmax_error, cmap=cmap_error\n",
    "    )\n",
    "    ax[i, 1].set_title(f\"{key} - reference\")\n",
    "    ax[i, 1].set_global()\n",
    "    ax[i, 1].coastlines()\n",
    "    plt.colorbar(im, ax=ax[i, 1])\n",
    "\n",
    "plt.tight_layout(rect=(0, 0, 1, 0.95))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histogram of vertical velocity behaves very similarly to in [Noah's analysis of SAM](https://github.com/VulcanClimateModeling/uwnet/blob/0b167d4baff0d497df1ea2986a713c081ed90399/2020-02-20-w500-with-nudging.ipynb), except that downdrafts appear to be better represented. Particularly, we see lower occurence of very high vertical velocities, as a result of convective peaks being smeared out at coarser resolution.\n",
    "\n",
    "Unlike in Noah's analysis, the strongest nudging timescale shows the lowest occurrence of peak vertical winds. The 1h timescale does have the highest standard deviation of 500hpa vertical wind, but only by a small margin, much smaller than in SAM.\n",
    "\n",
    "I would take from this that the coarse model dynamics do not want to support the same features as the high-resolution model. If we want to reproduce these features in the coarse model, it may be worth integrating machine learning physics routines in-line in the dynamics, to represent the impact of subgrid-scale dynamics in producing grid-scale updrafts. These features appear to be very learn-able, occuring within existing updraft features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "i_window = 4*24\n",
    "\n",
    "def hist(x):\n",
    "    w_bins = np.arange(-20,40) * .005\n",
    "    counts = np.histogram(x, w_bins)[0]\n",
    "    return pd.Series(counts/counts.sum(), index=w_bins[:-1])\n",
    "\n",
    "df = pd.DataFrame({key: hist(snapshot[key][varname][i_time-i_window:i_time+i_window, :, iz, :, :].values) for key in snapshot.keys()})\n",
    "df.plot(logy=True)\n",
    "\n",
    "plt.ylabel('pdf')\n",
    "plt.xlabel(f'{varname}')\n",
    "\n",
    "plt.figure()\n",
    "pd.Series({key: snapshot[key][varname][i_time-i_window:i_time+i_window, :, iz, :, :].values.std() for key in snapshot.keys()}).plot(kind=\"bar\")\n",
    "plt.ylabel(f\"{p_target_hpa} hPa {varname} std\")\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "pd.Series({key: (\n",
    "    snapshot[key][varname][i_time-i_window:i_time+i_window, :, iz, :, :].values -\n",
    "    snapshot[\"reference\"][varname][i_time-i_window:i_time+i_window, :, iz, :, :].values\n",
    ").std() for key in snapshot.keys()}).plot(kind=\"bar\")\n",
    "plt.ylabel(f\"{p_target_hpa} hPa {varname} rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evolution of error shows performance mostly settles after as little as 1 day for timescales less than 12 hours. We could safely assume the model has spun up after 2 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 2))\n",
    "it_max = 5*24*4\n",
    "varname = \"air_temperature\"\n",
    "p_target_hpa = 850\n",
    "iz = np.argmin(np.abs(p - p_target_hpa*100))\n",
    "for key in sorted(list(snapshot.keys())):\n",
    "    if key != 'reference':\n",
    "        rmse = np.std(snapshot[key][varname][:it_max, :, iz, :, :].values - snapshot['reference'][varname][:it_max, :, iz, :, :].values, axis=(1, 2, 3))\n",
    "        ax.plot(snapshot['nudge-01h']['time'][:it_max], rmse, label=key)\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_title(f'rmse of {varname} at {p_target_hpa} hPa')\n",
    "ax.set_ylim(0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 2))\n",
    "it_max = 5*24*4\n",
    "varname = \"vertical_wind\"\n",
    "p_target_hpa = 500\n",
    "iz = np.argmin(np.abs(p - p_target_hpa*100))\n",
    "for key in sorted(list(snapshot.keys())):\n",
    "    if key != 'reference':\n",
    "        rmse = np.std(snapshot[key][varname][:it_max, :, iz, :, :].values - snapshot['reference'][varname][:it_max, :, iz, :, :].values, axis=(1, 2, 3))\n",
    "        ax.plot(snapshot['nudge-01h']['time'][:it_max], rmse, label=key)\n",
    "ax.legend(loc=\"lower right\")\n",
    "ax.set_title(f'rmse of {varname} at {p_target_hpa} hPa')\n",
    "ax.set_ylim(0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing maps of physics tendency to nudging tendency, the nudging tendency has smaller magnitudes for longer timescales and comparable magnitudes for shorter timescales. At longer timescales, the nudging field is much smoother than the physics tendency, reflecting the fact that it amounts to an integration of the error in the physics tendency over a longer period. At short nudging timescales (especially 1h), we see small-scale features appear in the nudging tendency. The 3-hour nudging timescale seems to have a decent balance between complexity and performance. I would suggest trying a range of nudging timescales and see how their use affects the final prognostic performance of the trained model, evaluated against the coarsened high-resolution data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i_time = 5*24*4\n",
    "p_target_hpa = 500\n",
    "compare_label = get_timescale_label(1)\n",
    "iz = np.argmin(np.abs(p - p_target_hpa*100))\n",
    "figsize=(24, 30)\n",
    "varname = \"air_temperature\"\n",
    "vmin=-1.5e-4\n",
    "vmax=1.5e-4\n",
    "\n",
    "lat_target = -35\n",
    "\n",
    "fig, ax = plt.subplots(len(physics_tendencies.keys()), 3, subplot_kw={'projection': PLOT_TRANSFORM}, figsize=figsize)\n",
    "\n",
    "fig.suptitle(varname)\n",
    "\n",
    "for i, key in enumerate(sorted(list(physics_tendencies.keys()))):\n",
    "    im = viz.pcolormesh_cube(\n",
    "        latb, lonb, physics_tendencies[key][varname][i_time, :, iz].values, ax=ax[i, 0], vmin=vmin, vmax=vmax, cmap=\"RdBu\"\n",
    "    )\n",
    "    ax[i, 0].set_title(f\"physics tendency of {key} (units/s)\")\n",
    "    ax[i, 0].set_global()\n",
    "    ax[i, 0].coastlines()\n",
    "    #plt.colorbar(im, ax=ax[i, 0])\n",
    "    im = viz.pcolormesh_cube(\n",
    "        latb, lonb, nudging_tendencies[key][varname][i_time, :, iz].values, ax=ax[i, 1], vmin=vmin, vmax=vmax, cmap=\"RdBu\"\n",
    "    )\n",
    "    ax[i, 1].set_title(f\"nudging tendency of {key} (units/s)\")\n",
    "    ax[i, 1].set_global()\n",
    "    ax[i, 1].coastlines()\n",
    "    #plt.colorbar(im, ax=ax[i, 1])\n",
    "\n",
    "    im = viz.pcolormesh_cube(\n",
    "        latb, lonb, physics_tendencies[key][varname][i_time, :, iz].values + nudging_tendencies[key][varname][i_time, :, iz].values, ax=ax[i, 2], vmin=vmin, vmax=vmax, cmap=\"RdBu\"\n",
    "    )\n",
    "    ax[i, 2].set_title(f\"physics + nudging tendency of {key} (units/s)\")\n",
    "    ax[i, 2].set_global()\n",
    "    ax[i, 2].coastlines()\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
