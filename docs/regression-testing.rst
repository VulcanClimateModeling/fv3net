Regression Testing
==================

The package ``synth`` provides tools for generating synthetic xarray datasets for testing purposes.
Compared to real data, it is much easier and cheaper to manage code for generating synthetic 
using software version control tools like git.


Usage
-----

This packages allows the user to build up a "schema" describing their data. 
A schema is a reduced description of a zarr/xarray dataset that can

1. be easily serialized to disk and loaded again
2. used to generate a random dataset
3. validate an existing dataset

This package defines a set of dataclasses defining a schema. It also contains fixtures
for datasets used by VCM-ML in training models, allowing for unit and regression testing
of data pipelines. 


The function :func:`synth.read_schema_from_zarr` can be used to generate a schema 
from an existing dataset. For example, suppose we have loaded a zarr group like this::

    import sys
    import fsspec
    import zarr
    import synth

    url = "gs://path/to/data.zarr"
    mapper = fsspec.get_mapper(url)
    group = zarr.open_group(mapper)

This data could comprise many gigabytes, so it is unweildy to manage, and use
within a testing framework. To generate a condensed description, you can
generate a reduced "schema" like this::

    schema = synth.read_schema_from_zarr(
        group, coords=("time", "tile", "grid_xt", "grid_yt")
    )

Next, ``schema`` can be serialized to json and dumped to disk like
this::

    with open("schema.json", "w") as f:
        synth.dump(schema, f)

The json file includes a "version" entry to ensure that schema's generated by
older versions of this code remain legible to new versions. 

.. note::

    While old schema may be read by newer versions of the code, there is no
    guarantee that ``generate`` will return the same values, so checks of
    bit-for-bit reproducibilty may need to be updated.


The json file can
then be checked into version control and loaded inside of a
test script like this::

    with open("schema.json" , "r") as f:
        schema = synth.load(f)
    
A fake xarray dataset can be created::

    ds = synth.generate(schema)

The data for each chunk will be identically generated from a uniform distribution.
The upper and lower bounds of this distribution are set by default in the code, but can 
be overrided using the ``ranges`` argument of ``synth.generate``.

A dataset can be validated by checking its schema against a reference schema::

    with open("schema.json" , "r") as f:
        ref_schema = synth.load(f)

    zarr_schema = synth.read_schema_from_zarr(
        group, coords=("time", "tile", "grid_xt", "grid_yt")
    )
    
    assert zarr_schema == ref_schema

Note that the equality operator ``==`` checks the following:

1. the dataset has the same coordinates in terms of names and dimension names
2. the dataset has the same variables in terms of names, dimension names, and data shape, type, and chunks.

Note that coordinate shapes and values and dataset, coordinate, and variable attributes 
are not compared. The equality operator can also be used directly on coordinate and 
variable schema. 


Marking pytest functions as regression tests
--------------------------------------------

Regression tests are identified using `pytest.mark.regression`. These will
only be triggered if the fv3net unit tests succeed.

Examples
--------

The "create training data" workflow currently is tested using this framework.
See test `here <https://github.com/VulcanClimateModeling/fv3net/blob/be447a44725d7fb766bbe35685862246f06f37f9/tests/create_training_data/test_integration.py#L1>`_.


Pytest dataset fixtures
-----------------------

The package contains fixtures for three datasets used for ML training in fv3net: one-step
tendenciecs, nudging tendencies, and fine-res apparent sources. The fixtures in 
``synth._dataset_fixtures`` may be imported into a ``conftest.py`` file and then
used in testing mappers and other functions that load or use these data::
    from synth import (
        test_training_datadir,
        data_source_name,
        one_step_dataset_path,
        nudging_dataset_path,
        fine_res_dataset_path,
        data_source_path,
        grid_dataset,
    )

Fixtures exist for each invidiual dataset (e.g, ``one_step_dataset_path``), returning its path in a
temporary testing directory, and for a parametrized fixture (``data_source_path``) that will
sequentially return the paths of all three datasets.


Existing tools
--------------

Python has some `rich tools <https://faker.readthedocs.io/en/master/>`_ for
generating fake data, but nothing specialized to xarray/zarr.


