version: 2.1
orbs:
  gcp-gcr: circleci/gcp-gcr@0.15.0
  gh: circleci/github-cli@2.1.0
  jq: circleci/jq@2.2.0
jobs:
  pytest:
    parameters:
      kind:
        description: "Type of test"
        default: "test_unit"
        type: enum
        enum: ["test_unit", "test_regression", "typecheck", "test_dataflow", "test_prognostic_run_report"]
    docker:
      - image: us.gcr.io/vcm-ml/circleci-miniconda3-gfortran:latest
        auth:
          username: _json_key
          password: $DECODED_GOOGLE_CREDENTIALS
        environment:
          GOOGLE_APPLICATION_CREDENTIALS: /etc/key.json
    steps:
      - checkout
      - gh/setup
      - jq/install
      - add_ssh_keys:
          fingerprints:
            - "28:8a:c3:e4:58:e7:c0:11:2c:fc:62:84:85:91:1f:7d" # github user key
      - run:
          name: "Setup google cloud credentials"
          command: |
            # Setup credentials
            echo $ENCODED_GOOGLE_CREDENTIALS | \
            base64 --decode > $GOOGLE_APPLICATION_CREDENTIALS
      - run: make update_submodules
      - run:
          name: "Concatenate package dependency files"
          command: cat conda-linux-64.lock constraints.txt > combined_deps.txt
      - restore_cache:
          keys:
            # change this string to invalidate cache
            - v2-fv3net-env-{{ checksum "combined_deps.txt" }}
      - run:
          name: "Install fv3net dependencies"
          command: |
            make install_deps
      - save_cache:
          key: v2-fv3net-env-{{ checksum "combined_deps.txt" }}
          paths:
            - /opt/conda/envs/fv3net
      - run:
          name: "Install fv3net packages"
          command: |
            make install_local_packages
            source activate fv3net && pip freeze
      - run:
          name: "Run test"
          no_output_timeout: 20m
          command: |
            source activate fv3net
            make <<parameters.kind>>
      - run:
          name: "Coverage Report"
          command: |
            # initialize output directory
            mkdir -p /tmp/coverage

            source activate fv3net

            set +e
            make coverage_report

            make htmlcov
            .circleci/update_coverage_description.sh <<parameters.kind>>
            mv htmlcov /tmp/coverage/htmlcov-<<parameters.kind>>
            set -e
            exit 0
      - store_artifacts:
          path: /tmp/coverage/
  argo:
    machine:
      image: ubuntu-2004:202111-02
    steps:
      - checkout
      - run: |
            sudo apt-get update && sudo apt-get install -y apt-transport-https gnupg2
            curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
            echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
            sudo apt-get update
            sudo apt-get install -y kubectl
      - run: sudo apt-get install make
      - run: make test_argo

  build_default:
    parameters:
      image:
        default: fv3net
        type: enum
        enum:
          - prognostic_run
          - prognostic_run_gpu
          - post_process_run
          - fv3net
          - fv3fit
          - fv3fit_torch
          - dataflow
          - artifacts
    machine:
      image: ubuntu-2004:202111-02
    environment:
      GOOGLE_PROJECT_ID: vcm-ml
      GOOGLE_COMPUTE_ZONE: us-central1
      GOOGLE_APPLICATION_CREDENTIALS: /tmp/key.json
      IMAGE: <<parameters.image>>
    steps:
      - checkout
      - run: |
          sudo apt-get update && sudo apt-get install -y make jq \
            python3 \
            python3-pip \
            python3-numpy # needed by pip-compile to handle vcm/setup.py
      - run: sudo pip3 install -c constraints.txt pip-tools
      - add_ssh_keys:
          fingerprints:
            - "28:8a:c3:e4:58:e7:c0:11:2c:fc:62:84:85:91:1f:7d" # github user key
      - run: make update_submodules
      - run:
          name: "gcloud auth"
          command: |
            echo $ENCODED_GOOGLE_CREDENTIALS | base64 -d > $GOOGLE_APPLICATION_CREDENTIALS
            echo "export GCLOUD_SERVICE_KEY=\$(echo \$ENCODED_GOOGLE_CREDENTIALS | base64 --decode)" >> $BASH_ENV
      - gcp-gcr/gcr-auth
      - run: |
          sudo chown -R circleci:circleci /home/circleci/.docker && \
          .circleci/build_and_push_image.sh
  lint:
    docker:
      - image: circleci/python:3.8
    steps:
      - checkout
      - run: sudo pip install -c constraints.txt pre-commit xarray
      - run: make lint

  integration_tests:
    docker:
      - image: google/cloud-sdk
    environment:
      GOOGLE_APPLICATION_CREDENTIALS: /tmp/key.json
      CLUSTER_NAME: ml-cluster-dev
      USE_GKE_GCLOUD_AUTH_PLUGIN: True
    steps:
      - checkout
      - run:
          name: Install dependencies
          command: |
            apt-get install -y wget jq make gettext gzip
            pip3 install yq==2.11.0

            # install kustomize
            wget "https://raw.githubusercontent.com/\
            kubernetes-sigs/kustomize/master/hack/install_kustomize.sh"
            chmod +x install_kustomize.sh
            ./install_kustomize.sh  3.8.6
            mv kustomize /usr/local/bin/

            # Install argo
            curl -sLO https://github.com/argoproj/argo-workflows/releases/download/v3.3.7/argo-linux-amd64.gz
            gunzip argo-linux-amd64.gz
            chmod +x argo-linux-amd64
            mv ./argo-linux-amd64 /usr/local/bin/argo
      - run:
          name: Setup firewall rule
          command: |
            ci_public_ip=$(wget -qO- http://ipecho.net/plain | xargs echo)
            echo $ENCODED_GOOGLE_CREDENTIALS | base64 -d > $GOOGLE_APPLICATION_CREDENTIALS
            gcloud config set project $K8S_PROXY_PROJECT
            gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS
            proxy_ip=$(gcloud compute instances list --filter="labels.role=$K8S_PROXY_TARGET_TAG" \
                --filter="labels.cluster=$CLUSTER_NAME" \
                --format="value(networkInterfaces.accessConfigs[0].natIP)")
            firewall_name="ci-to-gke-proxy-"$(openssl rand -hex 5)
            echo 'export firewall_name='$firewall_name >> $BASH_ENV
            echo 'export proxy_ip='$proxy_ip >> $BASH_ENV
            gcloud compute firewall-rules create $firewall_name \
                --allow tcp:$K8S_PROXY_PORT \
                --network $K8S_PROXY_NETWORK --direction IN \
                --source-ranges $ci_public_ip
      - run:
          name: Setup kubectl
          command: |
            gcloud container clusters get-credentials --zone us-central1-c  $CLUSTER_NAME
            kube_cluster_name=$(kubectl config get-clusters | grep $CLUSTER_NAME)
            kubectl config set-cluster $kube_cluster_name --server="https://"$proxy_ip
            echo $K8S_PROXY_ENCODED_CA_CRT | base64 -d > proxy.crt
            kubectl config set-cluster $kube_cluster_name --certificate-authority="./proxy.crt"
      - run:
          name: Submit integration tests
          command: |
            make run_integration_tests VERSION=$CIRCLE_SHA1
      - run:
          name: Firewall rule deletion
          when: always
          command: gcloud compute firewall-rules delete $firewall_name

  validate_radiation_port:
    machine:
      image: ubuntu-2004:202111-02
    environment:
      GOOGLE_PROJECT_ID: vcm-ml
      GOOGLE_APPLICATION_CREDENTIALS: /tmp/key.json
    steps:
      - checkout
      - run:
          name: "gcloud auth"
          command: echo $ENCODED_GOOGLE_CREDENTIALS | base64 -d > $GOOGLE_APPLICATION_CREDENTIALS
      - run:
          name: "Install nix and cachix"
          command: |
            sh <(curl -L https://nixos.org/nix/install) --no-daemon
            . /home/circleci/.nix-profile/etc/profile.d/nix.sh
            nix-env -iA cachix -f https://cachix.org/api/v1/install
            cachix use vulcanclimatemodeling
      - run:
          name: "Get data"
          command: |
            cd external/radiation
            . /home/circleci/.nix-profile/etc/profile.d/nix.sh
            nix-shell --command "gcloud auth activate-service-account --key-file=$GOOGLE_APPLICATION_CREDENTIALS"
            nix-shell --command "./get_data.sh"
      - run:
          name: "Validate radiation port"
          command: |
            cd external/radiation
            . /home/circleci/.nix-profile/etc/profile.d/nix.sh
            nix-shell --command "pytest -s tests/test_driver"
  build_and_test_scream:
    parameters:
      image:
        default: prognostic_scream_run
        type: enum
        enum:
          - prognostic_scream_run
    machine:
      image: ubuntu-2004:202111-02
    resource_class: large
    environment:
      GOOGLE_PROJECT_ID: vcm-ml
      GOOGLE_APPLICATION_CREDENTIALS: /tmp/key.json
      GOOGLE_COMPUTE_ZONE: us-central1
    steps:
      - checkout
      - run:
          name: "gcloud auth"
          command: |
            echo $ENCODED_GOOGLE_CREDENTIALS | base64 -d > $GOOGLE_APPLICATION_CREDENTIALS
            echo "export GCLOUD_SERVICE_KEY=\$(echo \$ENCODED_GOOGLE_CREDENTIALS | base64 --decode)" >> $BASH_ENV
      - gcp-gcr/gcr-auth
      - run:
          name: "Build and push scream image"
          no_output_timeout: 20m
          command: |
                sudo chown -R circleci:circleci /home/circleci/.docker && \
                .circleci/build_and_push_image.sh
parameters:
  run-weekly-workflow:
    type: boolean
    default: false

workflows:
  version: 2
  test_and_lint:
    when:
      not: << pipeline.parameters.run-weekly-workflow >>
    jobs:
      - lint
      - argo
      - pytest:
          context:
            - GITHUB_CREDENTIALS
          name: pytest-<<matrix.kind>>
          matrix:
            parameters:
              kind: ["test_unit", "test_regression", "test_prognostic_run_report"]
      - hold:
          type: approval
          filters:
            branches:
              ignore:
                - master
      # manually trigger on PRs
      - build_default:
          name: "Test Dataflow (held)"
          image: dataflow
          requires:
            - hold
          filters:
            tags:
              ignore: /^v.*/
            branches:
              ignore: master
      # automatically trigger on master
      - build_default:
          name: "Test Dataflow (not held)"
          image: dataflow
          filters:
            tags:
              only: /^v.*/
            branches:
              only: master
      - build_default:
          matrix:
            parameters:
              image:
                - prognostic_run
                - prognostic_run_gpu
                - post_process_run
                - fv3net
                - artifacts
      - build_default:
          name: "build/push fv3fit image"
          image: fv3fit
          requires:
            - pytest
      - build_default:
          name: "build/push fv3fit_torch image"
          image: fv3fit_torch
          requires:
            - pytest
      # manually trigger on PRs
      - integration_tests:
          requires:
            - build_default
            - hold
          filters:
            tags:
              ignore: /^v.*/
            branches:
              ignore: master
      # automatically trigger on master
      - integration_tests:
          requires:
            - build_default
          filters:
            tags:
              only: /^v.*/
            branches:
              only: master
      - validate_radiation_port
  weekly-scream-workflow:
    when: << pipeline.parameters.run-weekly-workflow >>
    jobs:
      - build_and_test_scream
