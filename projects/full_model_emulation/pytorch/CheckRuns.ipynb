{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xarray as xr\n",
    "random = np.random.RandomState(seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomState(MT19937) at 0x7FE9069F4640"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample, n_tile, nx, ny, n_feature = 5, 6, 12, 12, 2\n",
    "size=(n_sample, n_tile, nx, ny, n_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block !important;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray (sample: 5, tile: 6, x: 12, y: 12, z: 2)&gt;\n",
       "array([[[[[6.45894113e+00, 4.37587211e+00],\n",
       "          [8.91773001e+00, 9.63662761e+00],\n",
       "          [3.83441519e+00, 7.91725038e+00],\n",
       "          ...,\n",
       "          [7.80529176e+00, 1.18274426e+00],\n",
       "          [6.39921021e+00, 1.43353287e+00],\n",
       "          [9.44668917e+00, 5.21848322e+00]],\n",
       "\n",
       "         [[4.14661940e+00, 2.64555612e+00],\n",
       "          [7.74233689e+00, 4.56150332e+00],\n",
       "          [5.68433949e+00, 1.87898004e-01],\n",
       "          ...,\n",
       "          [1.28926298e+00, 3.15428351e+00],\n",
       "          [3.63710771e+00, 5.70196770e+00],\n",
       "          [4.38601513e+00, 9.88373838e+00]],\n",
       "\n",
       "         [[1.02044811e+00, 2.08876756e+00],\n",
       "          [1.61309518e+00, 6.53108325e+00],\n",
       "          [2.53291603e+00, 4.66310773e+00],\n",
       "          ...,\n",
       "...\n",
       "          ...,\n",
       "          [1.46508191e+00, 1.36721807e+00],\n",
       "          [8.85839863e+00, 1.42671272e-01],\n",
       "          [6.79763879e+00, 8.04785339e+00]],\n",
       "\n",
       "         [[7.17913038e+00, 1.44409263e+00],\n",
       "          [6.40503996e+00, 1.23486772e+00],\n",
       "          [4.62247278e+00, 6.54832523e+00],\n",
       "          ...,\n",
       "          [6.90017764e+00, 4.70643618e+00],\n",
       "          [3.83858836e+00, 1.44060190e+00],\n",
       "          [9.87500201e+00, 7.27201097e+00]],\n",
       "\n",
       "         [[4.55069492e+00, 4.92917735e+00],\n",
       "          [2.53842712e-03, 2.96075067e-01],\n",
       "          [8.07576643e+00, 4.61696955e-01],\n",
       "          ...,\n",
       "          [3.63413875e+00, 4.85819898e+00],\n",
       "          [5.26453255e+00, 7.93133290e+00],\n",
       "          [6.15129710e+00, 6.92809647e-01]]]]])\n",
       "Coordinates:\n",
       "  * sample   (sample) int64 0 1 2 3 4\n",
       "  * tile     (tile) int64 0 1 2 3 4 5\n",
       "  * x        (x) int64 0 1 2 3 4 5 6 7 8 9 10 11\n",
       "  * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11\n",
       "  * z        (z) int64 0 1</pre><div class='xr-wrap' style='display:none'><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'></div><ul class='xr-dim-list'><li><span class='xr-has-index'>sample</span>: 5</li><li><span class='xr-has-index'>tile</span>: 6</li><li><span class='xr-has-index'>x</span>: 12</li><li><span class='xr-has-index'>y</span>: 12</li><li><span class='xr-has-index'>z</span>: 2</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-e6a98e18-399d-4515-ab6d-f50b3d6fcd0b' class='xr-array-in' type='checkbox' checked><label for='section-e6a98e18-399d-4515-ab6d-f50b3d6fcd0b' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>6.459 4.376 8.918 9.637 3.834 7.917 ... 4.858 5.265 7.931 6.151 0.6928</span></div><div class='xr-array-data'><pre>array([[[[[6.45894113e+00, 4.37587211e+00],\n",
       "          [8.91773001e+00, 9.63662761e+00],\n",
       "          [3.83441519e+00, 7.91725038e+00],\n",
       "          ...,\n",
       "          [7.80529176e+00, 1.18274426e+00],\n",
       "          [6.39921021e+00, 1.43353287e+00],\n",
       "          [9.44668917e+00, 5.21848322e+00]],\n",
       "\n",
       "         [[4.14661940e+00, 2.64555612e+00],\n",
       "          [7.74233689e+00, 4.56150332e+00],\n",
       "          [5.68433949e+00, 1.87898004e-01],\n",
       "          ...,\n",
       "          [1.28926298e+00, 3.15428351e+00],\n",
       "          [3.63710771e+00, 5.70196770e+00],\n",
       "          [4.38601513e+00, 9.88373838e+00]],\n",
       "\n",
       "         [[1.02044811e+00, 2.08876756e+00],\n",
       "          [1.61309518e+00, 6.53108325e+00],\n",
       "          [2.53291603e+00, 4.66310773e+00],\n",
       "          ...,\n",
       "...\n",
       "          ...,\n",
       "          [1.46508191e+00, 1.36721807e+00],\n",
       "          [8.85839863e+00, 1.42671272e-01],\n",
       "          [6.79763879e+00, 8.04785339e+00]],\n",
       "\n",
       "         [[7.17913038e+00, 1.44409263e+00],\n",
       "          [6.40503996e+00, 1.23486772e+00],\n",
       "          [4.62247278e+00, 6.54832523e+00],\n",
       "          ...,\n",
       "          [6.90017764e+00, 4.70643618e+00],\n",
       "          [3.83858836e+00, 1.44060190e+00],\n",
       "          [9.87500201e+00, 7.27201097e+00]],\n",
       "\n",
       "         [[4.55069492e+00, 4.92917735e+00],\n",
       "          [2.53842712e-03, 2.96075067e-01],\n",
       "          [8.07576643e+00, 4.61696955e-01],\n",
       "          ...,\n",
       "          [3.63413875e+00, 4.85819898e+00],\n",
       "          [5.26453255e+00, 7.93133290e+00],\n",
       "          [6.15129710e+00, 6.92809647e-01]]]]])</pre></div></div></li><li class='xr-section-item'><input id='section-d67dd17c-e39e-4f3f-9a6d-c0f4f03dae67' class='xr-section-summary-in' type='checkbox'  checked><label for='section-d67dd17c-e39e-4f3f-9a6d-c0f4f03dae67' class='xr-section-summary' >Coordinates: <span>(5)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>sample</span></div><div class='xr-var-dims'>(sample)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4</div><input id='attrs-c40ee843-fce6-4cda-b33c-21c67b2acadf' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-c40ee843-fce6-4cda-b33c-21c67b2acadf' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-359c8662-68d6-490f-9365-8a3ceeb7ba37' class='xr-var-data-in' type='checkbox'><label for='data-359c8662-68d6-490f-9365-8a3ceeb7ba37' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([0, 1, 2, 3, 4])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>tile</span></div><div class='xr-var-dims'>(tile)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 5</div><input id='attrs-f13a07e8-8ad4-43aa-a2a8-095cc09b27db' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-f13a07e8-8ad4-43aa-a2a8-095cc09b27db' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-6e113e95-ad4d-4b78-99fe-ef7a5f4dc5e7' class='xr-var-data-in' type='checkbox'><label for='data-6e113e95-ad4d-4b78-99fe-ef7a5f4dc5e7' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([0, 1, 2, 3, 4, 5])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>x</span></div><div class='xr-var-dims'>(x)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 5 6 7 8 9 10 11</div><input id='attrs-41802f3a-ec92-41e3-89e9-279d8e342c18' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-41802f3a-ec92-41e3-89e9-279d8e342c18' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-8d4d2fe1-5aa9-4228-8608-5933dfdccd90' class='xr-var-data-in' type='checkbox'><label for='data-8d4d2fe1-5aa9-4228-8608-5933dfdccd90' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>y</span></div><div class='xr-var-dims'>(y)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1 2 3 4 5 6 7 8 9 10 11</div><input id='attrs-1aa06a03-80ca-4833-942d-1f9facd7fb71' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-1aa06a03-80ca-4833-942d-1f9facd7fb71' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-b5af200c-5667-4f66-bc55-844dfd1776a5' class='xr-var-data-in' type='checkbox'><label for='data-b5af200c-5667-4f66-bc55-844dfd1776a5' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>z</span></div><div class='xr-var-dims'>(z)</div><div class='xr-var-dtype'>int64</div><div class='xr-var-preview xr-preview'>0 1</div><input id='attrs-b92054b8-2ca0-49b4-bd29-a961c98daf3b' class='xr-var-attrs-in' type='checkbox' disabled><label for='attrs-b92054b8-2ca0-49b4-bd29-a961c98daf3b' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-3a3f9a78-99f4-4c47-8008-7115ab8d7fc2' class='xr-var-data-in' type='checkbox'><label for='data-3a3f9a78-99f4-4c47-8008-7115ab8d7fc2' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'></dl></div><div class='xr-var-data'><pre>array([0, 1])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-29981d34-f570-4b2f-8f0a-cfa33b7bf40b' class='xr-section-summary-in' type='checkbox' disabled ><label for='section-29981d34-f570-4b2f-8f0a-cfa33b7bf40b' class='xr-section-summary'  title='Expand/collapse section'>Attributes: <span>(0)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray (sample: 5, tile: 6, x: 12, y: 12, z: 2)>\n",
       "array([[[[[6.45894113e+00, 4.37587211e+00],\n",
       "          [8.91773001e+00, 9.63662761e+00],\n",
       "          [3.83441519e+00, 7.91725038e+00],\n",
       "          ...,\n",
       "          [7.80529176e+00, 1.18274426e+00],\n",
       "          [6.39921021e+00, 1.43353287e+00],\n",
       "          [9.44668917e+00, 5.21848322e+00]],\n",
       "\n",
       "         [[4.14661940e+00, 2.64555612e+00],\n",
       "          [7.74233689e+00, 4.56150332e+00],\n",
       "          [5.68433949e+00, 1.87898004e-01],\n",
       "          ...,\n",
       "          [1.28926298e+00, 3.15428351e+00],\n",
       "          [3.63710771e+00, 5.70196770e+00],\n",
       "          [4.38601513e+00, 9.88373838e+00]],\n",
       "\n",
       "         [[1.02044811e+00, 2.08876756e+00],\n",
       "          [1.61309518e+00, 6.53108325e+00],\n",
       "          [2.53291603e+00, 4.66310773e+00],\n",
       "          ...,\n",
       "...\n",
       "          ...,\n",
       "          [1.46508191e+00, 1.36721807e+00],\n",
       "          [8.85839863e+00, 1.42671272e-01],\n",
       "          [6.79763879e+00, 8.04785339e+00]],\n",
       "\n",
       "         [[7.17913038e+00, 1.44409263e+00],\n",
       "          [6.40503996e+00, 1.23486772e+00],\n",
       "          [4.62247278e+00, 6.54832523e+00],\n",
       "          ...,\n",
       "          [6.90017764e+00, 4.70643618e+00],\n",
       "          [3.83858836e+00, 1.44060190e+00],\n",
       "          [9.87500201e+00, 7.27201097e+00]],\n",
       "\n",
       "         [[4.55069492e+00, 4.92917735e+00],\n",
       "          [2.53842712e-03, 2.96075067e-01],\n",
       "          [8.07576643e+00, 4.61696955e-01],\n",
       "          ...,\n",
       "          [3.63413875e+00, 4.85819898e+00],\n",
       "          [5.26453255e+00, 7.93133290e+00],\n",
       "          [6.15129710e+00, 6.92809647e-01]]]]])\n",
       "Coordinates:\n",
       "  * sample   (sample) int64 0 1 2 3 4\n",
       "  * tile     (tile) int64 0 1 2 3 4 5\n",
       "  * x        (x) int64 0 1 2 3 4 5 6 7 8 9 10 11\n",
       "  * y        (y) int64 0 1 2 3 4 5 6 7 8 9 10 11\n",
       "  * z        (z) int64 0 1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xr.DataArray(\n",
    "            random.uniform(low=0, high=10, size=size),\n",
    "            dims=[\"sample\", \"tile\", \"x\", \"y\", \"z\"],\n",
    "            coords=[range(size[i]) for i in range(len(size))],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-26 18:50:39.937758: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-07-26 18:50:39.942223: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-07-26 18:50:39.942245: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_records = 10\n",
    "nt, grid,nz = 40, 10,2\n",
    "\n",
    "def records():\n",
    "    for _ in range(n_records):\n",
    "        record = {\n",
    "            \"a\": np.random.uniform(0,1,[ nt, grid, nz]),\n",
    "            \"f\": np.random.uniform(0,1,[ nt, grid, nz]),\n",
    "        }\n",
    "        print(record.keys)\n",
    "        yield record\n",
    "        \n",
    "tfdataset = tf.data.Dataset.from_generator(\n",
    "    records,\n",
    "    output_types=(tf.float32),\n",
    "    output_shapes=(tf.TensorShape([nt,grid,nz])),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.records()>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(n_records):\n",
    "    record = {\n",
    "        \"a\": np.random.uniform(0,1,[ nt, grid, nz]),\n",
    "        \"f\": np.random.uniform(0,1,[ nt, grid, nz]),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 10, 2)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(record[\"f\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "record = {\n",
    "    \"a\": np.random.uniform([ nt, grid, nz]),\n",
    "    \"f\": np.random.uniform([ nt, grid, nz]),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': array([31.10933808,  7.7081584 ,  1.09996103]),\n",
       " 'f': array([17.02310492,  4.71533128,  1.92979082])}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40, 10, 2)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(np.random.uniform(0,1,[ nt, grid, nz]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphPredict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mUntitled-1.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000012untitled?line=9'>10</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfv3fit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_shared\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mconfig\u001b[39;00m \u001b[39mimport\u001b[39;00m TRAINING_FUNCTIONS, get_hyperparameter_class\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000012untitled?line=10'>11</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfv3fit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_shared\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhyperparameters\u001b[39;00m \u001b[39mimport\u001b[39;00m Hyperparameters\n\u001b[0;32m---> <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000012untitled?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mgraphPredict\u001b[39;00m \u001b[39mimport\u001b[39;00m PytorchModel\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000012untitled?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfv3fit\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtfdataset\u001b[39;00m \u001b[39mimport\u001b[39;00m tfdataset_from_batches\n\u001b[1;32m     <a href='vscode-notebook-cell:Untitled-1.ipynb?jupyter-notebook#ch0000012untitled?line=13'>14</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpytest\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'graphPredict'"
     ]
    }
   ],
   "source": [
    "import dataclasses\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import xarray as xr\n",
    "import fv3fit\n",
    "from fv3fit._shared.config import TRAINING_FUNCTIONS, get_hyperparameter_class\n",
    "from fv3fit.tfdataset import tfdataset_from_batches\n",
    "import fv3fit\n",
    "from typing import Any, Callable, Optional, Sequence, TextIO, Tuple\n",
    "from fv3fit._shared.config import TRAINING_FUNCTIONS, get_hyperparameter_class\n",
    "from fv3fit._shared.hyperparameters import Hyperparameters\n",
    "from graphPredict import PytorchModel\n",
    "from fv3fit.tfdataset import tfdataset_from_batches\n",
    "import pytest\n",
    "from typing import Callable, Sequence, Union\n",
    "import xarray as xr\n",
    "\n",
    "\n",
    "GENERAL_TRAINING_TYPES = [\n",
    "    \"graph\",\n",
    "]\n",
    "\n",
    "# automatically test on every registered training class\n",
    "@pytest.fixture(params=GENERAL_TRAINING_TYPES)\n",
    "def model_type(request):\n",
    "    return request.param\n",
    "\n",
    "@dataclasses.dataclass\n",
    "class TrainingResult:\n",
    "    model: PytorchModel\n",
    "    output_variables: Sequence[str]\n",
    "    test_dataset: xr.Dataset\n",
    "    hyperparameters: Hyperparameters\n",
    "\n",
    "\n",
    "def train_identity_model(model_type, sample_func, hyperparameters=None):\n",
    "    input_variables, output_variables, train_dataset = get_data(\n",
    "        model_type, sample_func\n",
    "    )\n",
    "    if hyperparameters is None:\n",
    "        cls = get_hyperparameter_class(model_type)\n",
    "        hyperparameters = cls.init_testing(input_variables, output_variables)\n",
    "    input_variables, output_variables, test_dataset = get_data(\n",
    "        model_type, sample_func\n",
    "    )\n",
    "    train_tfdataset = tfdataset_from_batches([train_dataset for _ in range(10)])\n",
    "    val_tfdataset = tfdataset_from_batches([test_dataset])\n",
    "    train = fv3fit.get_training_function(model_type)\n",
    "    model = train(hyperparameters, train_tfdataset, val_tfdataset)\n",
    "    return TrainingResult(model, output_variables, test_dataset, hyperparameters)\n",
    "\n",
    "\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_train_default_model_on_identity(model_type: str, regtest):\n",
    "    \"\"\"\n",
    "    The model with default configuration options can learn the identity function,\n",
    "    using gaussian-sampled data around 0 with unit variance.\n",
    "    \"\"\"\n",
    "\n",
    "    fv3fit.set_random_seed(1)\n",
    "    # don't set n_feature too high for this, because of curse of dimensionality\n",
    "    n_sample, grid, n_feature = 5, 864 , 2\n",
    "    sample_func = get_uniform_sample_func(size=(n_sample, grid, n_feature))\n",
    "\n",
    "    assert_can_learn_identity(\n",
    "        model_type, sample_func=sample_func, max_rmse=0.2, regtest=regtest,\n",
    "    )\n",
    "\n",
    "def get_uniform_sample_func(size, low=0, high=1, seed=0):\n",
    "    random = np.random.RandomState(seed=seed)\n",
    "\n",
    "    def sample_func():\n",
    "        return xr.DataArray(\n",
    "            random.uniform(low=low, high=high, size=size),\n",
    "            dims=[\"sample\", \"grid\", \"z\"],\n",
    "            coords=[range(size[i]) for i in range(len(size))],\n",
    "        )\n",
    "\n",
    "    return sample_func\n",
    "\n",
    "\n",
    "def assert_can_learn_identity(\n",
    "    model_type,\n",
    "    sample_func: Callable[[], xr.DataArray],\n",
    "    max_rmse: float,\n",
    "    regtest: Optional[TextIO] = None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        model_type: type of model to train\n",
    "        hyperparameters: model configuration\n",
    "        sample_func: function that returns example DataArrays for training and\n",
    "            validation, should return different data on subsequent calls\n",
    "        max_rmse: maximum permissible root mean squared error\n",
    "        regtest: if given, write hash of output dataset to this file object\n",
    "    \"\"\"\n",
    "    result = train_identity_model(model_type, sample_func=sample_func)\n",
    "    out_dataset = result.model.predict(result.test_dataset)\n",
    "    for name in result.output_variables:\n",
    "        assert out_dataset[name].dims == result.test_dataset[name].dims\n",
    "    rmse = (\n",
    "        np.mean(\n",
    "            [\n",
    "                np.mean((out_dataset[name] - result.test_dataset[name]) ** 2)\n",
    "                / np.std(result.test_dataset[name]) ** 2\n",
    "                for name in result.output_variables\n",
    "            ]\n",
    "        )\n",
    "        ** 0.5\n",
    "    )\n",
    "    assert rmse < max_rmse\n",
    "\n",
    "\n",
    "\n",
    "def get_data() -> tf.data.Dataset:\n",
    "    n_records = 10\n",
    "    nt, grid,nz = 40, 10,2\n",
    "    low=0\n",
    "    high=1\n",
    "    \n",
    "    def records():\n",
    "        for _ in range(n_records):\n",
    "            record = {\n",
    "                \"a\": np.random.uniform(low, high,[ nt, grid, nz]),\n",
    "                \"f\": np.random.uniform(low, high,[ nt, grid, nz]),\n",
    "            }\n",
    "            yield record\n",
    "        \n",
    "    tfdataset = tf.data.Dataset.from_generator(\n",
    "        records,\n",
    "        output_types=(tf.float32),\n",
    "        output_shapes=(tf.TensorShape([nt,grid,nz])),\n",
    "    )\n",
    "    return tfdataset\n",
    "\n",
    "    # set random seed\n",
    "    # prepare randomly-generated \"identity function\" data\n",
    "    # train the model\n",
    "    # get the outputs from the model\n",
    "    # check the outputs are sufficiently accurate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from dgl.nn import SAGEConv\n",
    "from dgl.nn.pytorch import NNConv\n",
    "import numpy as np\n",
    "import dask.diagnostics\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import time\n",
    "import select as sl\n",
    "import pickle\n",
    "from load_data import *\n",
    "from utilsMPGNNUnet import *\n",
    "import wandb\n",
    "from fv3net.artifacts.resolve_url import resolve_url\n",
    "from vcm import get_fs\n",
    "\n",
    "lead=6\n",
    "\n",
    "coarsenInd=1\n",
    "g = pickle.load(open(\"UpdatedGraph_Neighbour10\", 'rb'))\n",
    "residual=0\n",
    "num_step=15\n",
    "aggregat='mean'\n",
    "\n",
    "control_str='MultiSAGEUnet'#'TNSTTNST' #'TNTSTNTST'\n",
    "\n",
    "print(control_str)\n",
    "\n",
    "epochs=50\n",
    "\n",
    "variableList=['h500','h200','h850']\n",
    "TotalSamples=8500\n",
    "Chuncksize=20\n",
    "\n",
    "\n",
    "\n",
    "lr=0.001\n",
    "disablecuda ='store_true'\n",
    "batch_size=1\n",
    "drop_prob = 0\n",
    "out_feat=2\n",
    "\n",
    "savemodelpath = (\n",
    "    \"weight_layer_\"\n",
    "    + control_str\n",
    "    + \"_lead\"\n",
    "    + str(lead)\n",
    "    + \"_epochs_\"\n",
    "    + str(epochs)\n",
    "    +\"MP_Block_\"\n",
    "    +str(num_step)\n",
    "    + \"aggregat_\"\n",
    "    +aggregat\n",
    "    +\"coarsen_\"\n",
    "    +str(coarsenInd)\n",
    "    +\"residual_\"\n",
    "    +str(residual)\n",
    "    +\".pt\"\n",
    ")\n",
    "\n",
    "print(savemodelpath)\n",
    "\n",
    "BUCKET = \"vcm-ml-experiments\"\n",
    "PROJECT = \"full-model-emulation\"\n",
    "\n",
    "model_out_url = resolve_url(BUCKET, PROJECT, savemodelpath)\n",
    "data_url = \"gs://vcm-ml-scratch/ebrahimn/2022-07-02/experiment-1-y/fv3gfs_run/\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_url = \"gs://vcm-ml-scratch/ebrahimn/2022-07-02/experiment-1-y/fv3gfs_run/\" \n",
    "state_training_data = xr.open_zarr(\n",
    "    fsspec.get_mapper(os.path.join(data_url, \"atmos_dt_atmos.zarr\")), consolidated=True\n",
    ")\n",
    "# state_training_data2 = xr.open_zarr(fsspec.get_mapper(os.path.join(data_url, 'sfc_dt_atmos.zarr')))\n",
    "lat_lon_data = xr.open_zarr(\n",
    "    fsspec.get_mapper(os.path.join(data_url, \"state_after_timestep.zarr\"))\n",
    ")\n",
    "\n",
    "landSea = xr.open_zarr(\n",
    "    fsspec.get_mapper(\n",
    "        \"gs://vcm-ml-experiments/default/2022-05-09/baseline-35day-spec-sst/fv3gfs_run/state_after_timestep.zarr\"\n",
    "    )\n",
    ")\n",
    "landSea_Mask = landSea.land_sea_mask[1].load()\n",
    "landSea_Mask = landSea_Mask[:, ::coarsenInd, ::coarsenInd].values.flatten()\n",
    "\n",
    "\n",
    "lat=(lat_lon_data.latitude[1].load())\n",
    "lon=(lat_lon_data.longitude[1].load())\n",
    "lat=lat[:,::coarsenInd,::coarsenInd].values.flatten()\n",
    "lon=lon[:,::coarsenInd,::coarsenInd].values.flatten()\n",
    "# cosLat=np.expand_dims(np.cos(lat),axis=1)\n",
    "# cosLon=np.expand_dims(np.cos(lon),axis=1)\n",
    "# sinLat=np.expand_dims(np.sin(lat),axis=1)\n",
    "# sinLon=np.expand_dims(np.sin(lon),axis=1)\n",
    "cosLat=np.cos(lat)\n",
    "cosLon=np.cos(lon)\n",
    "sinLat=np.sin(lat)\n",
    "sinLon=np.sin(lon)\n",
    "for i in range(2):\n",
    "        if i==0:\n",
    "            sinLon=torch.tensor(sinLon).unsqueeze(0).repeat(1,1)\n",
    "            cosLon=torch.tensor(cosLon).unsqueeze(0).repeat(1,1)\n",
    "            sinLat=torch.tensor(sinLat).unsqueeze(0).repeat(1,1)\n",
    "            cosLat=torch.tensor(cosLat).unsqueeze(0).repeat(1,1)\n",
    "            landSea_Mask=torch.tensor(landSea_Mask).unsqueeze(0).repeat(1,1)\n",
    "        elif i==1:\n",
    "            sinLon=(sinLon).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            cosLon=(cosLon).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            sinLat=(sinLat).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            cosLat=(cosLat).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            landSea_Mask=(landSea_Mask).unsqueeze(0).repeat(batch_size,1,1)\n",
    "\n",
    "exteraVar=torch.cat((sinLon, sinLat,cosLon,cosLat,landSea_Mask), 1).to(device)\n",
    "exteraVar=np.swapaxes(exteraVar,2, 1)\n",
    "print(device)\n",
    "\n",
    "num_nodes=len(lon)\n",
    "print(f\"numebr of grids: {num_nodes}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "edg=np.asarray(g.edges())\n",
    "latInd=lat[edg[1]]\n",
    "lonInd=lon[edg[1]]\n",
    "latlon=[latInd.T,lonInd.T]\n",
    "# latlon=np.swapaxes(latlon, 1, 0)\n",
    "latlon=torch.from_numpy(np.swapaxes(latlon, 1, 0)).float()\n",
    "latlon=latlon.to(device)\n",
    "\n",
    "\n",
    "Zmean=5765.8457   #Z500mean=5765.8457, \n",
    "Zstd=90.79599   #Z500std=90.79599\n",
    "\n",
    "Tmean=10643.382          #Thickmean=10643.382\n",
    "Tstd=162.12427              #Thickstd=162.12427\n",
    "valInde=0\n",
    "\n",
    "print('loading model')\n",
    "\n",
    "class UnetGraphSAGE(nn.Module):\n",
    "    def __init__(self, g, in_feats, h_feats,out_feat,num_step,aggregat):\n",
    "        super(UnetGraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats,aggregat)\n",
    "        self.conv2 = SAGEConv(h_feats, int(h_feats/2), aggregat)\n",
    "        self.conv3 = SAGEConv(int(h_feats/2), int(h_feats/4), aggregat)\n",
    "        self.conv4 = SAGEConv(int(h_feats/4), int(h_feats/4), aggregat)\n",
    "        self.conv5 = SAGEConv(int(h_feats/2), int(h_feats/2), aggregat)\n",
    "        self.conv6 = SAGEConv(h_feats, out_feat,aggregat)\n",
    "        self.g=g\n",
    "        self.num_step=num_step\n",
    "        \n",
    "    def forward(self, in_feat,exteraVar1):\n",
    "\n",
    "        for _ in range(self.num_step):\n",
    "            h = self.conv1(self.g, in_feat)\n",
    "            h = F.relu(h)\n",
    "            h = self.conv2(self.g, h)\n",
    "            h = F.relu(h)\n",
    "            h = self.conv3(self.g, h)\n",
    "            h = F.relu(h)\n",
    "            tuple = (self.conv4(self.g, h),h)\n",
    "            h = torch.cat(tuple,dim=1)\n",
    "            h = F.relu(h)\n",
    "            tuple = (self.conv5(self.g, h),h)\n",
    "            h = torch.cat(tuple,dim=1)\n",
    "            h = F.relu(h)\n",
    "            h = self.conv6(self.g, h)\n",
    "            in_feat=torch.cat((h, torch.squeeze(exteraVar1)), 1).float()\n",
    "        return h\n",
    "        \n",
    "\n",
    "loss = nn.MSELoss()\n",
    "g = g.to(device)\n",
    "model = UnetGraphSAGE(g,7,256, 2,num_step,aggregat).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
    "model.train()\n",
    "\n",
    "all_indices=np.random.permutation(np.arange(start=0, stop=int(TotalSamples/Chuncksize)))\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    for ss in all_indices:\n",
    "\n",
    "        Z500train=state_training_data[variableList[0]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "        T2mtrain1=state_training_data[variableList[1]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "        T2mtrain2=state_training_data[variableList[2]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "\n",
    "        Z500train=np.swapaxes(Z500train.values, 1, 0)\n",
    "        T2mtrain1=np.swapaxes(T2mtrain1.values, 1, 0)\n",
    "        T2mtrain2=np.swapaxes(T2mtrain2.values, 1, 0)\n",
    "\n",
    "        T2mtrain=T2mtrain1-T2mtrain2\n",
    "\n",
    "        T2mtrain=T2mtrain.reshape(np.size(T2mtrain, 0), np.size(T2mtrain, 1)*np.size(T2mtrain, 2)*np.size(T2mtrain, 3))\n",
    "        Z500train=Z500train.reshape(np.size(Z500train, 0), np.size(Z500train, 1)*np.size(Z500train, 2)*np.size(Z500train, 3))\n",
    "\n",
    "        # Zmean = np.mean(Z500train)\n",
    "        # Zstd = np.std(Z500train)\n",
    "\n",
    "        # Tmean = np.mean(T2mtrain)\n",
    "        # Tstd = np.std(T2mtrain)\n",
    "\n",
    "\n",
    "        T2mtrain=(T2mtrain-Tmean)/Tstd\n",
    "        Z500train=(Z500train-Zmean)/Zstd\n",
    "\n",
    "        \n",
    "\n",
    "        T2mtrain=np.expand_dims(T2mtrain,axis=0)\n",
    "        Z500train=np.expand_dims(Z500train,axis=0)\n",
    "\n",
    "        dataSets=np.concatenate((Z500train,T2mtrain),axis=0)\n",
    "\n",
    "        num_samples=np.size(dataSets,1)\n",
    "        print(f\"Total samples: {num_samples}\")\n",
    "\n",
    "\n",
    "        len_val = round(num_samples * 0.25)\n",
    "        len_train = round(num_samples * 0.75)\n",
    "        train = dataSets[:,: len_train]\n",
    "        val = dataSets[:,len_train+14: len_train + len_val]\n",
    "\n",
    "        x_train=train[:,0:-2*lead,:]\n",
    "        y_train=train[:,lead:-lead,:]\n",
    "        y_train2=train[:,2*lead::,:]\n",
    "\n",
    "        x_val=val[:,0:-lead,:]\n",
    "        y_val=val[:,lead::,:]\n",
    "\n",
    "        if residual==1:\n",
    "            y_train=y_train-x_train\n",
    "            y_val=y_val-x_val\n",
    "\n",
    "\n",
    "        x_train=np.swapaxes(x_train, 1, 0)\n",
    "        y_train=np.swapaxes(y_train, 1, 0)\n",
    "        y_train2=np.swapaxes(y_train2, 1, 0)\n",
    "\n",
    "        x_train=np.swapaxes(x_train, 2, 1)\n",
    "        y_train=np.swapaxes(y_train, 2, 1)\n",
    "        y_train2=np.swapaxes(y_train2, 2, 1)\n",
    "\n",
    "        x_train=torch.Tensor(x_train).to(device)\n",
    "        y_train=torch.Tensor(y_train).to(device)\n",
    "        y_train2=torch.Tensor(y_train2).to(device)\n",
    "\n",
    "        train_data = torch.utils.data.TensorDataset(x_train, y_train, y_train2)\n",
    "        train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "        x_val=np.swapaxes(x_val, 1, 0)\n",
    "        y_val=np.swapaxes(y_val, 1, 0)\n",
    "        x_val=np.swapaxes(x_val, 2, 1)\n",
    "        y_val=np.swapaxes(y_val, 2, 1)\n",
    "        x_val=torch.Tensor(x_val).to(device)\n",
    "        y_val=torch.Tensor(y_val).to(device)\n",
    "\n",
    "\n",
    "        val_data = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "        val_iter = torch.utils.data.DataLoader(val_data, batch_size)\n",
    "        \n",
    "\n",
    "        if valInde==0:\n",
    "            min_val_loss = np.inf\n",
    "            valInde+=1\n",
    "\n",
    "        l_sum, n = 0.0, 0\n",
    "        for x, y, y2 in train_iter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahimn/miniconda3/envs/dlwp2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'load_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/ebrahimn/fv3net/projects/full_model_emulation/pytorch/CheckRuns.ipynb Cell 15'\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/pytorch/CheckRuns.ipynb#ch0000014vscode-remote?line=18'>19</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mselect\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msl\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/pytorch/CheckRuns.ipynb#ch0000014vscode-remote?line=19'>20</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/pytorch/CheckRuns.ipynb#ch0000014vscode-remote?line=20'>21</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mload_data\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/pytorch/CheckRuns.ipynb#ch0000014vscode-remote?line=21'>22</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mutilsMPGNNUnet\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/pytorch/CheckRuns.ipynb#ch0000014vscode-remote?line=22'>23</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwandb\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'load_data'"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from dgl.nn import SAGEConv\n",
    "from dgl.nn.pytorch import NNConv\n",
    "import numpy as np\n",
    "import dask.diagnostics\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import time\n",
    "import select as sl\n",
    "import pickle\n",
    "from load_data import *\n",
    "from utilsMPGNNUnet import *\n",
    "import wandb\n",
    "from fv3net.artifacts.resolve_url import resolve_url\n",
    "from vcm import get_fs\n",
    "\n",
    "lead=6\n",
    "\n",
    "coarsenInd=1\n",
    "g = pickle.load(open(\"UpdatedGraph_Neighbour10\", 'rb'))\n",
    "residual=0\n",
    "num_step=15\n",
    "aggregat='mean'\n",
    "\n",
    "control_str='MultiSAGEUnet'#'TNSTTNST' #'TNTSTNTST'\n",
    "\n",
    "print(control_str)\n",
    "\n",
    "epochs=50\n",
    "\n",
    "variableList=['h500','h200','h850']\n",
    "TotalSamples=8500\n",
    "Chuncksize=20\n",
    "\n",
    "\n",
    "\n",
    "lr=0.001\n",
    "disablecuda ='store_true'\n",
    "batch_size=1\n",
    "drop_prob = 0\n",
    "out_feat=2\n",
    "\n",
    "savemodelpath = (\n",
    "    \"weight_layer_\"\n",
    "    + control_str\n",
    "    + \"_lead\"\n",
    "    + str(lead)\n",
    "    + \"_epochs_\"\n",
    "    + str(epochs)\n",
    "    +\"MP_Block_\"\n",
    "    +str(num_step)\n",
    "    + \"aggregat_\"\n",
    "    +aggregat\n",
    "    +\"coarsen_\"\n",
    "    +str(coarsenInd)\n",
    "    +\"residual_\"\n",
    "    +str(residual)\n",
    "    +\".pt\"\n",
    ")\n",
    "\n",
    "print(savemodelpath)\n",
    "\n",
    "BUCKET = \"vcm-ml-experiments\"\n",
    "PROJECT = \"full-model-emulation\"\n",
    "\n",
    "model_out_url = resolve_url(BUCKET, PROJECT, savemodelpath)\n",
    "data_url = \"gs://vcm-ml-scratch/ebrahimn/2022-07-02/experiment-1-y/fv3gfs_run/\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_url = \"gs://vcm-ml-scratch/ebrahimn/2022-07-02/experiment-1-y/fv3gfs_run/\" \n",
    "state_training_data = xr.open_zarr(\n",
    "    fsspec.get_mapper(os.path.join(data_url, \"atmos_dt_atmos.zarr\")), consolidated=True\n",
    ")\n",
    "# state_training_data2 = xr.open_zarr(fsspec.get_mapper(os.path.join(data_url, 'sfc_dt_atmos.zarr')))\n",
    "lat_lon_data = xr.open_zarr(\n",
    "    fsspec.get_mapper(os.path.join(data_url, \"state_after_timestep.zarr\"))\n",
    ")\n",
    "\n",
    "landSea = xr.open_zarr(\n",
    "    fsspec.get_mapper(\n",
    "        \"gs://vcm-ml-experiments/default/2022-05-09/baseline-35day-spec-sst/fv3gfs_run/state_after_timestep.zarr\"\n",
    "    )\n",
    ")\n",
    "landSea_Mask = landSea.land_sea_mask[1].load()\n",
    "landSea_Mask = landSea_Mask[:, ::coarsenInd, ::coarsenInd].values.flatten()\n",
    "\n",
    "\n",
    "lat=(lat_lon_data.latitude[1].load())\n",
    "lon=(lat_lon_data.longitude[1].load())\n",
    "lat=lat[:,::coarsenInd,::coarsenInd].values.flatten()\n",
    "lon=lon[:,::coarsenInd,::coarsenInd].values.flatten()\n",
    "# cosLat=np.expand_dims(np.cos(lat),axis=1)\n",
    "# cosLon=np.expand_dims(np.cos(lon),axis=1)\n",
    "# sinLat=np.expand_dims(np.sin(lat),axis=1)\n",
    "# sinLon=np.expand_dims(np.sin(lon),axis=1)\n",
    "cosLat=np.cos(lat)\n",
    "cosLon=np.cos(lon)\n",
    "sinLat=np.sin(lat)\n",
    "sinLon=np.sin(lon)\n",
    "for i in range(2):\n",
    "        if i==0:\n",
    "            sinLon=torch.tensor(sinLon).unsqueeze(0).repeat(1,1)\n",
    "            cosLon=torch.tensor(cosLon).unsqueeze(0).repeat(1,1)\n",
    "            sinLat=torch.tensor(sinLat).unsqueeze(0).repeat(1,1)\n",
    "            cosLat=torch.tensor(cosLat).unsqueeze(0).repeat(1,1)\n",
    "            landSea_Mask=torch.tensor(landSea_Mask).unsqueeze(0).repeat(1,1)\n",
    "        elif i==1:\n",
    "            sinLon=(sinLon).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            cosLon=(cosLon).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            sinLat=(sinLat).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            cosLat=(cosLat).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            landSea_Mask=(landSea_Mask).unsqueeze(0).repeat(batch_size,1,1)\n",
    "\n",
    "exteraVar=torch.cat((sinLon, sinLat,cosLon,cosLat,landSea_Mask), 1).to(device)\n",
    "exteraVar=np.swapaxes(exteraVar,2, 1)\n",
    "print(device)\n",
    "\n",
    "num_nodes=len(lon)\n",
    "print(f\"numebr of grids: {num_nodes}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "edg=np.asarray(g.edges())\n",
    "latInd=lat[edg[1]]\n",
    "lonInd=lon[edg[1]]\n",
    "latlon=[latInd.T,lonInd.T]\n",
    "# latlon=np.swapaxes(latlon, 1, 0)\n",
    "latlon=torch.from_numpy(np.swapaxes(latlon, 1, 0)).float()\n",
    "latlon=latlon.to(device)\n",
    "\n",
    "\n",
    "Zmean=5765.8457   #Z500mean=5765.8457, \n",
    "Zstd=90.79599   #Z500std=90.79599\n",
    "\n",
    "Tmean=10643.382          #Thickmean=10643.382\n",
    "Tstd=162.12427              #Thickstd=162.12427\n",
    "valInde=0\n",
    "\n",
    "print('loading model')\n",
    "\n",
    "class UnetGraphSAGE(nn.Module):\n",
    "    def __init__(self, g, in_feats, h_feats,out_feat,num_step,aggregat):\n",
    "        super(UnetGraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats,aggregat)\n",
    "        self.conv2 = SAGEConv(h_feats, int(h_feats/2), aggregat)\n",
    "        self.conv3 = SAGEConv(int(h_feats/2), int(h_feats/4), aggregat)\n",
    "        self.conv4 = SAGEConv(int(h_feats/4), int(h_feats/4), aggregat)\n",
    "        self.conv5 = SAGEConv(int(h_feats/2), int(h_feats/2), aggregat)\n",
    "        self.conv6 = SAGEConv(h_feats, out_feat,aggregat)\n",
    "        self.g=g\n",
    "        self.num_step=num_step\n",
    "        \n",
    "    def forward(self, in_feat,exteraVar1):\n",
    "\n",
    "        for _ in range(self.num_step):\n",
    "            h = self.conv1(self.g, in_feat)\n",
    "            h = F.relu(h)\n",
    "            h = self.conv2(self.g, h)\n",
    "            h = F.relu(h)\n",
    "            h = self.conv3(self.g, h)\n",
    "            h = F.relu(h)\n",
    "            tuple = (self.conv4(self.g, h),h)\n",
    "            h = torch.cat(tuple,dim=1)\n",
    "            h = F.relu(h)\n",
    "            tuple = (self.conv5(self.g, h),h)\n",
    "            h = torch.cat(tuple,dim=1)\n",
    "            h = F.relu(h)\n",
    "            h = self.conv6(self.g, h)\n",
    "            in_feat=torch.cat((h, torch.squeeze(exteraVar1)), 1).float()\n",
    "        return h\n",
    "        \n",
    "\n",
    "loss = nn.MSELoss()\n",
    "g = g.to(device)\n",
    "model = UnetGraphSAGE(g,7,256, 2,num_step,aggregat).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
    "model.train()\n",
    "\n",
    "all_indices=np.random.permutation(np.arange(start=0, stop=int(TotalSamples/Chuncksize)))\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    for ss in all_indices:\n",
    "\n",
    "        Z500train=state_training_data[variableList[0]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "        T2mtrain1=state_training_data[variableList[1]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "        T2mtrain2=state_training_data[variableList[2]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "\n",
    "        Z500train=np.swapaxes(Z500train.values, 1, 0)\n",
    "        T2mtrain1=np.swapaxes(T2mtrain1.values, 1, 0)\n",
    "        T2mtrain2=np.swapaxes(T2mtrain2.values, 1, 0)\n",
    "\n",
    "        T2mtrain=T2mtrain1-T2mtrain2\n",
    "\n",
    "        T2mtrain=T2mtrain.reshape(np.size(T2mtrain, 0), np.size(T2mtrain, 1)*np.size(T2mtrain, 2)*np.size(T2mtrain, 3))\n",
    "        Z500train=Z500train.reshape(np.size(Z500train, 0), np.size(Z500train, 1)*np.size(Z500train, 2)*np.size(Z500train, 3))\n",
    "\n",
    "        # Zmean = np.mean(Z500train)\n",
    "        # Zstd = np.std(Z500train)\n",
    "\n",
    "        # Tmean = np.mean(T2mtrain)\n",
    "        # Tstd = np.std(T2mtrain)\n",
    "\n",
    "\n",
    "        T2mtrain=(T2mtrain-Tmean)/Tstd\n",
    "        Z500train=(Z500train-Zmean)/Zstd\n",
    "\n",
    "        \n",
    "\n",
    "        T2mtrain=np.expand_dims(T2mtrain,axis=0)\n",
    "        Z500train=np.expand_dims(Z500train,axis=0)\n",
    "\n",
    "        dataSets=np.concatenate((Z500train,T2mtrain),axis=0)\n",
    "\n",
    "        num_samples=np.size(dataSets,1)\n",
    "        print(f\"Total samples: {num_samples}\")\n",
    "\n",
    "\n",
    "        len_val = round(num_samples * 0.25)\n",
    "        len_train = round(num_samples * 0.75)\n",
    "        train = dataSets[:,: len_train]\n",
    "        val = dataSets[:,len_train+14: len_train + len_val]\n",
    "\n",
    "        x_train=train[:,0:-2*lead,:]\n",
    "        y_train=train[:,lead:-lead,:]\n",
    "        y_train2=train[:,2*lead::,:]\n",
    "        x_train[:,6,:]-y_train[:,0,:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('dlwp2': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dc1de1bc25c146e7b8663a4caf5b22ebe32509a4e20c2a1f047c1febb517b1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
