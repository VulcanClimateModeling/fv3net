{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.contrib.data import load_data\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "g1 = pickle.load(open(\"UpdatedGraph_Neighbour10\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vcm.catalog import catalog\n",
    "import xarray as xr\n",
    "\n",
    "grid = catalog[\"grid/c48\"].read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><svg style=\"position: absolute; width: 0; height: 0; overflow: hidden\">\n",
       "<defs>\n",
       "<symbol id=\"icon-database\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z\"></path>\n",
       "<path d=\"M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "<path d=\"M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z\"></path>\n",
       "</symbol>\n",
       "<symbol id=\"icon-file-text2\" viewBox=\"0 0 32 32\">\n",
       "<path d=\"M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z\"></path>\n",
       "<path d=\"M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "<path d=\"M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z\"></path>\n",
       "</symbol>\n",
       "</defs>\n",
       "</svg>\n",
       "<style>/* CSS stylesheet for displaying xarray objects in jupyterlab.\n",
       " *\n",
       " */\n",
       "\n",
       ":root {\n",
       "  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));\n",
       "  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));\n",
       "  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));\n",
       "  --xr-border-color: var(--jp-border-color2, #e0e0e0);\n",
       "  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);\n",
       "  --xr-background-color: var(--jp-layout-color0, white);\n",
       "  --xr-background-color-row-even: var(--jp-layout-color1, white);\n",
       "  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);\n",
       "}\n",
       "\n",
       "html[theme=dark],\n",
       "body.vscode-dark {\n",
       "  --xr-font-color0: rgba(255, 255, 255, 1);\n",
       "  --xr-font-color2: rgba(255, 255, 255, 0.54);\n",
       "  --xr-font-color3: rgba(255, 255, 255, 0.38);\n",
       "  --xr-border-color: #1F1F1F;\n",
       "  --xr-disabled-color: #515151;\n",
       "  --xr-background-color: #111111;\n",
       "  --xr-background-color-row-even: #111111;\n",
       "  --xr-background-color-row-odd: #313131;\n",
       "}\n",
       "\n",
       ".xr-wrap {\n",
       "  display: block;\n",
       "  min-width: 300px;\n",
       "  max-width: 700px;\n",
       "}\n",
       "\n",
       ".xr-text-repr-fallback {\n",
       "  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-header {\n",
       "  padding-top: 6px;\n",
       "  padding-bottom: 6px;\n",
       "  margin-bottom: 4px;\n",
       "  border-bottom: solid 1px var(--xr-border-color);\n",
       "}\n",
       "\n",
       ".xr-header > div,\n",
       ".xr-header > ul {\n",
       "  display: inline;\n",
       "  margin-top: 0;\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-obj-type,\n",
       ".xr-array-name {\n",
       "  margin-left: 2px;\n",
       "  margin-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-obj-type {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-sections {\n",
       "  padding-left: 0 !important;\n",
       "  display: grid;\n",
       "  grid-template-columns: 150px auto auto 1fr 20px 20px;\n",
       "}\n",
       "\n",
       ".xr-section-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-section-item input {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-item input + label {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label {\n",
       "  cursor: pointer;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-item input:enabled + label:hover {\n",
       "  color: var(--xr-font-color0);\n",
       "}\n",
       "\n",
       ".xr-section-summary {\n",
       "  grid-column: 1;\n",
       "  color: var(--xr-font-color2);\n",
       "  font-weight: 500;\n",
       "}\n",
       "\n",
       ".xr-section-summary > span {\n",
       "  display: inline-block;\n",
       "  padding-left: 0.5em;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label {\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in + label:before {\n",
       "  display: inline-block;\n",
       "  content: '►';\n",
       "  font-size: 11px;\n",
       "  width: 15px;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:disabled + label:before {\n",
       "  color: var(--xr-disabled-color);\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label:before {\n",
       "  content: '▼';\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked + label > span {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-section-summary,\n",
       ".xr-section-inline-details {\n",
       "  padding-top: 4px;\n",
       "  padding-bottom: 4px;\n",
       "}\n",
       "\n",
       ".xr-section-inline-details {\n",
       "  grid-column: 2 / -1;\n",
       "}\n",
       "\n",
       ".xr-section-details {\n",
       "  display: none;\n",
       "  grid-column: 1 / -1;\n",
       "  margin-bottom: 5px;\n",
       "}\n",
       "\n",
       ".xr-section-summary-in:checked ~ .xr-section-details {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-array-wrap {\n",
       "  grid-column: 1 / -1;\n",
       "  display: grid;\n",
       "  grid-template-columns: 20px auto;\n",
       "}\n",
       "\n",
       ".xr-array-wrap > label {\n",
       "  grid-column: 1;\n",
       "  vertical-align: top;\n",
       "}\n",
       "\n",
       ".xr-preview {\n",
       "  color: var(--xr-font-color3);\n",
       "}\n",
       "\n",
       ".xr-array-preview,\n",
       ".xr-array-data {\n",
       "  padding: 0 5px !important;\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-array-data,\n",
       ".xr-array-in:checked ~ .xr-array-preview {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       ".xr-array-in:checked ~ .xr-array-data,\n",
       ".xr-array-preview {\n",
       "  display: inline-block;\n",
       "}\n",
       "\n",
       ".xr-dim-list {\n",
       "  display: inline-block !important;\n",
       "  list-style: none;\n",
       "  padding: 0 !important;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list li {\n",
       "  display: inline-block;\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "}\n",
       "\n",
       ".xr-dim-list:before {\n",
       "  content: '(';\n",
       "}\n",
       "\n",
       ".xr-dim-list:after {\n",
       "  content: ')';\n",
       "}\n",
       "\n",
       ".xr-dim-list li:not(:last-child):after {\n",
       "  content: ',';\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-has-index {\n",
       "  font-weight: bold;\n",
       "}\n",
       "\n",
       ".xr-var-list,\n",
       ".xr-var-item {\n",
       "  display: contents;\n",
       "}\n",
       "\n",
       ".xr-var-item > div,\n",
       ".xr-var-item label,\n",
       ".xr-var-item > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-even);\n",
       "  margin-bottom: 0;\n",
       "}\n",
       "\n",
       ".xr-var-item > .xr-var-name:hover span {\n",
       "  padding-right: 5px;\n",
       "}\n",
       "\n",
       ".xr-var-list > li:nth-child(odd) > div,\n",
       ".xr-var-list > li:nth-child(odd) > label,\n",
       ".xr-var-list > li:nth-child(odd) > .xr-var-name span {\n",
       "  background-color: var(--xr-background-color-row-odd);\n",
       "}\n",
       "\n",
       ".xr-var-name {\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-var-dims {\n",
       "  grid-column: 2;\n",
       "}\n",
       "\n",
       ".xr-var-dtype {\n",
       "  grid-column: 3;\n",
       "  text-align: right;\n",
       "  color: var(--xr-font-color2);\n",
       "}\n",
       "\n",
       ".xr-var-preview {\n",
       "  grid-column: 4;\n",
       "}\n",
       "\n",
       ".xr-var-name,\n",
       ".xr-var-dims,\n",
       ".xr-var-dtype,\n",
       ".xr-preview,\n",
       ".xr-attrs dt {\n",
       "  white-space: nowrap;\n",
       "  overflow: hidden;\n",
       "  text-overflow: ellipsis;\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-var-name:hover,\n",
       ".xr-var-dims:hover,\n",
       ".xr-var-dtype:hover,\n",
       ".xr-attrs dt:hover {\n",
       "  overflow: visible;\n",
       "  width: auto;\n",
       "  z-index: 1;\n",
       "}\n",
       "\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  display: none;\n",
       "  background-color: var(--xr-background-color) !important;\n",
       "  padding-bottom: 5px !important;\n",
       "}\n",
       "\n",
       ".xr-var-attrs-in:checked ~ .xr-var-attrs,\n",
       ".xr-var-data-in:checked ~ .xr-var-data {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       ".xr-var-data > table {\n",
       "  float: right;\n",
       "}\n",
       "\n",
       ".xr-var-name span,\n",
       ".xr-var-data,\n",
       ".xr-attrs {\n",
       "  padding-left: 25px !important;\n",
       "}\n",
       "\n",
       ".xr-attrs,\n",
       ".xr-var-attrs,\n",
       ".xr-var-data {\n",
       "  grid-column: 1 / -1;\n",
       "}\n",
       "\n",
       "dl.xr-attrs {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  display: grid;\n",
       "  grid-template-columns: 125px auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt,\n",
       ".xr-attrs dd {\n",
       "  padding: 0;\n",
       "  margin: 0;\n",
       "  float: left;\n",
       "  padding-right: 10px;\n",
       "  width: auto;\n",
       "}\n",
       "\n",
       ".xr-attrs dt {\n",
       "  font-weight: normal;\n",
       "  grid-column: 1;\n",
       "}\n",
       "\n",
       ".xr-attrs dt:hover span {\n",
       "  display: inline-block;\n",
       "  background: var(--xr-background-color);\n",
       "  padding-right: 10px;\n",
       "}\n",
       "\n",
       ".xr-attrs dd {\n",
       "  grid-column: 2;\n",
       "  white-space: pre-wrap;\n",
       "  word-break: break-all;\n",
       "}\n",
       "\n",
       ".xr-icon-database,\n",
       ".xr-icon-file-text2 {\n",
       "  display: inline-block;\n",
       "  vertical-align: middle;\n",
       "  width: 1em;\n",
       "  height: 1.5em !important;\n",
       "  stroke-width: 0;\n",
       "  stroke: currentColor;\n",
       "  fill: currentColor;\n",
       "}\n",
       "</style><pre class='xr-text-repr-fallback'>&lt;xarray.DataArray &#x27;lat&#x27; (y: 48, x: 48)&gt;\n",
       "array([[-34.891106, -35.59881 , -36.285767, ..., -36.285767, -35.59881 ,\n",
       "        -34.891106],\n",
       "       [-33.414417, -34.10818 , -34.78246 , ..., -34.78246 , -34.10818 ,\n",
       "        -33.414417],\n",
       "       [-31.93678 , -32.614727, -33.27444 , ..., -33.27444 , -32.614727,\n",
       "        -31.93678 ],\n",
       "       ...,\n",
       "       [ 31.93678 ,  32.614727,  33.27444 , ...,  33.27444 ,  32.614727,\n",
       "         31.93678 ],\n",
       "       [ 33.414417,  34.10818 ,  34.78246 , ...,  34.78246 ,  34.10818 ,\n",
       "         33.414417],\n",
       "       [ 34.891106,  35.59881 ,  36.285767, ...,  36.285767,  35.59881 ,\n",
       "         34.891106]], dtype=float32)\n",
       "Coordinates:\n",
       "  * x        (x) float64 1.0 2.0 3.0 4.0 5.0 6.0 ... 44.0 45.0 46.0 47.0 48.0\n",
       "  * y        (y) float64 1.0 2.0 3.0 4.0 5.0 6.0 ... 44.0 45.0 46.0 47.0 48.0\n",
       "Attributes:\n",
       "    cell_methods:  time: point\n",
       "    long_name:     latitude\n",
       "    units:         degrees_N</pre><div class='xr-wrap' hidden><div class='xr-header'><div class='xr-obj-type'>xarray.DataArray</div><div class='xr-array-name'>'lat'</div><ul class='xr-dim-list'><li><span class='xr-has-index'>y</span>: 48</li><li><span class='xr-has-index'>x</span>: 48</li></ul></div><ul class='xr-sections'><li class='xr-section-item'><div class='xr-array-wrap'><input id='section-b2e720f0-d9ec-4daf-ab6c-921203083152' class='xr-array-in' type='checkbox' checked><label for='section-b2e720f0-d9ec-4daf-ab6c-921203083152' title='Show/hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-array-preview xr-preview'><span>-34.89 -35.6 -36.29 -36.95 -37.59 ... 37.59 36.95 36.29 35.6 34.89</span></div><div class='xr-array-data'><pre>array([[-34.891106, -35.59881 , -36.285767, ..., -36.285767, -35.59881 ,\n",
       "        -34.891106],\n",
       "       [-33.414417, -34.10818 , -34.78246 , ..., -34.78246 , -34.10818 ,\n",
       "        -33.414417],\n",
       "       [-31.93678 , -32.614727, -33.27444 , ..., -33.27444 , -32.614727,\n",
       "        -31.93678 ],\n",
       "       ...,\n",
       "       [ 31.93678 ,  32.614727,  33.27444 , ...,  33.27444 ,  32.614727,\n",
       "         31.93678 ],\n",
       "       [ 33.414417,  34.10818 ,  34.78246 , ...,  34.78246 ,  34.10818 ,\n",
       "         33.414417],\n",
       "       [ 34.891106,  35.59881 ,  36.285767, ...,  36.285767,  35.59881 ,\n",
       "         34.891106]], dtype=float32)</pre></div></div></li><li class='xr-section-item'><input id='section-7890cb7b-08e4-47f8-906a-dd84322056f8' class='xr-section-summary-in' type='checkbox'  checked><label for='section-7890cb7b-08e4-47f8-906a-dd84322056f8' class='xr-section-summary' >Coordinates: <span>(2)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><ul class='xr-var-list'><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>x</span></div><div class='xr-var-dims'>(x)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>1.0 2.0 3.0 4.0 ... 46.0 47.0 48.0</div><input id='attrs-4407afa5-51cd-4e0f-a5c0-a38fbdbf39ab' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-4407afa5-51cd-4e0f-a5c0-a38fbdbf39ab' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-d842f77e-288e-41bb-95ca-7e46309877d0' class='xr-var-data-in' type='checkbox'><label for='data-d842f77e-288e-41bb-95ca-7e46309877d0' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>cartesian_axis :</span></dt><dd>X</dd><dt><span>long_name :</span></dt><dd>T-cell longitude</dd><dt><span>units :</span></dt><dd>degrees_E</dd></dl></div><div class='xr-var-data'><pre>array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.,\n",
       "       15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28.,\n",
       "       29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42.,\n",
       "       43., 44., 45., 46., 47., 48.])</pre></div></li><li class='xr-var-item'><div class='xr-var-name'><span class='xr-has-index'>y</span></div><div class='xr-var-dims'>(y)</div><div class='xr-var-dtype'>float64</div><div class='xr-var-preview xr-preview'>1.0 2.0 3.0 4.0 ... 46.0 47.0 48.0</div><input id='attrs-33152da3-3c3f-42b2-9e45-18f8c5945aef' class='xr-var-attrs-in' type='checkbox' ><label for='attrs-33152da3-3c3f-42b2-9e45-18f8c5945aef' title='Show/Hide attributes'><svg class='icon xr-icon-file-text2'><use xlink:href='#icon-file-text2'></use></svg></label><input id='data-40faae66-35d2-42e7-ba29-5d67019c8915' class='xr-var-data-in' type='checkbox'><label for='data-40faae66-35d2-42e7-ba29-5d67019c8915' title='Show/Hide data repr'><svg class='icon xr-icon-database'><use xlink:href='#icon-database'></use></svg></label><div class='xr-var-attrs'><dl class='xr-attrs'><dt><span>cartesian_axis :</span></dt><dd>Y</dd><dt><span>long_name :</span></dt><dd>T-cell latitude</dd><dt><span>units :</span></dt><dd>degrees_N</dd></dl></div><div class='xr-var-data'><pre>array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14.,\n",
       "       15., 16., 17., 18., 19., 20., 21., 22., 23., 24., 25., 26., 27., 28.,\n",
       "       29., 30., 31., 32., 33., 34., 35., 36., 37., 38., 39., 40., 41., 42.,\n",
       "       43., 44., 45., 46., 47., 48.])</pre></div></li></ul></div></li><li class='xr-section-item'><input id='section-6438496a-13f0-4549-bc30-bdca00eb77f5' class='xr-section-summary-in' type='checkbox'  checked><label for='section-6438496a-13f0-4549-bc30-bdca00eb77f5' class='xr-section-summary' >Attributes: <span>(3)</span></label><div class='xr-section-inline-details'></div><div class='xr-section-details'><dl class='xr-attrs'><dt><span>cell_methods :</span></dt><dd>time: point</dd><dt><span>long_name :</span></dt><dd>latitude</dd><dt><span>units :</span></dt><dd>degrees_N</dd></dl></div></li></ul></div></div>"
      ],
      "text/plain": [
       "<xarray.DataArray 'lat' (y: 48, x: 48)>\n",
       "array([[-34.891106, -35.59881 , -36.285767, ..., -36.285767, -35.59881 ,\n",
       "        -34.891106],\n",
       "       [-33.414417, -34.10818 , -34.78246 , ..., -34.78246 , -34.10818 ,\n",
       "        -33.414417],\n",
       "       [-31.93678 , -32.614727, -33.27444 , ..., -33.27444 , -32.614727,\n",
       "        -31.93678 ],\n",
       "       ...,\n",
       "       [ 31.93678 ,  32.614727,  33.27444 , ...,  33.27444 ,  32.614727,\n",
       "         31.93678 ],\n",
       "       [ 33.414417,  34.10818 ,  34.78246 , ...,  34.78246 ,  34.10818 ,\n",
       "         33.414417],\n",
       "       [ 34.891106,  35.59881 ,  36.285767, ...,  36.285767,  35.59881 ,\n",
       "         34.891106]], dtype=float32)\n",
       "Coordinates:\n",
       "  * x        (x) float64 1.0 2.0 3.0 4.0 5.0 6.0 ... 44.0 45.0 46.0 47.0 48.0\n",
       "  * y        (y) float64 1.0 2.0 3.0 4.0 5.0 6.0 ... 44.0 45.0 46.0 47.0 48.0\n",
       "Attributes:\n",
       "    cell_methods:  time: point\n",
       "    long_name:     latitude\n",
       "    units:         degrees_N"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.lat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'grid/c24'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-917d4d9d74c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatalog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"grid/c24\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fv3net/lib/python3.8/site-packages/intake/catalog/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpart\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdiscover\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'grid/c24'"
     ]
    }
   ],
   "source": [
    "grid = catalog[\"grid/c24\"].read()\n",
    "grid.lat[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/ebrahimn/.dgl/aifb.tgz from https://data.dgl.ai/dataset/aifb.tgz...\n",
      "Loading dataset aifb\n",
      "Number of nodes:  8285\n",
      "Number of edges:  66371\n",
      "Number of relations:  91\n",
      "Number of classes:  4\n",
      "removing nodes that are more than 3 hops away\n"
     ]
    }
   ],
   "source": [
    "# load graph data\n",
    "from dgl.contrib.data import load_data\n",
    "import torch\n",
    "\n",
    "data = load_data(dataset='aifb')\n",
    "num_nodes = data.num_nodes\n",
    "num_rels = data.num_rels\n",
    "num_classes = data.num_classes\n",
    "labels = data.labels\n",
    "train_idx = data.train_idx\n",
    "# split training and validation set\n",
    "val_idx = train_idx[:len(train_idx) // 5]\n",
    "train_idx = train_idx[len(train_idx) // 5:]\n",
    "\n",
    "# edge type and normalization factor\n",
    "edge_type = torch.from_numpy(data.edge_type)\n",
    "edge_norm = torch.from_numpy(data.edge_norm).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "max() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * ()\n * (Tensor other)\n * (int dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: out, axis\n * (name dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: out, axis\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-9187dfb1f7d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/fv3net/lib/python3.8/site-packages/numpy/core/overrides.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fv3net/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2789\u001b[0m     \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2790\u001b[0m     \"\"\"\n\u001b[0;32m-> 2791\u001b[0;31m     return _wrapreduction(a, np.maximum, 'max', axis, None, out,\n\u001b[0m\u001b[1;32m   2792\u001b[0m                           keepdims=keepdims, initial=initial, where=where)\n\u001b[1;32m   2793\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fv3net/lib/python3.8/site-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: max() received an invalid combination of arguments - got (out=NoneType, axis=NoneType, ), but expected one of:\n * ()\n * (Tensor other)\n * (int dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: out, axis\n * (name dim, bool keepdim)\n      didn't match because some of the keywords were incorrect: out, axis\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.max(edge_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahimn/miniconda3/envs/fv3net/lib/python3.8/site-packages/dgl/heterograph.py:72: DGLWarning: Recommend creating graphs by `dgl.graph(data)` instead of `dgl.DGLGraph(data)`.\n",
      "  dgl_warning('Recommend creating graphs by `dgl.graph(data)`'\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "\n",
    "g = DGLGraph((data.edge_src, data.edge_dst))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   0, 4230, 5147,  ..., 4215, 5493, 8284]),\n",
       " tensor([   0,    0,    0,  ..., 8284, 8284, 8284]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.edges()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Graph(num_nodes=8285, num_edges=65439,\n",
       "      ndata_schemes={}\n",
       "      edata_schemes={})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "g.edata.update({'rel_type': edge_type, 'norm': edge_norm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65439"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(edge_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 55, 55,  0, 39,  0, 30, 18, 19,  8,  1,  6, 18, 19,  1,  6,  1,  6,\n",
       "        18, 19,  4, 24, 14, 15,  4,  1,  6,  8, 12,  8,  0, 10,  2,  5,  4,  0,\n",
       "        13,  0,  4,  1])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edge_type[0:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahimn/miniconda3/envs/dlwp2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "numebr of grids: 13824\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from dgl.nn import SAGEConv\n",
    "from dgl.nn.pytorch import GraphConv, ChebConv, GATConv\n",
    "import numpy as np\n",
    "import dask.diagnostics\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import time\n",
    "import select as sl\n",
    "import pickle\n",
    "from load_data import *\n",
    "from utils import *\n",
    "from Newmodel import *\n",
    "import wandb\n",
    "from fv3net.artifacts.resolve_url import resolve_url\n",
    "from vcm import get_fs\n",
    "\n",
    "lead=1\n",
    "\n",
    "lead2=1\n",
    "day=1\n",
    "control_str='TATNTATN'#'TNSTTNST' #'TNTSTNTST'\n",
    "epochs=10\n",
    "num_heads=2 \n",
    "\n",
    "variableList=['h500','h200','h850']\n",
    "TotalSamples=8500\n",
    "Chuncksize=500\n",
    "\n",
    "\n",
    "\n",
    "lr=0.001\n",
    "disablecuda ='store_true'\n",
    "batch_size=1\n",
    "window=24*day\n",
    "pred_len=1\n",
    "channels=[7, 16, 32, 64, 32, 128]\n",
    "drop_prob = 0\n",
    "out_feat=2\n",
    "\n",
    "savemodelpath = (\n",
    "    \"weight_layer_\"\n",
    "    + control_str\n",
    "    + \"_lag\"\n",
    "    + str(lead)\n",
    "    +\"_lead\"\n",
    "    +str(lead2)\n",
    "    + \"_window\"\n",
    "    + str(window)\n",
    "    + \"_epochs_\"\n",
    "    + str(epochs)\n",
    "    + \".pt\"\n",
    ")\n",
    "\n",
    "BUCKET = \"vcm-ml-experiments\"\n",
    "PROJECT = \"full-model-emulation\"\n",
    "\n",
    "model_out_url = resolve_url(BUCKET, PROJECT, savemodelpath)\n",
    "data_url = \"gs://vcm-ml-scratch/ebrahimn/2022-07-02/experiment-1-y/fv3gfs_run/\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_url = \"gs://vcm-ml-scratch/ebrahimn/2022-07-02/experiment-1-y/fv3gfs_run/\" \n",
    "state_training_data = xr.open_zarr(\n",
    "    fsspec.get_mapper(os.path.join(data_url, \"atmos_dt_atmos.zarr\"))\n",
    ")\n",
    "# state_training_data2 = xr.open_zarr(fsspec.get_mapper(os.path.join(data_url, 'sfc_dt_atmos.zarr')))\n",
    "lat_lon_data = xr.open_zarr(\n",
    "    fsspec.get_mapper(os.path.join(data_url, \"state_after_timestep.zarr\"))\n",
    ")\n",
    "\n",
    "landSea = xr.open_zarr(\n",
    "    fsspec.get_mapper(\n",
    "        \"gs://vcm-ml-experiments/default/2022-05-09/baseline-35day-spec-sst/fv3gfs_run/state_after_timestep.zarr\"\n",
    "    )\n",
    ")\n",
    "landSea_Mask = landSea.land_sea_mask[1].load()\n",
    "landSea_Mask = landSea_Mask[:, ::, ::].values.flatten()\n",
    "\n",
    "\n",
    "lat=(lat_lon_data.latitude[1].load())\n",
    "lon=(lat_lon_data.longitude[1].load())\n",
    "lat=lat[:,::,::].values.flatten()\n",
    "lon=lon[:,::,::].values.flatten()\n",
    "# cosLat=np.expand_dims(np.cos(lat),axis=1)\n",
    "# cosLon=np.expand_dims(np.cos(lon),axis=1)\n",
    "# sinLat=np.expand_dims(np.sin(lat),axis=1)\n",
    "# sinLon=np.expand_dims(np.sin(lon),axis=1)\n",
    "cosLat=np.cos(lat)\n",
    "cosLon=np.cos(lon)\n",
    "sinLat=np.sin(lat)\n",
    "sinLon=np.sin(lon)\n",
    "for i in range(3):\n",
    "        if i==0:\n",
    "            sinLon=torch.tensor(sinLon).unsqueeze(0).repeat(int(window/lead),1)\n",
    "            cosLon=torch.tensor(cosLon).unsqueeze(0).repeat(int(window/lead),1)\n",
    "            sinLat=torch.tensor(sinLat).unsqueeze(0).repeat(int(window/lead),1)\n",
    "            cosLat=torch.tensor(cosLat).unsqueeze(0).repeat(int(window/lead),1)\n",
    "            landSea_Mask=torch.tensor(landSea_Mask).unsqueeze(0).repeat(int(window/lead),1)\n",
    "        elif i==1:\n",
    "            sinLon=(sinLon).unsqueeze(0).repeat(1,1,1)\n",
    "            cosLon=(cosLon).unsqueeze(0).repeat(1,1,1)\n",
    "            sinLat=(sinLat).unsqueeze(0).repeat(1,1,1)\n",
    "            cosLat=(cosLat).unsqueeze(0).repeat(1,1,1)\n",
    "            landSea_Mask=(landSea_Mask).unsqueeze(0).repeat(1,1,1)\n",
    "\n",
    "        elif i==2:\n",
    "            sinLon=(sinLon).unsqueeze(0).repeat(batch_size,1,1,1)\n",
    "            cosLon=(cosLon).unsqueeze(0).repeat(batch_size,1,1,1)\n",
    "            sinLat=(sinLat).unsqueeze(0).repeat(batch_size,1,1,1)\n",
    "            cosLat=(cosLat).unsqueeze(0).repeat(batch_size,1,1,1)\n",
    "            landSea_Mask=(landSea_Mask).unsqueeze(0).repeat(batch_size,1,1,1)\n",
    "\n",
    "exteraVar=torch.cat((sinLon, sinLat,cosLon,cosLat,landSea_Mask), 1).to(device)\n",
    "\n",
    "num_nodes=len(lon)\n",
    "print(f\"numebr of grids: {num_nodes}\")\n",
    "\n",
    "\n",
    "\n",
    "g = pickle.load(open(\"UpdatedGraph\", 'rb'))\n",
    "loss = nn.MSELoss()\n",
    "g = g.to(device)\n",
    "model = STGCN_WAVE(channels, int(window/lead), out_feat, num_nodes, g, drop_prob, device, num_heads, control_str).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
    "model.train()\n",
    "\n",
    "Zmean=5765.8457   #Z500mean=5765.8457, \n",
    "Zstd=90.79599   #Z500std=90.79599\n",
    "\n",
    "Tmean=10643.382          #Thickmean=10643.382\n",
    "Tstd=162.12427              #Thickstd=162.12427\n",
    "valInde=0\n",
    "\n",
    "\n",
    "all_indices=np.random.permutation(np.arange(start=0, stop=int(TotalSamples/Chuncksize)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    for ss in all_indices:\n",
    "\n",
    "        Z500train=state_training_data[variableList[0]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize))\n",
    "        T2mtrain1=state_training_data[variableList[1]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize))\n",
    "        T2mtrain2=state_training_data[variableList[2]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize))\n",
    "\n",
    "        Z500train=np.swapaxes(Z500train.values, 1, 0)\n",
    "        T2mtrain1=np.swapaxes(T2mtrain1.values, 1, 0)\n",
    "        T2mtrain2=np.swapaxes(T2mtrain2.values, 1, 0)\n",
    "\n",
    "        T2mtrain=T2mtrain1-T2mtrain2\n",
    "\n",
    "        T2mtrain=T2mtrain.reshape(np.size(T2mtrain, 0), np.size(T2mtrain, 1)*np.size(T2mtrain, 2)*np.size(T2mtrain, 3))\n",
    "        Z500train=Z500train.reshape(np.size(Z500train, 0), np.size(Z500train, 1)*np.size(Z500train, 2)*np.size(Z500train, 3))\n",
    "\n",
    "        # Zmean = np.mean(Z500train)\n",
    "        # Zstd = np.std(Z500train)\n",
    "\n",
    "        # Tmean = np.mean(T2mtrain)\n",
    "        # Tstd = np.std(T2mtrain)\n",
    "\n",
    "\n",
    "        T2mtrain=(T2mtrain-Tmean)/Tstd\n",
    "        Z500train=(Z500train-Zmean)/Zstd\n",
    "\n",
    "        \n",
    "\n",
    "        T2mtrain=np.expand_dims(T2mtrain,axis=0)\n",
    "        Z500train=np.expand_dims(Z500train,axis=0)\n",
    "\n",
    "        dataSets=np.concatenate((Z500train,T2mtrain),axis=0)\n",
    "\n",
    "        num_samples=np.size(dataSets,1)\n",
    "        print(f\"Total samples: {num_samples}\")\n",
    "\n",
    "\n",
    "        len_val = round(num_samples * 0.25)\n",
    "        len_train = round(num_samples * 0.75)\n",
    "        train = dataSets[:,: len_train]\n",
    "        val = dataSets[:,len_train+14: len_train + len_val]\n",
    "\n",
    "        x_train, y_train = data_transform(train, window, pred_len, lead, lead2, device)\n",
    "        x_val, y_val = data_transform(val, window, pred_len, lead, lead2, device)\n",
    "        # x_test, y_test = data_transform(test, n_his, n_pred, device)\n",
    "        print('size of training:',np.shape(x_train),' size of validation',np.shape(x_val))\n",
    "        train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "        train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "        val_data = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "        val_iter = torch.utils.data.DataLoader(val_data, batch_size)\n",
    "        # test_data = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "        # test_iter = torch.utils.data.DataLoader(test_data, batch_size)\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        if valInde==0:\n",
    "            min_val_loss = np.inf\n",
    "            valInde+=1\n",
    "\n",
    "        l_sum, n = 0.0, 0\n",
    "        for x, y in train_iter:\n",
    "            exteraVar1=exteraVar[:x.size(0)]\n",
    "            x=torch.cat((x, exteraVar1), 1).float()\n",
    "            y_pred = model(x).view(len(x), out_feat, -1)\n",
    "            l = loss(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "            print(\"section \",ss,\" epoch\", epoch, \", train loss:\", l.item())\n",
    "\n",
    "        scheduler.step()\n",
    "        val_loss = evaluate_model(model, loss, val_iter,exteraVar,out_feat)\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), savemodelpath)\n",
    "        print(\"epoch\", epoch, \", train loss:\", l_sum / n, \", validation loss:\", val_loss)\n",
    "\n",
    "        fs = get_fs(model_out_url)\n",
    "        fs.put(savemodelpath, model_out_url)\n",
    "        print(savemodelpath, model_out_url)\n",
    "\n",
    "\n",
    "\n",
    "# best_model = STGCN_WAVE(channels, window, num_nodes, g, drop_prob, num_layers, device, control_str).to(device)\n",
    "# best_model.load_state_dict(torch.load(savemodelpath))\n",
    "\n",
    "# l = evaluate_model(best_model, loss, test_iter)\n",
    "# MAE, MAPE, RMSE = evaluate_metric(best_model, test_iter, scaler)\n",
    "# print(\"test loss:\", l, \"\\nMAE:\", MAE, \", MAPE:\", MAPE, \", RMSE:\", RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "        ss=0\n",
    "        Z500train=state_training_data[variableList[0]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize))\n",
    "        T2mtrain1=state_training_data[variableList[1]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize))\n",
    "        T2mtrain2=state_training_data[variableList[2]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize))\n",
    "\n",
    "        Z500train=np.swapaxes(Z500train.values, 1, 0)\n",
    "        T2mtrain1=np.swapaxes(T2mtrain1.values, 1, 0)\n",
    "        T2mtrain2=np.swapaxes(T2mtrain2.values, 1, 0)\n",
    "\n",
    "        T2mtrain=T2mtrain1-T2mtrain2\n",
    "\n",
    "        T2mtrain=T2mtrain.reshape(np.size(T2mtrain, 0), np.size(T2mtrain, 1)*np.size(T2mtrain, 2)*np.size(T2mtrain, 3))\n",
    "        Z500train=Z500train.reshape(np.size(Z500train, 0), np.size(Z500train, 1)*np.size(Z500train, 2)*np.size(Z500train, 3))\n",
    "\n",
    "\n",
    "        T2mtrain=(T2mtrain-Tmean)/Tstd\n",
    "        Z500train=(Z500train-Zmean)/Zstd\n",
    "\n",
    "        \n",
    "\n",
    "        T2mtrain=np.expand_dims(T2mtrain,axis=0)\n",
    "        Z500train=np.expand_dims(Z500train,axis=0)\n",
    "        dataSets=np.concatenate((Z500train,T2mtrain),axis=0)\n",
    "\n",
    "        num_samples=np.size(dataSets,1)\n",
    "        print(f\"Total samples: {num_samples}\")\n",
    "\n",
    "\n",
    "        len_train = round(num_samples * 0.75)\n",
    "        train = dataSets[:,: len_train]\n",
    "\n",
    "        x_train, y_train = data_transform(train, window, pred_len, lead, lead2, device)\n",
    "        print('size of training:',np.shape(x_train),' size of validation',np.shape(x_val))\n",
    "        train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "        train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)1\n",
    "\n",
    "        for x, y in train_iter:\n",
    "            exteraVar1=exteraVar[:x.size(0)]\n",
    "            x=torch.cat((x, exteraVar1), 1).float()\n",
    "            x.requires_grad_()\n",
    "            y_pred = model(x).view(len(x), out_feat, -1)\n",
    "            y_pred.sum().backward()\n",
    "\n",
    "            # print the gradient of random index  \n",
    "            print(x.grad.reshape(-1)[np.random.randint(0,x.grad.nelement())])\n",
    "\n",
    "            #make sure that none of the indices are zero\n",
    "            if np.any(x.grad.cpu().detach().numpy()==0):\n",
    "                print('Global dependency issue')\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch library\n",
    "import torch\n",
    "\n",
    "# create tensors with requires_grad = true\n",
    "x=torch.rand(3, 3)\n",
    "x.requires_grad_()\n",
    "A=torch.eye(3)\n",
    "\n",
    "\n",
    "y = A*x\n",
    "\n",
    "y.sum().backward()\n",
    "# Access the gradients using x.grad\n",
    "dx = x.grad\n",
    "print(\"x.grad :\", dx)\n",
    "\n",
    "if np.any(x.grad.cpu().detach().numpy()==0):\n",
    "    print('Global dependency issue')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPGNN\n",
      "numebr of grids: 1536\n",
      "Total samples: 500\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from dgl.nn import SAGEConv\n",
    "from dgl.nn.pytorch import GraphConv, ChebConv, GATConv\n",
    "import numpy as np\n",
    "import dask.diagnostics\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import time\n",
    "import select as sl\n",
    "import pickle\n",
    "from load_data import *\n",
    "from utils import *\n",
    "from Newmodel import *\n",
    "import wandb\n",
    "from fv3net.artifacts.resolve_url import resolve_url\n",
    "from vcm import get_fs\n",
    "\n",
    "lead=1\n",
    "lead2=4\n",
    "day=1\n",
    "coarsenInd=3\n",
    "\n",
    "control_str='MPGNN'#'TNSTTNST' #'TNTSTNTST'\n",
    "\n",
    "print(control_str)\n",
    "\n",
    "epochs=20\n",
    "num_heads=2 \n",
    "\n",
    "variableList=['h500','h200','h850']\n",
    "TotalSamples=8500\n",
    "Chuncksize=500\n",
    "\n",
    "lr=0.001\n",
    "disablecuda ='store_true'\n",
    "batch_size=2\n",
    "window=1\n",
    "pred_len=1\n",
    "out_feat=2\n",
    "\n",
    "savemodelpath = (\n",
    "    \"weight_layer_\"\n",
    "    + control_str\n",
    "    + \"_lag\"\n",
    "    + str(lead)\n",
    "    +\"_lead\"\n",
    "    +str(lead2)\n",
    "    + \"_window\"\n",
    "    + str(window)\n",
    "    + \"_epochs_\"\n",
    "    + str(epochs)\n",
    "    + \".pt\"\n",
    ")\n",
    "\n",
    "BUCKET = \"vcm-ml-experiments\"\n",
    "PROJECT = \"full-model-emulation\"\n",
    "\n",
    "model_out_url = resolve_url(BUCKET, PROJECT, savemodelpath)\n",
    "data_url = \"gs://vcm-ml-scratch/ebrahimn/2022-07-02/experiment-1-y/fv3gfs_run/\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_url = \"gs://vcm-ml-scratch/ebrahimn/2022-07-02/experiment-1-y/fv3gfs_run/\" \n",
    "state_training_data = xr.open_zarr(\n",
    "    fsspec.get_mapper(os.path.join(data_url, \"atmos_dt_atmos.zarr\")), consolidated=True\n",
    ")\n",
    "# state_training_data2 = xr.open_zarr(fsspec.get_mapper(os.path.join(data_url, 'sfc_dt_atmos.zarr')))\n",
    "lat_lon_data = xr.open_zarr(\n",
    "    fsspec.get_mapper(os.path.join(data_url, \"state_after_timestep.zarr\"))\n",
    ")\n",
    "\n",
    "landSea = xr.open_zarr(\n",
    "    fsspec.get_mapper(\n",
    "        \"gs://vcm-ml-experiments/default/2022-05-09/baseline-35day-spec-sst/fv3gfs_run/state_after_timestep.zarr\"\n",
    "    )\n",
    ")\n",
    "landSea_Mask = landSea.land_sea_mask[1].load()\n",
    "landSea_Mask = landSea_Mask[:, ::coarsenInd, ::coarsenInd].values.flatten()\n",
    "\n",
    "\n",
    "lat=(lat_lon_data.latitude[1].load())\n",
    "lon=(lat_lon_data.longitude[1].load())\n",
    "lat=lat[:,::coarsenInd,::coarsenInd].values.flatten()\n",
    "lon=lon[:,::coarsenInd,::coarsenInd].values.flatten()\n",
    "# cosLat=np.expand_dims(np.cos(lat),axis=1)\n",
    "# cosLon=np.expand_dims(np.cos(lon),axis=1)\n",
    "# sinLat=np.expand_dims(np.sin(lat),axis=1)\n",
    "# sinLon=np.expand_dims(np.sin(lon),axis=1)\n",
    "cosLat=np.cos(lat)\n",
    "cosLon=np.cos(lon)\n",
    "sinLat=np.sin(lat)\n",
    "sinLon=np.sin(lon)\n",
    "for i in range(3):\n",
    "        if i==0:\n",
    "            sinLon=torch.tensor(sinLon).unsqueeze(0).repeat(int(window/lead),1)\n",
    "            cosLon=torch.tensor(cosLon).unsqueeze(0).repeat(int(window/lead),1)\n",
    "            sinLat=torch.tensor(sinLat).unsqueeze(0).repeat(int(window/lead),1)\n",
    "            cosLat=torch.tensor(cosLat).unsqueeze(0).repeat(int(window/lead),1)\n",
    "            landSea_Mask=torch.tensor(landSea_Mask).unsqueeze(0).repeat(int(window/lead),1)\n",
    "        elif i==1:\n",
    "            sinLon=(sinLon).unsqueeze(0).repeat(1,1,1)\n",
    "            cosLon=(cosLon).unsqueeze(0).repeat(1,1,1)\n",
    "            sinLat=(sinLat).unsqueeze(0).repeat(1,1,1)\n",
    "            cosLat=(cosLat).unsqueeze(0).repeat(1,1,1)\n",
    "            landSea_Mask=(landSea_Mask).unsqueeze(0).repeat(1,1,1)\n",
    "\n",
    "        elif i==2:\n",
    "            sinLon=(sinLon).unsqueeze(0).repeat(batch_size,1,1,1)\n",
    "            cosLon=(cosLon).unsqueeze(0).repeat(batch_size,1,1,1)\n",
    "            sinLat=(sinLat).unsqueeze(0).repeat(batch_size,1,1,1)\n",
    "            cosLat=(cosLat).unsqueeze(0).repeat(batch_size,1,1,1)\n",
    "            landSea_Mask=(landSea_Mask).unsqueeze(0).repeat(batch_size,1,1,1)\n",
    "\n",
    "exteraVar=torch.cat((sinLon, sinLat,cosLon,cosLat,landSea_Mask), 1).to(device)\n",
    "\n",
    "num_nodes=len(lon)\n",
    "print(f\"numebr of grids: {num_nodes}\")\n",
    "\n",
    "\n",
    "\n",
    "g = pickle.load(open(\"UpdatedGraph_Neighbour10_Coarsen3\", 'rb'))\n",
    "loss = nn.MSELoss()\n",
    "g = g.to(device)\n",
    "\n",
    "Zmean=5765.8457   #Z500mean=5765.8457, \n",
    "Zstd=90.79599   #Z500std=90.79599\n",
    "\n",
    "Tmean=10643.382          #Thickmean=10643.382\n",
    "Tstd=162.12427              #Thickstd=162.12427\n",
    "valInde=0\n",
    "\n",
    "\n",
    "class MPNNGNN(nn.Module):\n",
    "    \"\"\"MPNN.\n",
    "\n",
    "    MPNN is introduced in `Neural Message Passing for Quantum Chemistry\n",
    "    <https://arxiv.org/abs/1704.01212>`__.\n",
    "\n",
    "    This class performs message passing in MPNN and returns the updated node representations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    node_in_feats : int\n",
    "        Size for the input node features.\n",
    "    node_out_feats : int\n",
    "        Size for the output node representations. Default to 64.\n",
    "    edge_in_feats : int\n",
    "        Size for the input edge features. Default to 128.\n",
    "    edge_hidden_feats : int\n",
    "        Size for the hidden edge representations.\n",
    "    num_step_message_passing : int\n",
    "        Number of message passing steps. Default to 6.\n",
    "    \"\"\"\n",
    "    def __init__(self, node_in_feats, edge_in_feats, node_hidden_feats=64,node_out_feats=64,\n",
    "                 edge_hidden_feats=128, num_step_message_passing=6):\n",
    "        super(MPNNGNN, self).__init__()\n",
    "\n",
    "        self.project_node_feats = nn.Sequential(\n",
    "            nn.Linear(node_in_feats, node_hidden_feats),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(node_hidden_feats, node_hidden_feats)\n",
    "        )\n",
    "        self.num_step_message_passing = num_step_message_passing\n",
    "        edge_network = nn.Sequential(\n",
    "            nn.Linear(edge_in_feats, edge_hidden_feats),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(edge_hidden_feats, node_hidden_feats * node_hidden_feats)\n",
    "        )\n",
    "        self.gnn_layer = NNConv(\n",
    "            in_feats=node_hidden_feats,\n",
    "            out_feats=node_hidden_feats,\n",
    "            edge_func=edge_network,\n",
    "            aggregator_type='sum'\n",
    "        )\n",
    "        self.gru = nn.GRU(node_hidden_feats, node_hidden_feats)\n",
    "\n",
    "        self.decoder = nn.Sequential(nn.Linear( node_hidden_feats , node_hidden_feats),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear( node_hidden_feats, node_out_feats)\n",
    "                              )\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize model parameters.\"\"\"\n",
    "        self.project_node_feats[0].reset_parameters()\n",
    "        self.gnn_layer.reset_parameters()\n",
    "        for layer in self.gnn_layer.edge_func:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.reset_parameters()\n",
    "        self.gru.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, g, node_feats, edge_feats):\n",
    "        \"\"\"Performs message passing and updates node representations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g : DGLGraph\n",
    "            DGLGraph for a batch of graphs.\n",
    "        node_feats : float32 tensor of shape (V, node_in_feats)\n",
    "            Input node features. V for the number of nodes in the batch of graphs.\n",
    "        edge_feats : float32 tensor of shape (E, edge_in_feats)\n",
    "            Input edge features. E for the number of edges in the batch of graphs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        node_feats : float32 tensor of shape (V, node_out_feats)\n",
    "            Output node representations.\n",
    "        \"\"\"\n",
    "        node_feats = self.project_node_feats(node_feats) # (V, node_out_feats)\n",
    "        hidden_feats = node_feats.unsqueeze(0)           # (1, V, node_out_feats)\n",
    "\n",
    "        for _ in range(self.num_step_message_passing):\n",
    "            # print(node_feats.shape)\n",
    "            node_feats = F.relu(self.gnn_layer(g, node_feats, edge_feats))\n",
    "            node_feats, hidden_feats = self.gru(node_feats.unsqueeze(0), hidden_feats)\n",
    "            node_feats = node_feats.squeeze(0)\n",
    "\n",
    "        return self.decoder(node_feats)\n",
    "        \n",
    "\n",
    "\n",
    "all_indices=np.random.permutation(np.arange(start=0, stop=int(TotalSamples/Chuncksize)))\n",
    "\n",
    "\n",
    "for epoch in range(1, 2):\n",
    "    \n",
    "        ss=0\n",
    "\n",
    "        Z500train=state_training_data[variableList[0]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "        T2mtrain1=state_training_data[variableList[1]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "        T2mtrain2=state_training_data[variableList[2]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "\n",
    "        Z500train=np.swapaxes(Z500train.values, 1, 0)\n",
    "        T2mtrain1=np.swapaxes(T2mtrain1.values, 1, 0)\n",
    "        T2mtrain2=np.swapaxes(T2mtrain2.values, 1, 0)\n",
    "\n",
    "        T2mtrain=T2mtrain1-T2mtrain2\n",
    "\n",
    "        T2mtrain=T2mtrain.reshape(np.size(T2mtrain, 0), np.size(T2mtrain, 1)*np.size(T2mtrain, 2)*np.size(T2mtrain, 3))\n",
    "        Z500train=Z500train.reshape(np.size(Z500train, 0), np.size(Z500train, 1)*np.size(Z500train, 2)*np.size(Z500train, 3))\n",
    "\n",
    "        # Zmean = np.mean(Z500train)\n",
    "        # Zstd = np.std(Z500train)\n",
    "\n",
    "        # Tmean = np.mean(T2mtrain)\n",
    "        # Tstd = np.std(T2mtrain)\n",
    "\n",
    "\n",
    "        T2mtrain=(T2mtrain-Tmean)/Tstd\n",
    "        Z500train=(Z500train-Zmean)/Zstd\n",
    "\n",
    "        \n",
    "\n",
    "        T2mtrain=np.expand_dims(T2mtrain,axis=0)\n",
    "        Z500train=np.expand_dims(Z500train,axis=0)\n",
    "\n",
    "        dataSets=np.concatenate((Z500train,T2mtrain),axis=0)\n",
    "\n",
    "        num_samples=np.size(dataSets,1)\n",
    "        print(f\"Total samples: {num_samples}\")\n",
    "\n",
    "\n",
    "        len_val = round(num_samples * 0.25)\n",
    "        len_train = round(num_samples * 0.75)\n",
    "        train = dataSets[:,: len_train]\n",
    "        val = dataSets[:,len_train+14: len_train + len_val]\n",
    "\n",
    "        x_train=train[:,0:-lead,:]\n",
    "        y_train=train[:,lead::,:]\n",
    "        x_val=val[:,0:-lead,:]\n",
    "        y_val=val[:,lead::,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=train[:,0:-lead,:]\n",
    "y_train=train[:,lead::,:]\n",
    "x_val=val[:,0:-lead,:]\n",
    "y_val=val[:,lead::,:]\n",
    "\n",
    "\n",
    "x_train=torch.Tensor(x_train).to(device)\n",
    "y_train=torch.Tensor(y_train).to(device)\n",
    "x_train=np.swapaxes(x_train, 1, 0)\n",
    "y_train=np.swapaxes(y_train, 1, 0)\n",
    "train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "x_val=torch.Tensor(x_val).to(device)\n",
    "y_val=torch.Tensor(y_val).to(device)\n",
    "x_val=np.swapaxes(x_val, 1, 0)\n",
    "y_val=np.swapaxes(y_val, 1, 0)\n",
    "val_data = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "val_iter = torch.utils.data.DataLoader(val_data, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 374, 1536)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 111, 1536)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=torch.Tensor(x_train).to(device)\n",
    "y_train=torch.Tensor(y_train).to(device)\n",
    "x_train=np.swapaxes(x_train, 1, 0)\n",
    "y_train=np.swapaxes(y_train, 1, 0)\n",
    "train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "x_val=torch.Tensor(x_val).to(device)\n",
    "y_val=torch.Tensor(y_val).to(device)\n",
    "x_val=np.swapaxes(x_val, 1, 0)\n",
    "y_val=np.swapaxes(y_val, 1, 0)\n",
    "val_data = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "val_iter = torch.utils.data.DataLoader(val_data, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([10, 2, 1536])\n",
      "torch.Size([4, 2, 1536])\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_iter:\n",
    "        print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahimn/miniconda3/envs/dlwp2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPGNN\n",
      "numebr of grids: 1536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1534820/3689726334.py:121: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  edg=np.asarray(g.edges())\n",
      "/tmp/ipykernel_1534820/3689726334.py:121: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  edg=np.asarray(g.edges())\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from dgl.nn import SAGEConv\n",
    "from dgl.nn.pytorch import NNConv\n",
    "import numpy as np\n",
    "import dask.diagnostics\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import time\n",
    "import select as sl\n",
    "import pickle\n",
    "from load_data import *\n",
    "from utils import *\n",
    "from Newmodel import *\n",
    "import wandb\n",
    "from fv3net.artifacts.resolve_url import resolve_url\n",
    "from vcm import get_fs\n",
    "\n",
    "lead=4\n",
    "coarsenInd=3\n",
    "control_str='MPGNN'#'TNSTTNST' #'TNTSTNTST'\n",
    "\n",
    "print(control_str)\n",
    "\n",
    "epochs=20\n",
    "num_heads=2 \n",
    "\n",
    "variableList=['h500','h200','h850']\n",
    "TotalSamples=8500\n",
    "Chuncksize=500\n",
    "\n",
    "\n",
    "\n",
    "lr=0.001\n",
    "disablecuda ='store_true'\n",
    "batch_size=1\n",
    "drop_prob = 0\n",
    "out_feat=2\n",
    "\n",
    "savemodelpath = (\n",
    "    \"weight_layer_\"\n",
    "    + control_str\n",
    "    + \"_lead\"\n",
    "    + str(lead)\n",
    "    + \"_epochs_\"\n",
    "    + str(epochs)\n",
    "    + \".pt\"\n",
    ")\n",
    "\n",
    "BUCKET = \"vcm-ml-experiments\"\n",
    "PROJECT = \"full-model-emulation\"\n",
    "\n",
    "model_out_url = resolve_url(BUCKET, PROJECT, savemodelpath)\n",
    "data_url = \"gs://vcm-ml-scratch/ebrahimn/2022-07-02/experiment-1-y/fv3gfs_run/\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_url = \"gs://vcm-ml-scratch/ebrahimn/2022-07-02/experiment-1-y/fv3gfs_run/\" \n",
    "state_training_data = xr.open_zarr(\n",
    "    fsspec.get_mapper(os.path.join(data_url, \"atmos_dt_atmos.zarr\")), consolidated=True\n",
    ")\n",
    "# state_training_data2 = xr.open_zarr(fsspec.get_mapper(os.path.join(data_url, 'sfc_dt_atmos.zarr')))\n",
    "lat_lon_data = xr.open_zarr(\n",
    "    fsspec.get_mapper(os.path.join(data_url, \"state_after_timestep.zarr\"))\n",
    ")\n",
    "\n",
    "landSea = xr.open_zarr(\n",
    "    fsspec.get_mapper(\n",
    "        \"gs://vcm-ml-experiments/default/2022-05-09/baseline-35day-spec-sst/fv3gfs_run/state_after_timestep.zarr\"\n",
    "    )\n",
    ")\n",
    "landSea_Mask = landSea.land_sea_mask[1].load()\n",
    "landSea_Mask = landSea_Mask[:, ::coarsenInd, ::coarsenInd].values.flatten()\n",
    "\n",
    "\n",
    "lat=(lat_lon_data.latitude[1].load())\n",
    "lon=(lat_lon_data.longitude[1].load())\n",
    "lat=lat[:,::coarsenInd,::coarsenInd].values.flatten()\n",
    "lon=lon[:,::coarsenInd,::coarsenInd].values.flatten()\n",
    "# cosLat=np.expand_dims(np.cos(lat),axis=1)\n",
    "# cosLon=np.expand_dims(np.cos(lon),axis=1)\n",
    "# sinLat=np.expand_dims(np.sin(lat),axis=1)\n",
    "# sinLon=np.expand_dims(np.sin(lon),axis=1)\n",
    "cosLat=np.cos(lat)\n",
    "cosLon=np.cos(lon)\n",
    "sinLat=np.sin(lat)\n",
    "sinLon=np.sin(lon)\n",
    "for i in range(3):\n",
    "        if i==0:\n",
    "            sinLon=torch.tensor(sinLon).unsqueeze(0).repeat(1,1)\n",
    "            cosLon=torch.tensor(cosLon).unsqueeze(0).repeat(1,1)\n",
    "            sinLat=torch.tensor(sinLat).unsqueeze(0).repeat(1,1)\n",
    "            cosLat=torch.tensor(cosLat).unsqueeze(0).repeat(1,1)\n",
    "            landSea_Mask=torch.tensor(landSea_Mask).unsqueeze(0).repeat(1,1)\n",
    "        elif i==2:\n",
    "            sinLon=(sinLon).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            cosLon=(cosLon).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            sinLat=(sinLat).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            cosLat=(cosLat).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            landSea_Mask=(landSea_Mask).unsqueeze(0).repeat(batch_size,1,1)\n",
    "\n",
    "exteraVar=torch.cat((sinLon, sinLat,cosLon,cosLat,landSea_Mask), 1).to(device)\n",
    "exteraVar=np.swapaxes(exteraVar,2, 1)\n",
    "\n",
    "num_nodes=len(lon)\n",
    "print(f\"numebr of grids: {num_nodes}\")\n",
    "\n",
    "\n",
    "\n",
    "g = pickle.load(open(\"UpdatedGraph_Neighbour10_Coarsen3\", 'rb'))\n",
    "\n",
    "edg=np.asarray(g.edges())\n",
    "latInd=lat[edg[1]]\n",
    "lonInd=lon[edg[1]]\n",
    "latlon=[latInd.T,lonInd.T]\n",
    "# latlon=np.swapaxes(latlon, 1, 0)\n",
    "latlon=torch.from_numpy(np.swapaxes(latlon, 1, 0)).float()\n",
    "\n",
    "\n",
    "\n",
    "Zmean=5765.8457   #Z500mean=5765.8457, \n",
    "Zstd=90.79599   #Z500std=90.79599\n",
    "\n",
    "Tmean=10643.382          #Thickmean=10643.382\n",
    "Tstd=162.12427              #Thickstd=162.12427\n",
    "valInde=0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# best_model = STGCN_WAVE(channels, window, num_nodes, g, drop_prob, num_layers, device, control_str).to(device)\n",
    "# best_model.load_state_dict(torch.load(savemodelpath))\n",
    "\n",
    "# l = evaluate_model(best_model, loss, test_iter)\n",
    "# MAE, MAPE, RMSE = evaluate_metric(best_model, test_iter, scaler)\n",
    "# print(\"test loss:\", l, \"\\nMAE:\", MAE, \", MAPE:\", MAPE, \", RMSE:\", RMSE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MPNNGNN(nn.Module):\n",
    "    \"\"\"MPNN.\n",
    "\n",
    "    MPNN is introduced in `Neural Message Passing for Quantum Chemistry\n",
    "    <https://arxiv.org/abs/1704.01212>`__.\n",
    "\n",
    "    This class performs message passing in MPNN and returns the updated node representations.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    node_in_feats : int\n",
    "        Size for the input node features.\n",
    "    node_out_feats : int\n",
    "        Size for the output node representations. Default to 64.\n",
    "    edge_in_feats : int\n",
    "        Size for the input edge features. Default to 128.\n",
    "    edge_hidden_feats : int\n",
    "        Size for the hidden edge representations.\n",
    "    num_step_message_passing : int\n",
    "        Number of message passing steps. Default to 6.\n",
    "    \"\"\"\n",
    "    def __init__(self, g, node_in_feats, edge_in_feats, node_hidden_feats=64,node_out_feats=64,\n",
    "                 edge_hidden_feats=128, num_step_message_passing=6):\n",
    "        super(MPNNGNN, self).__init__()\n",
    "\n",
    "        self.project_node_feats = nn.Sequential(\n",
    "            nn.Linear(node_in_feats, node_hidden_feats),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(node_hidden_feats, node_hidden_feats),\n",
    "            nn.LayerNorm(node_hidden_feats)\n",
    "        )\n",
    "        self.num_step_message_passing = num_step_message_passing\n",
    "        edge_network = nn.Sequential(\n",
    "            nn.Linear(edge_in_feats, edge_hidden_feats),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(edge_hidden_feats, node_hidden_feats * node_hidden_feats),\n",
    "            nn.LayerNorm(node_hidden_feats * node_hidden_feats)\n",
    "        )\n",
    "        self.gnn_layer = NNConv(\n",
    "            in_feats=node_hidden_feats,\n",
    "            out_feats=node_hidden_feats,\n",
    "            edge_func=edge_network,\n",
    "            aggregator_type='sum'\n",
    "        )\n",
    "        self.gru = nn.GRU(node_hidden_feats, node_hidden_feats)\n",
    "\n",
    "        self.decoder = nn.Sequential(nn.Linear( node_hidden_feats , node_hidden_feats),\n",
    "                              nn.ReLU(),\n",
    "                              nn.Linear( node_hidden_feats, node_out_feats)\n",
    "                              )\n",
    "        self.g=g\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        \"\"\"Reinitialize model parameters.\"\"\"\n",
    "        self.project_node_feats[0].reset_parameters()\n",
    "        self.gnn_layer.reset_parameters()\n",
    "        for layer in self.gnn_layer.edge_func:\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                layer.reset_parameters()\n",
    "        self.gru.reset_parameters()\n",
    "\n",
    "\n",
    "    def forward(self, node_feats, edge_feats):\n",
    "        \"\"\"Performs message passing and updates node representations.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        g : DGLGraph\n",
    "            DGLGraph for a batch of graphs.\n",
    "        node_feats : float32 tensor of shape (V, node_in_feats)\n",
    "            Input node features. V for the number of nodes in the batch of graphs.\n",
    "        edge_feats : float32 tensor of shape (E, edge_in_feats)\n",
    "            Input edge features. E for the number of edges in the batch of graphs.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        node_feats : float32 tensor of shape (V, node_out_feats)\n",
    "            Output node representations.\n",
    "        \"\"\"\n",
    "        node_feats = self.project_node_feats(node_feats) # (V, node_out_feats)\n",
    "        hidden_feats = node_feats.unsqueeze(0)           # (1, V, node_out_feats)\n",
    "\n",
    "        for _ in range(self.num_step_message_passing):\n",
    "            print(node_feats.shape)\n",
    "            print(hidden_feats.shape)\n",
    "\n",
    "            node_feats = F.relu(self.gnn_layer(self.g, node_feats, edge_feats))\n",
    "            node_feats, hidden_feats = self.gru(node_feats.unsqueeze(0), hidden_feats)\n",
    "            node_feats = node_feats.squeeze(0)\n",
    "\n",
    "        return self.decoder(node_feats)\n",
    "        \n",
    "\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "g = g.to(device)\n",
    "model = MPNNGNN(g,node_in_feats=7, edge_in_feats=2, edge_hidden_feats=256, node_hidden_feats=256, node_out_feats=2,num_step_message_passing=8).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
    "model.train()\n",
    "\n",
    "all_indices=np.random.permutation(np.arange(start=0, stop=int(TotalSamples/Chuncksize)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 500\n",
      "371\n",
      "Total samples: 500\n",
      "371\n",
      "Total samples: 500\n",
      "371\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000016vscode-remote?line=5'>6</a>\u001b[0m T2mtrain1\u001b[39m=\u001b[39mstate_training_data[variableList[\u001b[39m1\u001b[39m]]\u001b[39m.\u001b[39misel(time\u001b[39m=\u001b[39m\u001b[39mslice\u001b[39m((ss\u001b[39m*\u001b[39mChuncksize),(ss\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mChuncksize))\u001b[39m.\u001b[39mcoarsen(grid_yt\u001b[39m=\u001b[39mcoarsenInd)\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mcoarsen(grid_xt\u001b[39m=\u001b[39mcoarsenInd)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000016vscode-remote?line=6'>7</a>\u001b[0m T2mtrain2\u001b[39m=\u001b[39mstate_training_data[variableList[\u001b[39m2\u001b[39m]]\u001b[39m.\u001b[39misel(time\u001b[39m=\u001b[39m\u001b[39mslice\u001b[39m((ss\u001b[39m*\u001b[39mChuncksize),(ss\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mChuncksize))\u001b[39m.\u001b[39mcoarsen(grid_yt\u001b[39m=\u001b[39mcoarsenInd)\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mcoarsen(grid_xt\u001b[39m=\u001b[39mcoarsenInd)\u001b[39m.\u001b[39mmean()\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000016vscode-remote?line=8'>9</a>\u001b[0m Z500train\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mswapaxes(Z500train\u001b[39m.\u001b[39;49mvalues, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000016vscode-remote?line=9'>10</a>\u001b[0m T2mtrain1\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mswapaxes(T2mtrain1\u001b[39m.\u001b[39mvalues, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000016vscode-remote?line=10'>11</a>\u001b[0m T2mtrain2\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mswapaxes(T2mtrain2\u001b[39m.\u001b[39mvalues, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/xarray/core/dataarray.py:642\u001b[0m, in \u001b[0;36mDataArray.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    634\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalues\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    635\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[39m    The array's data as a numpy.ndarray.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39m    type does not support coercion like this (e.g. cupy).\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvariable\u001b[39m.\u001b[39;49mvalues\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/xarray/core/variable.py:512\u001b[0m, in \u001b[0;36mVariable.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    510\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalues\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    511\u001b[0m     \u001b[39m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[0;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m _as_array_or_item(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/xarray/core/variable.py:252\u001b[0m, in \u001b[0;36m_as_array_or_item\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_as_array_or_item\u001b[39m(data):\n\u001b[1;32m    239\u001b[0m     \u001b[39m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(data)\n\u001b[1;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    254\u001b[0m         \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/array/core.py:1689\u001b[0m, in \u001b[0;36mArray.__array__\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1689\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute()\n\u001b[1;32m   1690\u001b[0m     \u001b[39mif\u001b[39;00m dtype \u001b[39mand\u001b[39;00m x\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m dtype:\n\u001b[1;32m   1691\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mastype(dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/base.py:315\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    292\u001b[0m     \u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    316\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/base.py:603\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    601\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 603\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    604\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/local.py:557\u001b[0m, in \u001b[0;36mget_sync\u001b[0;34m(dsk, keys, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[39m\"\"\"A naive synchronous version of get_async\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \n\u001b[1;32m    554\u001b[0m \u001b[39mCan be useful for debugging.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    556\u001b[0m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mnum_workers\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)  \u001b[39m# if num_workers present, remove it\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m \u001b[39mreturn\u001b[39;00m get_async(\n\u001b[1;32m    558\u001b[0m     synchronous_executor\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m    559\u001b[0m     synchronous_executor\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m    560\u001b[0m     dsk,\n\u001b[1;32m    561\u001b[0m     keys,\n\u001b[1;32m    562\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    563\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mwaiting\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mready\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mrunning\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mfor\u001b[39;00m key, res_info, failed \u001b[39min\u001b[39;00m queue_get(queue)\u001b[39m.\u001b[39;49mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[39m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/concurrent/futures/_base.py:437\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    436\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/concurrent/futures/_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    388\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    390\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    392\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/local.py:542\u001b[0m, in \u001b[0;36mSynchronousExecutor.submit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m fut \u001b[39m=\u001b[39m Future()\n\u001b[1;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     fut\u001b[39m.\u001b[39mset_result(fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     fut\u001b[39m.\u001b[39mset_exception(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/local.py:238\u001b[0m, in \u001b[0;36mbatch_execute_tasks\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_execute_tasks\u001b[39m(it):\n\u001b[1;32m    235\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[39m    Batch computing of multiple tasks with `execute_task`\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[39mreturn\u001b[39;00m [execute_task(\u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m it]\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/local.py:238\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_execute_tasks\u001b[39m(it):\n\u001b[1;32m    235\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[39m    Batch computing of multiple tasks with `execute_task`\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[39mreturn\u001b[39;00m [execute_task(\u001b[39m*\u001b[39;49ma) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m it]\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/local.py:229\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    227\u001b[0m     failed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 229\u001b[0m     result \u001b[39m=\u001b[39m pack_exception(e, dumps)\n\u001b[1;32m    230\u001b[0m     failed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m key, result, failed\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     task, data \u001b[39m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, data)\n\u001b[1;32m    225\u001b[0m     \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m get_id()\n\u001b[1;32m    226\u001b[0m     result \u001b[39m=\u001b[39m dumps((result, \u001b[39mid\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m(_execute_task(a, cache) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/array/core.py:125\u001b[0m, in \u001b[0;36mgetter\u001b[0;34m(a, b, asarray, lock)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[39m# Below we special-case `np.matrix` to force a conversion to\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39m# `np.ndarray` and preserve original Dask behavior for `getter`,\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[39m# as for all purposes `np.matrix` is array-like and thus\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[39m# `is_arraylike` evaluates to `True` in that case.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m asarray \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m is_arraylike(c) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(c, np\u001b[39m.\u001b[39mmatrix)):\n\u001b[0;32m--> 125\u001b[0m         c \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(c)\n\u001b[1;32m    126\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m lock:\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/xarray/core/indexing.py:358\u001b[0m, in \u001b[0;36mImplicitToExplicitIndexingAdapter.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/xarray/core/indexing.py:522\u001b[0m, in \u001b[0;36mCopyOnWriteArray.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 522\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/xarray/core/indexing.py:423\u001b[0m, in \u001b[0;36mLazilyIndexedArray.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    422\u001b[0m     array \u001b[39m=\u001b[39m as_indexable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray)\n\u001b[0;32m--> 423\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(array[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey], dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/xarray/backends/zarr.py:73\u001b[0m, in \u001b[0;36mZarrArrayWrapper.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     71\u001b[0m array \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_array()\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, indexing\u001b[39m.\u001b[39mBasicIndexer):\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m array[key\u001b[39m.\u001b[39;49mtuple]\n\u001b[1;32m     74\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, indexing\u001b[39m.\u001b[39mVectorizedIndexer):\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m array\u001b[39m.\u001b[39mvindex[\n\u001b[1;32m     76\u001b[0m         indexing\u001b[39m.\u001b[39m_arrayize_vectorized_indexer(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape)\u001b[39m.\u001b[39mtuple\n\u001b[1;32m     77\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/zarr/core.py:788\u001b[0m, in \u001b[0;36mArray.__getitem__\u001b[0;34m(self, selection)\u001b[0m\n\u001b[1;32m    786\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvindex[selection]\n\u001b[1;32m    787\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 788\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_basic_selection(pure_selection, fields\u001b[39m=\u001b[39;49mfields)\n\u001b[1;32m    789\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/zarr/core.py:914\u001b[0m, in \u001b[0;36mArray.get_basic_selection\u001b[0;34m(self, selection, out, fields)\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_basic_selection_zd(selection\u001b[39m=\u001b[39mselection, out\u001b[39m=\u001b[39mout,\n\u001b[1;32m    912\u001b[0m                                         fields\u001b[39m=\u001b[39mfields)\n\u001b[1;32m    913\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 914\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_basic_selection_nd(selection\u001b[39m=\u001b[39;49mselection, out\u001b[39m=\u001b[39;49mout,\n\u001b[1;32m    915\u001b[0m                                         fields\u001b[39m=\u001b[39;49mfields)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/zarr/core.py:957\u001b[0m, in \u001b[0;36mArray._get_basic_selection_nd\u001b[0;34m(self, selection, out, fields)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_basic_selection_nd\u001b[39m(\u001b[39mself\u001b[39m, selection, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, fields\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    952\u001b[0m     \u001b[39m# implementation of basic selection for array with at least one dimension\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \n\u001b[1;32m    954\u001b[0m     \u001b[39m# setup indexer\u001b[39;00m\n\u001b[1;32m    955\u001b[0m     indexer \u001b[39m=\u001b[39m BasicIndexer(selection, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 957\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_selection(indexer\u001b[39m=\u001b[39;49mindexer, out\u001b[39m=\u001b[39;49mout, fields\u001b[39m=\u001b[39;49mfields)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/zarr/core.py:1247\u001b[0m, in \u001b[0;36mArray._get_selection\u001b[0;34m(self, indexer, out, fields)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_store, \u001b[39m\"\u001b[39m\u001b[39mgetitems\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m   1242\u001b[0m    \u001b[39many\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: x \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape)):\n\u001b[1;32m   1243\u001b[0m     \u001b[39m# sequentially get one key at a time from storage\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     \u001b[39mfor\u001b[39;00m chunk_coords, chunk_selection, out_selection \u001b[39min\u001b[39;00m indexer:\n\u001b[1;32m   1245\u001b[0m \n\u001b[1;32m   1246\u001b[0m         \u001b[39m# load chunk selection into output array\u001b[39;00m\n\u001b[0;32m-> 1247\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_chunk_getitem(chunk_coords, chunk_selection, out, out_selection,\n\u001b[1;32m   1248\u001b[0m                             drop_axes\u001b[39m=\u001b[39;49mindexer\u001b[39m.\u001b[39;49mdrop_axes, fields\u001b[39m=\u001b[39;49mfields)\n\u001b[1;32m   1249\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1250\u001b[0m     \u001b[39m# allow storage to get multiple items at once\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     lchunk_coords, lchunk_selection, lout_selection \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mindexer)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/zarr/core.py:1939\u001b[0m, in \u001b[0;36mArray._chunk_getitem\u001b[0;34m(self, chunk_coords, chunk_selection, out, out_selection, drop_axes, fields)\u001b[0m\n\u001b[1;32m   1935\u001b[0m ckey \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chunk_key(chunk_coords)\n\u001b[1;32m   1937\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1938\u001b[0m     \u001b[39m# obtain compressed data for chunk\u001b[39;00m\n\u001b[0;32m-> 1939\u001b[0m     cdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_store[ckey]\n\u001b[1;32m   1941\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m   1942\u001b[0m     \u001b[39m# chunk not initialized\u001b[39;00m\n\u001b[1;32m   1943\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fill_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/zarr/storage.py:717\u001b[0m, in \u001b[0;36mKVStore.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m--> 717\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mutable_mapping[key]\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/fsspec/mapping.py:137\u001b[0m, in \u001b[0;36mFSMap.__getitem__\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    135\u001b[0m k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_key_to_str(key)\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs\u001b[39m.\u001b[39;49mcat(k)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing_exceptions:\n\u001b[1;32m    139\u001b[0m     \u001b[39mif\u001b[39;00m default \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/fsspec/asyn.py:86\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     85\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m obj \u001b[39mor\u001b[39;00m args[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 86\u001b[0m     \u001b[39mreturn\u001b[39;00m sync(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloop, func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/fsspec/asyn.py:54\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m asyncio\u001b[39m.\u001b[39mrun_coroutine_threadsafe(_runner(event, coro, result, timeout), loop)\n\u001b[1;32m     52\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39m# this loops allows thread to get interrupted\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mif\u001b[39;00m event\u001b[39m.\u001b[39;49mwait(\u001b[39m1\u001b[39;49m):\n\u001b[1;32m     55\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    307\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    for ss in all_indices:\n",
    "\n",
    "        Z500train=state_training_data[variableList[0]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "        T2mtrain1=state_training_data[variableList[1]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "        T2mtrain2=state_training_data[variableList[2]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "\n",
    "        Z500train=np.swapaxes(Z500train.values, 1, 0)\n",
    "        T2mtrain1=np.swapaxes(T2mtrain1.values, 1, 0)\n",
    "        T2mtrain2=np.swapaxes(T2mtrain2.values, 1, 0)\n",
    "\n",
    "        T2mtrain=T2mtrain1-T2mtrain2\n",
    "\n",
    "        T2mtrain=T2mtrain.reshape(np.size(T2mtrain, 0), np.size(T2mtrain, 1)*np.size(T2mtrain, 2)*np.size(T2mtrain, 3))\n",
    "        Z500train=Z500train.reshape(np.size(Z500train, 0), np.size(Z500train, 1)*np.size(Z500train, 2)*np.size(Z500train, 3))\n",
    "\n",
    "        # Zmean = np.mean(Z500train)\n",
    "        # Zstd = np.std(Z500train)\n",
    "\n",
    "        # Tmean = np.mean(T2mtrain)\n",
    "        # Tstd = np.std(T2mtrain)\n",
    "\n",
    "\n",
    "        T2mtrain=(T2mtrain-Tmean)/Tstd\n",
    "        Z500train=(Z500train-Zmean)/Zstd\n",
    "\n",
    "        \n",
    "\n",
    "        T2mtrain=np.expand_dims(T2mtrain,axis=0)\n",
    "        Z500train=np.expand_dims(Z500train,axis=0)\n",
    "\n",
    "        dataSets=np.concatenate((Z500train,T2mtrain),axis=0)\n",
    "\n",
    "        num_samples=np.size(dataSets,1)\n",
    "        print(f\"Total samples: {num_samples}\")\n",
    "\n",
    "\n",
    "        len_val = round(num_samples * 0.25)\n",
    "        len_train = round(num_samples * 0.75)\n",
    "        train = dataSets[:,: len_train]\n",
    "        val = dataSets[:,len_train+14: len_train + len_val]\n",
    "\n",
    "        x_train=train[:,0:-lead,:]\n",
    "        y_train=train[:,lead::,:]\n",
    "        x_val=val[:,0:-lead,:]\n",
    "        y_val=val[:,lead::,:]\n",
    "\n",
    "\n",
    "        x_train=torch.Tensor(x_train).to(device)\n",
    "        y_train=torch.Tensor(y_train).to(device)\n",
    "        x_train=np.swapaxes(x_train, 1, 0)\n",
    "        y_train=np.swapaxes(y_train, 1, 0)\n",
    "        x_train=np.swapaxes(x_train, 2, 1)\n",
    "        y_train=np.swapaxes(y_train, 2, 1)\n",
    "\n",
    "        train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "        train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "        x_val=torch.Tensor(x_val).to(device)\n",
    "        y_val=torch.Tensor(y_val).to(device)\n",
    "        x_val=np.swapaxes(x_val, 1, 0)\n",
    "        y_val=np.swapaxes(y_val, 1, 0)\n",
    "        x_val=np.swapaxes(x_val, 2, 1)\n",
    "        y_val=np.swapaxes(y_val, 2, 1)\n",
    "\n",
    "        val_data = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "        val_iter = torch.utils.data.DataLoader(val_data, batch_size)\n",
    "        \n",
    "\n",
    "        if valInde==0:\n",
    "            min_val_loss = np.inf\n",
    "            valInde+=1\n",
    "\n",
    "        l_sum, n = 0.0, 0\n",
    "        cc=0\n",
    "        for x, y in train_iter:\n",
    "            cc+=1\n",
    "        print(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    for ss in all_indices:\n",
    "\n",
    "        Z500train=state_training_data[variableList[0]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "        T2mtrain1=state_training_data[variableList[1]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "        T2mtrain2=state_training_data[variableList[2]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "\n",
    "        Z500train=np.swapaxes(Z500train.values, 1, 0)\n",
    "        T2mtrain1=np.swapaxes(T2mtrain1.values, 1, 0)\n",
    "        T2mtrain2=np.swapaxes(T2mtrain2.values, 1, 0)\n",
    "\n",
    "        T2mtrain=T2mtrain1-T2mtrain2\n",
    "\n",
    "        T2mtrain=T2mtrain.reshape(np.size(T2mtrain, 0), np.size(T2mtrain, 1)*np.size(T2mtrain, 2)*np.size(T2mtrain, 3))\n",
    "        Z500train=Z500train.reshape(np.size(Z500train, 0), np.size(Z500train, 1)*np.size(Z500train, 2)*np.size(Z500train, 3))\n",
    "\n",
    "        # Zmean = np.mean(Z500train)\n",
    "        # Zstd = np.std(Z500train)\n",
    "\n",
    "        # Tmean = np.mean(T2mtrain)\n",
    "        # Tstd = np.std(T2mtrain)\n",
    "\n",
    "\n",
    "        T2mtrain=(T2mtrain-Tmean)/Tstd\n",
    "        Z500train=(Z500train-Zmean)/Zstd\n",
    "\n",
    "        \n",
    "\n",
    "        T2mtrain=np.expand_dims(T2mtrain,axis=0)\n",
    "        Z500train=np.expand_dims(Z500train,axis=0)\n",
    "\n",
    "        dataSets=np.concatenate((Z500train,T2mtrain),axis=0)\n",
    "\n",
    "        num_samples=np.size(dataSets,1)\n",
    "        print(f\"Total samples: {num_samples}\")\n",
    "\n",
    "\n",
    "        len_val = round(num_samples * 0.25)\n",
    "        len_train = round(num_samples * 0.75)\n",
    "        train = dataSets[:,: len_train]\n",
    "        val = dataSets[:,len_train+14: len_train + len_val]\n",
    "\n",
    "        x_train=train[:,0:-lead,:]\n",
    "        y_train=train[:,lead::,:]\n",
    "        x_val=val[:,0:-lead,:]\n",
    "        y_val=val[:,lead::,:]\n",
    "\n",
    "\n",
    "        x_train=torch.Tensor(x_train).to(device)\n",
    "        y_train=torch.Tensor(y_train).to(device)\n",
    "        x_train=np.swapaxes(x_train, 1, 0)\n",
    "        y_train=np.swapaxes(y_train, 1, 0)\n",
    "        x_train=np.swapaxes(x_train, 2, 1)\n",
    "        y_train=np.swapaxes(y_train, 2, 1)\n",
    "\n",
    "        train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "        train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "        x_val=torch.Tensor(x_val).to(device)\n",
    "        y_val=torch.Tensor(y_val).to(device)\n",
    "        x_val=np.swapaxes(x_val, 1, 0)\n",
    "        y_val=np.swapaxes(y_val, 1, 0)\n",
    "        x_val=np.swapaxes(x_val, 2, 1)\n",
    "        y_val=np.swapaxes(y_val, 2, 1)\n",
    "\n",
    "        val_data = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "        val_iter = torch.utils.data.DataLoader(val_data, batch_size)\n",
    "        \n",
    "\n",
    "        if valInde==0:\n",
    "            min_val_loss = np.inf\n",
    "            valInde+=1\n",
    "\n",
    "        l_sum, n = 0.0, 0\n",
    "        for x, y in train_iter:\n",
    "            exteraVar1=exteraVar[:x.size(0)]\n",
    "            x=torch.squeeze(torch.cat((x, exteraVar1), 2)).float()\n",
    "            y_pred = model(x,latlon).view(len(x), -1 ,out_feat)\n",
    "            l = loss(y_pred, y)\n",
    "            optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            optimizer.step()\n",
    "            l_sum += l.item() * y.shape[0]\n",
    "            n += y.shape[0]\n",
    "            print(\"section \",ss,\" epoch\", epoch, \", train loss:\", l.item())\n",
    "\n",
    "        scheduler.step()\n",
    "        val_loss = evaluate_model(model, loss, val_iter,exteraVar,out_feat)\n",
    "        if val_loss < min_val_loss:\n",
    "            min_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), savemodelpath)\n",
    "        print(\"epoch\", epoch, \", train loss:\", l_sum / n, \", validation loss:\", val_loss)\n",
    "\n",
    "        fs = get_fs(model_out_url)\n",
    "        fs.put(savemodelpath, model_out_url)\n",
    "        print(savemodelpath, model_out_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'squeeze' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb Cell 14'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000013vscode-remote?line=0'>1</a>\u001b[0m x\u001b[39m=\u001b[39msqueeze(x)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'squeeze' is not defined"
     ]
    }
   ],
   "source": [
    "x=squeeze(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'builtin_function_or_method' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000007vscode-remote?line=0'>1</a>\u001b[0m x\u001b[39m.\u001b[39;49mshape\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'builtin_function_or_method' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiSAGEUnet\n",
      "weight_layer_MultiSAGEUnet_lead6_epochs_50MP_Block_15aggregat_meancoarsen_1residual_0.pt\n",
      "cpu\n",
      "numebr of grids: 13824\n",
      "loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1713075/3128626329.py:135: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  edg=np.asarray(g.edges())\n",
      "/tmp/ipykernel_1713075/3128626329.py:135: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  edg=np.asarray(g.edges())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 20\n",
      "tensor([[[-0.1136,  0.0388],\n",
      "         [-0.0921,  0.0382],\n",
      "         [-0.0869,  0.0354],\n",
      "         ...,\n",
      "         [ 0.0797,  0.0729],\n",
      "         [ 0.0854,  0.0642],\n",
      "         [ 0.0697,  0.0870]]])\n",
      "tensor([[[ 0.0280,  0.0669],\n",
      "         [ 0.0654,  0.0742],\n",
      "         [ 0.0707,  0.0749],\n",
      "         ...,\n",
      "         [ 0.0078,  0.0728],\n",
      "         [ 0.0075,  0.0579],\n",
      "         [-0.0144,  0.0727]]])\n",
      "tensor([[[-0.0476,  0.0615],\n",
      "         [-0.0156,  0.0670],\n",
      "         [-0.0059,  0.0653],\n",
      "         ...,\n",
      "         [ 0.0520,  0.0833],\n",
      "         [ 0.0509,  0.0651],\n",
      "         [ 0.0335,  0.0809]]])\n",
      "Total samples: 20\n",
      "tensor([[[-0.0943, -0.0038],\n",
      "         [-0.1135, -0.0195],\n",
      "         [-0.1123, -0.0122],\n",
      "         ...,\n",
      "         [-0.0109,  0.0399],\n",
      "         [-0.0155,  0.0038],\n",
      "         [-0.0407, -0.0448]]])\n",
      "tensor([[[-0.1116,  0.0241],\n",
      "         [-0.1303,  0.0131],\n",
      "         [-0.1294,  0.0196],\n",
      "         ...,\n",
      "         [-0.0305, -0.0294],\n",
      "         [ 0.0013, -0.0069],\n",
      "         [-0.0247, -0.0700]]])\n",
      "tensor([[[-0.0393, -0.0221],\n",
      "         [-0.0659, -0.0356],\n",
      "         [-0.0602, -0.0286],\n",
      "         ...,\n",
      "         [-0.0208,  0.0592],\n",
      "         [-0.0340,  0.0167],\n",
      "         [-0.0535, -0.0053]]])\n",
      "Total samples: 20\n",
      "tensor([[[ 0.0245, -0.0607],\n",
      "         [ 0.0585,  0.0049],\n",
      "         [ 0.0725,  0.0120],\n",
      "         ...,\n",
      "         [-0.0187, -0.0367],\n",
      "         [-0.0563, -0.0286],\n",
      "         [-0.0840, -0.0220]]])\n",
      "tensor([[[ 0.0827, -0.0895],\n",
      "         [ 0.0881, -0.0351],\n",
      "         [ 0.1071, -0.0215],\n",
      "         ...,\n",
      "         [ 0.0082,  0.0058],\n",
      "         [-0.0323,  0.0251],\n",
      "         [-0.0689,  0.0291]]])\n",
      "tensor([[[ 0.0619, -0.0846],\n",
      "         [ 0.0781, -0.0241],\n",
      "         [ 0.0916, -0.0105],\n",
      "         ...,\n",
      "         [-0.0085, -0.0107],\n",
      "         [-0.0457,  0.0015],\n",
      "         [-0.0790,  0.0040]]])\n",
      "Total samples: 20\n",
      "tensor([[[-0.0828,  0.0262],\n",
      "         [-0.0913,  0.0320],\n",
      "         [-0.0736,  0.0245],\n",
      "         ...,\n",
      "         [ 0.0611,  0.1023],\n",
      "         [ 0.0554,  0.0811],\n",
      "         [ 0.0536,  0.0760]]])\n",
      "tensor([[[-0.0063,  0.0381],\n",
      "         [-0.0160,  0.0408],\n",
      "         [ 0.0001,  0.0281],\n",
      "         ...,\n",
      "         [ 0.0357,  0.0918],\n",
      "         [ 0.0338,  0.0815],\n",
      "         [ 0.0306,  0.0708]]])\n",
      "tensor([[[-1.0592e-01,  8.4326e-05],\n",
      "         [-1.2069e-01,  2.1257e-02],\n",
      "         [-1.0278e-01,  1.2782e-02],\n",
      "         ...,\n",
      "         [ 5.8629e-02,  7.7872e-02],\n",
      "         [ 5.9344e-02,  7.2156e-02],\n",
      "         [ 6.4947e-02,  7.0584e-02]]])\n",
      "Total samples: 20\n",
      "tensor([[[ 0.1686,  0.1216],\n",
      "         [ 0.2014,  0.1330],\n",
      "         [ 0.1650,  0.1059],\n",
      "         ...,\n",
      "         [-0.0587, -0.0519],\n",
      "         [-0.0589, -0.0620],\n",
      "         [-0.0267, -0.0390]]])\n",
      "tensor([[[ 0.1519,  0.1115],\n",
      "         [ 0.1810,  0.1263],\n",
      "         [ 0.1546,  0.1067],\n",
      "         ...,\n",
      "         [-0.0465, -0.0495],\n",
      "         [-0.0526, -0.0479],\n",
      "         [-0.0177, -0.0218]]])\n",
      "tensor([[[ 0.1408,  0.1238],\n",
      "         [ 0.1784,  0.1356],\n",
      "         [ 0.1459,  0.1067],\n",
      "         ...,\n",
      "         [-0.0479, -0.0720],\n",
      "         [-0.0424, -0.0821],\n",
      "         [-0.0081, -0.0614]]])\n",
      "Total samples: 20\n",
      "tensor([[[-0.0382,  0.0694],\n",
      "         [-0.0205,  0.0710],\n",
      "         [-0.0205,  0.0670],\n",
      "         ...,\n",
      "         [ 0.0229, -0.0871],\n",
      "         [ 0.0338, -0.0554],\n",
      "         [ 0.0287,  0.0061]]])\n",
      "tensor([[[ 0.0219,  0.0814],\n",
      "         [ 0.0410,  0.0873],\n",
      "         [ 0.0343,  0.0697],\n",
      "         ...,\n",
      "         [ 0.0066, -0.0564],\n",
      "         [ 0.0128, -0.0465],\n",
      "         [ 0.0010,  0.0166]]])\n",
      "tensor([[[-0.0781,  0.0441],\n",
      "         [-0.0651,  0.0486],\n",
      "         [-0.0590,  0.0565],\n",
      "         ...,\n",
      "         [ 0.0333, -0.1059],\n",
      "         [ 0.0324, -0.0438],\n",
      "         [ 0.0312, -0.0263]]])\n",
      "Total samples: 20\n",
      "tensor([[[ 0.0279, -0.1201],\n",
      "         [ 0.1053, -0.2269],\n",
      "         [ 0.1397, -0.1435],\n",
      "         ...,\n",
      "         [ 0.0002, -0.0459],\n",
      "         [ 0.0064, -0.0128],\n",
      "         [-0.0038, -0.0044]]])\n",
      "tensor([[[ 0.0515, -0.0939],\n",
      "         [ 0.1480, -0.1388],\n",
      "         [ 0.1581,  0.0113],\n",
      "         ...,\n",
      "         [-0.0354, -0.0368],\n",
      "         [-0.0215, -0.0208],\n",
      "         [-0.0309, -0.0168]]])\n",
      "tensor([[[ 0.0626, -0.1014],\n",
      "         [ 0.1432, -0.1872],\n",
      "         [ 0.1637, -0.0841],\n",
      "         ...,\n",
      "         [-0.0252, -0.0397],\n",
      "         [-0.0149, -0.0129],\n",
      "         [-0.0270, -0.0149]]])\n",
      "Total samples: 20\n",
      "tensor([[[-0.0651,  0.0063],\n",
      "         [-0.0645,  0.0080],\n",
      "         [-0.0369,  0.0684],\n",
      "         ...,\n",
      "         [-0.0041,  0.0028],\n",
      "         [ 0.0255, -0.0405],\n",
      "         [-0.0286, -0.1548]]])\n",
      "tensor([[[-0.0129,  0.0210],\n",
      "         [-0.0089,  0.0038],\n",
      "         [ 0.0211,  0.0763],\n",
      "         ...,\n",
      "         [-0.0316, -0.0214],\n",
      "         [ 0.0021, -0.0672],\n",
      "         [-0.0729, -0.2004]]])\n",
      "tensor([[[-0.0777, -0.0105],\n",
      "         [-0.0805,  0.0025],\n",
      "         [-0.0570,  0.0374],\n",
      "         ...,\n",
      "         [ 0.0037,  0.0122],\n",
      "         [ 0.0293, -0.0067],\n",
      "         [ 0.0050, -0.1061]]])\n",
      "Total samples: 20\n",
      "tensor([[[-0.0849,  0.0336],\n",
      "         [-0.1329, -0.1067],\n",
      "         [-0.1031, -0.0006],\n",
      "         ...,\n",
      "         [ 0.0853, -0.0591],\n",
      "         [ 0.0899, -0.0318],\n",
      "         [ 0.0715, -0.0330]]])\n",
      "tensor([[[-0.0294,  0.0162],\n",
      "         [-0.0758, -0.1176],\n",
      "         [-0.0499, -0.0165],\n",
      "         ...,\n",
      "         [ 0.0258, -0.0927],\n",
      "         [ 0.0326, -0.0502],\n",
      "         [ 0.0079, -0.0552]]])\n",
      "tensor([[[-0.1217,  0.0074],\n",
      "         [-0.1703, -0.1158],\n",
      "         [-0.1432,  0.0270],\n",
      "         ...,\n",
      "         [ 0.1304, -0.0387],\n",
      "         [ 0.1306, -0.0415],\n",
      "         [ 0.1106, -0.0381]]])\n",
      "Total samples: 20\n",
      "tensor([[[-0.0203,  0.2513],\n",
      "         [ 0.0862,  0.3361],\n",
      "         [ 0.1116,  0.3067],\n",
      "         ...,\n",
      "         [ 0.0196, -0.0457],\n",
      "         [ 0.0424,  0.0714],\n",
      "         [-0.0688,  0.1406]]])\n",
      "tensor([[[-0.0518,  0.2211],\n",
      "         [ 0.0377,  0.2992],\n",
      "         [ 0.0709,  0.2910],\n",
      "         ...,\n",
      "         [ 0.0583, -0.0260],\n",
      "         [ 0.0747,  0.0912],\n",
      "         [-0.0363,  0.1437]]])\n",
      "tensor([[[-0.0412,  0.2003],\n",
      "         [ 0.0185,  0.2846],\n",
      "         [ 0.0617,  0.2602],\n",
      "         ...,\n",
      "         [ 0.0905, -0.0037],\n",
      "         [ 0.1023,  0.1084],\n",
      "         [ 0.0038,  0.1463]]])\n",
      "Total samples: 20\n",
      "tensor([[[ 0.1031,  0.0432],\n",
      "         [ 0.0891,  0.0275],\n",
      "         [ 0.0793,  0.0159],\n",
      "         ...,\n",
      "         [-0.0799,  0.0126],\n",
      "         [-0.0955,  0.0034],\n",
      "         [-0.1007, -0.0017]]])\n",
      "tensor([[[ 0.1347,  0.0558],\n",
      "         [ 0.1183,  0.0287],\n",
      "         [ 0.1087,  0.0055],\n",
      "         ...,\n",
      "         [-0.0991,  0.0132],\n",
      "         [-0.1110,  0.0028],\n",
      "         [-0.1149, -0.0030]]])\n",
      "tensor([[[ 0.1107,  0.0475],\n",
      "         [ 0.0960,  0.0095],\n",
      "         [ 0.0913, -0.0314],\n",
      "         ...,\n",
      "         [-0.1083,  0.0085],\n",
      "         [-0.1246,  0.0023],\n",
      "         [-0.1247, -0.0065]]])\n",
      "Total samples: 20\n",
      "tensor([[[ 0.2193,  0.1046],\n",
      "         [ 0.2390,  0.1448],\n",
      "         [ 0.2200,  0.1273],\n",
      "         ...,\n",
      "         [-0.1156,  0.0380],\n",
      "         [-0.1360,  0.0293],\n",
      "         [-0.1281,  0.0461]]])\n",
      "tensor([[[ 0.1894,  0.0761],\n",
      "         [ 0.2164,  0.1317],\n",
      "         [ 0.1989,  0.1346],\n",
      "         ...,\n",
      "         [-0.1109,  0.0519],\n",
      "         [-0.1255,  0.0415],\n",
      "         [-0.1106,  0.0546]]])\n",
      "tensor([[[ 0.2195,  0.1191],\n",
      "         [ 0.2324,  0.1489],\n",
      "         [ 0.2131,  0.1189],\n",
      "         ...,\n",
      "         [-0.1336,  0.0088],\n",
      "         [-0.1459,  0.0078],\n",
      "         [-0.1400,  0.0297]]])\n",
      "Total samples: 20\n",
      "tensor([[[ 0.1624,  0.0955],\n",
      "         [ 0.2014, -0.0104],\n",
      "         [ 0.2002,  0.0318],\n",
      "         ...,\n",
      "         [-0.0482,  0.0433],\n",
      "         [-0.0651,  0.0219],\n",
      "         [-0.0806,  0.0369]]])\n",
      "tensor([[[ 0.1304,  0.0813],\n",
      "         [ 0.1740, -0.0184],\n",
      "         [ 0.1736,  0.0253],\n",
      "         ...,\n",
      "         [-0.0852,  0.0065],\n",
      "         [-0.0913,  0.0076],\n",
      "         [-0.0978, -0.0281]]])\n",
      "tensor([[[ 0.1592,  0.0744],\n",
      "         [ 0.2003, -0.0198],\n",
      "         [ 0.2028,  0.0108],\n",
      "         ...,\n",
      "         [-0.0659,  0.0292],\n",
      "         [-0.0797,  0.0322],\n",
      "         [-0.0911,  0.0223]]])\n",
      "Total samples: 20\n",
      "tensor([[[-0.0183,  0.0544],\n",
      "         [ 0.0373,  0.0803],\n",
      "         [ 0.0383,  0.0928],\n",
      "         ...,\n",
      "         [ 0.1255,  0.0512],\n",
      "         [ 0.1274,  0.0524],\n",
      "         [ 0.1320,  0.0657]]])\n",
      "tensor([[[0.0503, 0.0743],\n",
      "         [0.1072, 0.0912],\n",
      "         [0.1037, 0.1045],\n",
      "         ...,\n",
      "         [0.0701, 0.0514],\n",
      "         [0.0707, 0.0339],\n",
      "         [0.0785, 0.0612]]])\n",
      "tensor([[[-0.0498,  0.0419],\n",
      "         [ 0.0024,  0.0706],\n",
      "         [ 0.0107,  0.0797],\n",
      "         ...,\n",
      "         [ 0.1301,  0.0388],\n",
      "         [ 0.1397,  0.0453],\n",
      "         [ 0.1524,  0.0592]]])\n",
      "Total samples: 20\n",
      "tensor([[[ 0.1571,  0.0662],\n",
      "         [ 0.1708,  0.0849],\n",
      "         [ 0.1643,  0.0582],\n",
      "         ...,\n",
      "         [-0.0810,  0.0169],\n",
      "         [-0.0938,  0.0177],\n",
      "         [-0.1058,  0.0179]]])\n",
      "tensor([[[ 0.1617,  0.0605],\n",
      "         [ 0.1731,  0.0627],\n",
      "         [ 0.1499,  0.0448],\n",
      "         ...,\n",
      "         [-0.0959, -0.0103],\n",
      "         [-0.0972, -0.0066],\n",
      "         [-0.0892,  0.0034]]])\n",
      "tensor([[[ 0.1724,  0.0693],\n",
      "         [ 0.1859,  0.0805],\n",
      "         [ 0.1782,  0.0492],\n",
      "         ...,\n",
      "         [-0.0975, -0.0005],\n",
      "         [-0.1049,  0.0132],\n",
      "         [-0.1081,  0.0152]]])\n",
      "Total samples: 20\n",
      "tensor([[[ 0.1531, -0.0365],\n",
      "         [ 0.1718, -0.0787],\n",
      "         [ 0.1697, -0.0200],\n",
      "         ...,\n",
      "         [-0.0042, -0.0730],\n",
      "         [-0.0109, -0.0655],\n",
      "         [-0.0476,  0.0152]]])\n",
      "tensor([[[ 0.1694,  0.0502],\n",
      "         [ 0.1768, -0.0142],\n",
      "         [ 0.1834,  0.0056],\n",
      "         ...,\n",
      "         [-0.0173, -0.1178],\n",
      "         [-0.0240, -0.0716],\n",
      "         [-0.0581, -0.0013]]])\n",
      "tensor([[[ 0.1466,  0.0735],\n",
      "         [ 0.1532, -0.0102],\n",
      "         [ 0.1454,  0.0367],\n",
      "         ...,\n",
      "         [-0.0107, -0.1268],\n",
      "         [-0.0135, -0.0760],\n",
      "         [-0.0613, -0.0187]]])\n",
      "Total samples: 20\n",
      "tensor([[[-0.0654,  0.0588],\n",
      "         [-0.0643,  0.0384],\n",
      "         [-0.0616,  0.0181],\n",
      "         ...,\n",
      "         [-0.0666, -0.1357],\n",
      "         [-0.0547, -0.1003],\n",
      "         [-0.0384, -0.1228]]])\n",
      "tensor([[[-0.1081,  0.0456],\n",
      "         [-0.0992,  0.0205],\n",
      "         [-0.1029,  0.0015],\n",
      "         ...,\n",
      "         [-0.0614, -0.1601],\n",
      "         [-0.0524, -0.1304],\n",
      "         [-0.0384, -0.1346]]])\n",
      "tensor([[[-0.1396,  0.0245],\n",
      "         [-0.1319,  0.0002],\n",
      "         [-0.1306, -0.0059],\n",
      "         ...,\n",
      "         [-0.0503, -0.1667],\n",
      "         [-0.0413, -0.1356],\n",
      "         [-0.0225, -0.1318]]])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb Cell 17'\u001b[0m in \u001b[0;36m<cell line: 195>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000016vscode-remote?line=199'>200</a>\u001b[0m T2mtrain1\u001b[39m=\u001b[39mstate_training_data[variableList[\u001b[39m1\u001b[39m]]\u001b[39m.\u001b[39misel(time\u001b[39m=\u001b[39m\u001b[39mslice\u001b[39m((ss\u001b[39m*\u001b[39mChuncksize),(ss\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mChuncksize))\u001b[39m.\u001b[39mcoarsen(grid_yt\u001b[39m=\u001b[39mcoarsenInd)\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mcoarsen(grid_xt\u001b[39m=\u001b[39mcoarsenInd)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000016vscode-remote?line=200'>201</a>\u001b[0m T2mtrain2\u001b[39m=\u001b[39mstate_training_data[variableList[\u001b[39m2\u001b[39m]]\u001b[39m.\u001b[39misel(time\u001b[39m=\u001b[39m\u001b[39mslice\u001b[39m((ss\u001b[39m*\u001b[39mChuncksize),(ss\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mChuncksize))\u001b[39m.\u001b[39mcoarsen(grid_yt\u001b[39m=\u001b[39mcoarsenInd)\u001b[39m.\u001b[39mmean()\u001b[39m.\u001b[39mcoarsen(grid_xt\u001b[39m=\u001b[39mcoarsenInd)\u001b[39m.\u001b[39mmean()\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000016vscode-remote?line=202'>203</a>\u001b[0m Z500train\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mswapaxes(Z500train\u001b[39m.\u001b[39;49mvalues, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000016vscode-remote?line=203'>204</a>\u001b[0m T2mtrain1\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mswapaxes(T2mtrain1\u001b[39m.\u001b[39mvalues, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000016vscode-remote?line=204'>205</a>\u001b[0m T2mtrain2\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mswapaxes(T2mtrain2\u001b[39m.\u001b[39mvalues, \u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/xarray/core/dataarray.py:642\u001b[0m, in \u001b[0;36mDataArray.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    633\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    634\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalues\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m    635\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    636\u001b[0m \u001b[39m    The array's data as a numpy.ndarray.\u001b[39;00m\n\u001b[1;32m    637\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[39m    type does not support coercion like this (e.g. cupy).\u001b[39;00m\n\u001b[1;32m    641\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 642\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvariable\u001b[39m.\u001b[39;49mvalues\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/xarray/core/variable.py:512\u001b[0m, in \u001b[0;36mVariable.values\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    510\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalues\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    511\u001b[0m     \u001b[39m\"\"\"The variable's data as a numpy.ndarray\"\"\"\u001b[39;00m\n\u001b[0;32m--> 512\u001b[0m     \u001b[39mreturn\u001b[39;00m _as_array_or_item(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/xarray/core/variable.py:252\u001b[0m, in \u001b[0;36m_as_array_or_item\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_as_array_or_item\u001b[39m(data):\n\u001b[1;32m    239\u001b[0m     \u001b[39m\"\"\"Return the given values as a numpy array, or as an individual item if\u001b[39;00m\n\u001b[1;32m    240\u001b[0m \u001b[39m    it's a 0d datetime64 or timedelta64 array.\u001b[39;00m\n\u001b[1;32m    241\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[39m    TODO: remove this (replace with np.asarray) once these issues are fixed\u001b[39;00m\n\u001b[1;32m    251\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 252\u001b[0m     data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(data)\n\u001b[1;32m    253\u001b[0m     \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    254\u001b[0m         \u001b[39mif\u001b[39;00m data\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mM\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/array/core.py:1689\u001b[0m, in \u001b[0;36mArray.__array__\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   1688\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m-> 1689\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute()\n\u001b[1;32m   1690\u001b[0m     \u001b[39mif\u001b[39;00m dtype \u001b[39mand\u001b[39;00m x\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m dtype:\n\u001b[1;32m   1691\u001b[0m         x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mastype(dtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/base.py:315\u001b[0m, in \u001b[0;36mDaskMethodsMixin.compute\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    292\u001b[0m     \u001b[39m\"\"\"Compute this dask collection\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \n\u001b[1;32m    294\u001b[0m \u001b[39m    This turns a lazy Dask collection into its in-memory equivalent.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[39m    dask.base.compute\u001b[39;00m\n\u001b[1;32m    314\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 315\u001b[0m     (result,) \u001b[39m=\u001b[39m compute(\u001b[39mself\u001b[39;49m, traverse\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    316\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/base.py:603\u001b[0m, in \u001b[0;36mcompute\u001b[0;34m(traverse, optimize_graph, scheduler, get, *args, **kwargs)\u001b[0m\n\u001b[1;32m    600\u001b[0m     keys\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_keys__())\n\u001b[1;32m    601\u001b[0m     postcomputes\u001b[39m.\u001b[39mappend(x\u001b[39m.\u001b[39m__dask_postcompute__())\n\u001b[0;32m--> 603\u001b[0m results \u001b[39m=\u001b[39m schedule(dsk, keys, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    604\u001b[0m \u001b[39mreturn\u001b[39;00m repack([f(r, \u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m r, (f, a) \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(results, postcomputes)])\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/local.py:557\u001b[0m, in \u001b[0;36mget_sync\u001b[0;34m(dsk, keys, **kwargs)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[39m\"\"\"A naive synchronous version of get_async\u001b[39;00m\n\u001b[1;32m    553\u001b[0m \n\u001b[1;32m    554\u001b[0m \u001b[39mCan be useful for debugging.\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    556\u001b[0m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m\"\u001b[39m\u001b[39mnum_workers\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)  \u001b[39m# if num_workers present, remove it\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m \u001b[39mreturn\u001b[39;00m get_async(\n\u001b[1;32m    558\u001b[0m     synchronous_executor\u001b[39m.\u001b[39;49msubmit,\n\u001b[1;32m    559\u001b[0m     synchronous_executor\u001b[39m.\u001b[39;49m_max_workers,\n\u001b[1;32m    560\u001b[0m     dsk,\n\u001b[1;32m    561\u001b[0m     keys,\n\u001b[1;32m    562\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    563\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/local.py:500\u001b[0m, in \u001b[0;36mget_async\u001b[0;34m(submit, num_workers, dsk, result, cache, get_id, rerun_exceptions_locally, pack_exception, raise_exception, callbacks, dumps, loads, chunksize, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mwhile\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mwaiting\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mready\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39mor\u001b[39;00m state[\u001b[39m\"\u001b[39m\u001b[39mrunning\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    499\u001b[0m     fire_tasks(chunksize)\n\u001b[0;32m--> 500\u001b[0m     \u001b[39mfor\u001b[39;00m key, res_info, failed \u001b[39min\u001b[39;00m queue_get(queue)\u001b[39m.\u001b[39;49mresult():\n\u001b[1;32m    501\u001b[0m         \u001b[39mif\u001b[39;00m failed:\n\u001b[1;32m    502\u001b[0m             exc, tb \u001b[39m=\u001b[39m loads(res_info)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/concurrent/futures/_base.py:437\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    436\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m--> 437\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[1;32m    439\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[1;32m    441\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/concurrent/futures/_base.py:389\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[1;32m    388\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 389\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[1;32m    390\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    391\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    392\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/local.py:542\u001b[0m, in \u001b[0;36mSynchronousExecutor.submit\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    540\u001b[0m fut \u001b[39m=\u001b[39m Future()\n\u001b[1;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 542\u001b[0m     fut\u001b[39m.\u001b[39mset_result(fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs))\n\u001b[1;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    544\u001b[0m     fut\u001b[39m.\u001b[39mset_exception(e)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/local.py:238\u001b[0m, in \u001b[0;36mbatch_execute_tasks\u001b[0;34m(it)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_execute_tasks\u001b[39m(it):\n\u001b[1;32m    235\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[39m    Batch computing of multiple tasks with `execute_task`\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[39mreturn\u001b[39;00m [execute_task(\u001b[39m*\u001b[39ma) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m it]\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/local.py:238\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mbatch_execute_tasks\u001b[39m(it):\n\u001b[1;32m    235\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[39m    Batch computing of multiple tasks with `execute_task`\u001b[39;00m\n\u001b[1;32m    237\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 238\u001b[0m     \u001b[39mreturn\u001b[39;00m [execute_task(\u001b[39m*\u001b[39;49ma) \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m it]\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/local.py:229\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    227\u001b[0m     failed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    228\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 229\u001b[0m     result \u001b[39m=\u001b[39m pack_exception(e, dumps)\n\u001b[1;32m    230\u001b[0m     failed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m key, result, failed\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/local.py:224\u001b[0m, in \u001b[0;36mexecute_task\u001b[0;34m(key, task_info, dumps, loads, get_id, pack_exception)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    223\u001b[0m     task, data \u001b[39m=\u001b[39m loads(task_info)\n\u001b[0;32m--> 224\u001b[0m     result \u001b[39m=\u001b[39m _execute_task(task, data)\n\u001b[1;32m    225\u001b[0m     \u001b[39mid\u001b[39m \u001b[39m=\u001b[39m get_id()\n\u001b[1;32m    226\u001b[0m     result \u001b[39m=\u001b[39m dumps((result, \u001b[39mid\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/core.py:119\u001b[0m, in \u001b[0;36m_execute_task\u001b[0;34m(arg, cache, dsk)\u001b[0m\n\u001b[1;32m    115\u001b[0m     func, args \u001b[39m=\u001b[39m arg[\u001b[39m0\u001b[39m], arg[\u001b[39m1\u001b[39m:]\n\u001b[1;32m    116\u001b[0m     \u001b[39m# Note: Don't assign the subtask results to a variable. numpy detects\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# temporaries by their reference count and can execute certain\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     \u001b[39m# operations in-place.\u001b[39;00m\n\u001b[0;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49m(_execute_task(a, cache) \u001b[39mfor\u001b[39;49;00m a \u001b[39min\u001b[39;49;00m args))\n\u001b[1;32m    120\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m ishashable(arg):\n\u001b[1;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m arg\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/dask/array/core.py:125\u001b[0m, in \u001b[0;36mgetter\u001b[0;34m(a, b, asarray, lock)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[39m# Below we special-case `np.matrix` to force a conversion to\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39m# `np.ndarray` and preserve original Dask behavior for `getter`,\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[39m# as for all purposes `np.matrix` is array-like and thus\u001b[39;00m\n\u001b[1;32m    123\u001b[0m     \u001b[39m# `is_arraylike` evaluates to `True` in that case.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m     \u001b[39mif\u001b[39;00m asarray \u001b[39mand\u001b[39;00m (\u001b[39mnot\u001b[39;00m is_arraylike(c) \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(c, np\u001b[39m.\u001b[39mmatrix)):\n\u001b[0;32m--> 125\u001b[0m         c \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49masarray(c)\n\u001b[1;32m    126\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    127\u001b[0m     \u001b[39mif\u001b[39;00m lock:\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/xarray/core/indexing.py:358\u001b[0m, in \u001b[0;36mImplicitToExplicitIndexingAdapter.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    357\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 358\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/xarray/core/indexing.py:522\u001b[0m, in \u001b[0;36mCopyOnWriteArray.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 522\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39;49masarray(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49marray, dtype\u001b[39m=\u001b[39;49mdtype)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/xarray/core/indexing.py:423\u001b[0m, in \u001b[0;36mLazilyIndexedArray.__array__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__array__\u001b[39m(\u001b[39mself\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    422\u001b[0m     array \u001b[39m=\u001b[39m as_indexable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39marray)\n\u001b[0;32m--> 423\u001b[0m     \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39masarray(array[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey], dtype\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/xarray/backends/zarr.py:73\u001b[0m, in \u001b[0;36mZarrArrayWrapper.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m     71\u001b[0m array \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_array()\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, indexing\u001b[39m.\u001b[39mBasicIndexer):\n\u001b[0;32m---> 73\u001b[0m     \u001b[39mreturn\u001b[39;00m array[key\u001b[39m.\u001b[39;49mtuple]\n\u001b[1;32m     74\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, indexing\u001b[39m.\u001b[39mVectorizedIndexer):\n\u001b[1;32m     75\u001b[0m     \u001b[39mreturn\u001b[39;00m array\u001b[39m.\u001b[39mvindex[\n\u001b[1;32m     76\u001b[0m         indexing\u001b[39m.\u001b[39m_arrayize_vectorized_indexer(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape)\u001b[39m.\u001b[39mtuple\n\u001b[1;32m     77\u001b[0m     ]\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/zarr/core.py:788\u001b[0m, in \u001b[0;36mArray.__getitem__\u001b[0;34m(self, selection)\u001b[0m\n\u001b[1;32m    786\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvindex[selection]\n\u001b[1;32m    787\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 788\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_basic_selection(pure_selection, fields\u001b[39m=\u001b[39;49mfields)\n\u001b[1;32m    789\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/zarr/core.py:914\u001b[0m, in \u001b[0;36mArray.get_basic_selection\u001b[0;34m(self, selection, out, fields)\u001b[0m\n\u001b[1;32m    911\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_basic_selection_zd(selection\u001b[39m=\u001b[39mselection, out\u001b[39m=\u001b[39mout,\n\u001b[1;32m    912\u001b[0m                                         fields\u001b[39m=\u001b[39mfields)\n\u001b[1;32m    913\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 914\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_basic_selection_nd(selection\u001b[39m=\u001b[39;49mselection, out\u001b[39m=\u001b[39;49mout,\n\u001b[1;32m    915\u001b[0m                                         fields\u001b[39m=\u001b[39;49mfields)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/zarr/core.py:957\u001b[0m, in \u001b[0;36mArray._get_basic_selection_nd\u001b[0;34m(self, selection, out, fields)\u001b[0m\n\u001b[1;32m    951\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_basic_selection_nd\u001b[39m(\u001b[39mself\u001b[39m, selection, out\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, fields\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    952\u001b[0m     \u001b[39m# implementation of basic selection for array with at least one dimension\u001b[39;00m\n\u001b[1;32m    953\u001b[0m \n\u001b[1;32m    954\u001b[0m     \u001b[39m# setup indexer\u001b[39;00m\n\u001b[1;32m    955\u001b[0m     indexer \u001b[39m=\u001b[39m BasicIndexer(selection, \u001b[39mself\u001b[39m)\n\u001b[0;32m--> 957\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_selection(indexer\u001b[39m=\u001b[39;49mindexer, out\u001b[39m=\u001b[39;49mout, fields\u001b[39m=\u001b[39;49mfields)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/zarr/core.py:1247\u001b[0m, in \u001b[0;36mArray._get_selection\u001b[0;34m(self, indexer, out, fields)\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchunk_store, \u001b[39m\"\u001b[39m\u001b[39mgetitems\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m \\\n\u001b[1;32m   1242\u001b[0m    \u001b[39many\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m x: x \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape)):\n\u001b[1;32m   1243\u001b[0m     \u001b[39m# sequentially get one key at a time from storage\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m     \u001b[39mfor\u001b[39;00m chunk_coords, chunk_selection, out_selection \u001b[39min\u001b[39;00m indexer:\n\u001b[1;32m   1245\u001b[0m \n\u001b[1;32m   1246\u001b[0m         \u001b[39m# load chunk selection into output array\u001b[39;00m\n\u001b[0;32m-> 1247\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_chunk_getitem(chunk_coords, chunk_selection, out, out_selection,\n\u001b[1;32m   1248\u001b[0m                             drop_axes\u001b[39m=\u001b[39;49mindexer\u001b[39m.\u001b[39;49mdrop_axes, fields\u001b[39m=\u001b[39;49mfields)\n\u001b[1;32m   1249\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1250\u001b[0m     \u001b[39m# allow storage to get multiple items at once\u001b[39;00m\n\u001b[1;32m   1251\u001b[0m     lchunk_coords, lchunk_selection, lout_selection \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mindexer)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/zarr/core.py:1939\u001b[0m, in \u001b[0;36mArray._chunk_getitem\u001b[0;34m(self, chunk_coords, chunk_selection, out, out_selection, drop_axes, fields)\u001b[0m\n\u001b[1;32m   1935\u001b[0m ckey \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_chunk_key(chunk_coords)\n\u001b[1;32m   1937\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1938\u001b[0m     \u001b[39m# obtain compressed data for chunk\u001b[39;00m\n\u001b[0;32m-> 1939\u001b[0m     cdata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_store[ckey]\n\u001b[1;32m   1941\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m   1942\u001b[0m     \u001b[39m# chunk not initialized\u001b[39;00m\n\u001b[1;32m   1943\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fill_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/zarr/storage.py:717\u001b[0m, in \u001b[0;36mKVStore.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):\n\u001b[0;32m--> 717\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mutable_mapping[key]\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/fsspec/mapping.py:137\u001b[0m, in \u001b[0;36mFSMap.__getitem__\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m    135\u001b[0m k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_key_to_str(key)\n\u001b[1;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfs\u001b[39m.\u001b[39;49mcat(k)\n\u001b[1;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmissing_exceptions:\n\u001b[1;32m    139\u001b[0m     \u001b[39mif\u001b[39;00m default \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/fsspec/asyn.py:86\u001b[0m, in \u001b[0;36msync_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     84\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     85\u001b[0m     \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m obj \u001b[39mor\u001b[39;00m args[\u001b[39m0\u001b[39m]\n\u001b[0;32m---> 86\u001b[0m     \u001b[39mreturn\u001b[39;00m sync(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloop, func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/site-packages/fsspec/asyn.py:54\u001b[0m, in \u001b[0;36msync\u001b[0;34m(loop, func, timeout, *args, **kwargs)\u001b[0m\n\u001b[1;32m     51\u001b[0m asyncio\u001b[39m.\u001b[39mrun_coroutine_threadsafe(_runner(event, coro, result, timeout), loop)\n\u001b[1;32m     52\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     \u001b[39m# this loops allows thread to get interrupted\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m     \u001b[39mif\u001b[39;00m event\u001b[39m.\u001b[39;49mwait(\u001b[39m1\u001b[39;49m):\n\u001b[1;32m     55\u001b[0m         \u001b[39mbreak\u001b[39;00m\n\u001b[1;32m     56\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/threading.py:558\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    556\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    557\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 558\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    559\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/dlwp2/lib/python3.8/threading.py:306\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    305\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 306\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39;49macquire(\u001b[39mTrue\u001b[39;49;00m, timeout)\n\u001b[1;32m    307\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    308\u001b[0m         gotit \u001b[39m=\u001b[39m waiter\u001b[39m.\u001b[39macquire(\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from dgl.nn import SAGEConv\n",
    "from dgl.nn.pytorch import NNConv\n",
    "import numpy as np\n",
    "import dask.diagnostics\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import time\n",
    "import select as sl\n",
    "import pickle\n",
    "from load_data import *\n",
    "from utilsMPGNNUnet import *\n",
    "import wandb\n",
    "from fv3net.artifacts.resolve_url import resolve_url\n",
    "from vcm import get_fs\n",
    "\n",
    "lead=6\n",
    "\n",
    "coarsenInd=1\n",
    "g = pickle.load(open(\"UpdatedGraph_Neighbour10\", 'rb'))\n",
    "residual=0\n",
    "num_step=15\n",
    "aggregat='mean'\n",
    "\n",
    "control_str='MultiSAGEUnet'#'TNSTTNST' #'TNTSTNTST'\n",
    "\n",
    "print(control_str)\n",
    "\n",
    "epochs=50\n",
    "\n",
    "variableList=['h500','h200','h850']\n",
    "TotalSamples=8500\n",
    "Chuncksize=20\n",
    "\n",
    "\n",
    "\n",
    "lr=0.001\n",
    "disablecuda ='store_true'\n",
    "batch_size=1\n",
    "drop_prob = 0\n",
    "out_feat=2\n",
    "\n",
    "savemodelpath = (\n",
    "    \"weight_layer_\"\n",
    "    + control_str\n",
    "    + \"_lead\"\n",
    "    + str(lead)\n",
    "    + \"_epochs_\"\n",
    "    + str(epochs)\n",
    "    +\"MP_Block_\"\n",
    "    +str(num_step)\n",
    "    + \"aggregat_\"\n",
    "    +aggregat\n",
    "    +\"coarsen_\"\n",
    "    +str(coarsenInd)\n",
    "    +\"residual_\"\n",
    "    +str(residual)\n",
    "    +\".pt\"\n",
    ")\n",
    "\n",
    "print(savemodelpath)\n",
    "\n",
    "BUCKET = \"vcm-ml-experiments\"\n",
    "PROJECT = \"full-model-emulation\"\n",
    "\n",
    "model_out_url = resolve_url(BUCKET, PROJECT, savemodelpath)\n",
    "data_url = \"gs://vcm-ml-scratch/ebrahimn/2022-07-02/experiment-1-y/fv3gfs_run/\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_url = \"gs://vcm-ml-scratch/ebrahimn/2022-07-02/experiment-1-y/fv3gfs_run/\" \n",
    "state_training_data = xr.open_zarr(\n",
    "    fsspec.get_mapper(os.path.join(data_url, \"atmos_dt_atmos.zarr\")), consolidated=True\n",
    ")\n",
    "# state_training_data2 = xr.open_zarr(fsspec.get_mapper(os.path.join(data_url, 'sfc_dt_atmos.zarr')))\n",
    "lat_lon_data = xr.open_zarr(\n",
    "    fsspec.get_mapper(os.path.join(data_url, \"state_after_timestep.zarr\"))\n",
    ")\n",
    "\n",
    "landSea = xr.open_zarr(\n",
    "    fsspec.get_mapper(\n",
    "        \"gs://vcm-ml-experiments/default/2022-05-09/baseline-35day-spec-sst/fv3gfs_run/state_after_timestep.zarr\"\n",
    "    )\n",
    ")\n",
    "landSea_Mask = landSea.land_sea_mask[1].load()\n",
    "landSea_Mask = landSea_Mask[:, ::coarsenInd, ::coarsenInd].values.flatten()\n",
    "\n",
    "\n",
    "lat=(lat_lon_data.latitude[1].load())\n",
    "lon=(lat_lon_data.longitude[1].load())\n",
    "lat=lat[:,::coarsenInd,::coarsenInd].values.flatten()\n",
    "lon=lon[:,::coarsenInd,::coarsenInd].values.flatten()\n",
    "# cosLat=np.expand_dims(np.cos(lat),axis=1)\n",
    "# cosLon=np.expand_dims(np.cos(lon),axis=1)\n",
    "# sinLat=np.expand_dims(np.sin(lat),axis=1)\n",
    "# sinLon=np.expand_dims(np.sin(lon),axis=1)\n",
    "cosLat=np.cos(lat)\n",
    "cosLon=np.cos(lon)\n",
    "sinLat=np.sin(lat)\n",
    "sinLon=np.sin(lon)\n",
    "for i in range(2):\n",
    "        if i==0:\n",
    "            sinLon=torch.tensor(sinLon).unsqueeze(0).repeat(1,1)\n",
    "            cosLon=torch.tensor(cosLon).unsqueeze(0).repeat(1,1)\n",
    "            sinLat=torch.tensor(sinLat).unsqueeze(0).repeat(1,1)\n",
    "            cosLat=torch.tensor(cosLat).unsqueeze(0).repeat(1,1)\n",
    "            landSea_Mask=torch.tensor(landSea_Mask).unsqueeze(0).repeat(1,1)\n",
    "        elif i==1:\n",
    "            sinLon=(sinLon).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            cosLon=(cosLon).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            sinLat=(sinLat).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            cosLat=(cosLat).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            landSea_Mask=(landSea_Mask).unsqueeze(0).repeat(batch_size,1,1)\n",
    "\n",
    "exteraVar=torch.cat((sinLon, sinLat,cosLon,cosLat,landSea_Mask), 1).to(device)\n",
    "exteraVar=np.swapaxes(exteraVar,2, 1)\n",
    "print(device)\n",
    "\n",
    "num_nodes=len(lon)\n",
    "print(f\"numebr of grids: {num_nodes}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "edg=np.asarray(g.edges())\n",
    "latInd=lat[edg[1]]\n",
    "lonInd=lon[edg[1]]\n",
    "latlon=[latInd.T,lonInd.T]\n",
    "# latlon=np.swapaxes(latlon, 1, 0)\n",
    "latlon=torch.from_numpy(np.swapaxes(latlon, 1, 0)).float()\n",
    "latlon=latlon.to(device)\n",
    "\n",
    "\n",
    "Zmean=5765.8457   #Z500mean=5765.8457, \n",
    "Zstd=90.79599   #Z500std=90.79599\n",
    "\n",
    "Tmean=10643.382          #Thickmean=10643.382\n",
    "Tstd=162.12427              #Thickstd=162.12427\n",
    "valInde=0\n",
    "\n",
    "print('loading model')\n",
    "\n",
    "class UnetGraphSAGE(nn.Module):\n",
    "    def __init__(self, g, in_feats, h_feats,out_feat,num_step,aggregat):\n",
    "        super(UnetGraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats,aggregat)\n",
    "        self.conv2 = SAGEConv(h_feats, int(h_feats/2), aggregat)\n",
    "        self.conv3 = SAGEConv(int(h_feats/2), int(h_feats/4), aggregat)\n",
    "        self.conv4 = SAGEConv(int(h_feats/4), int(h_feats/4), aggregat)\n",
    "        self.conv5 = SAGEConv(int(h_feats/2), int(h_feats/2), aggregat)\n",
    "        self.conv6 = SAGEConv(h_feats, out_feat,aggregat)\n",
    "        self.g=g\n",
    "        self.num_step=num_step\n",
    "        \n",
    "    def forward(self, in_feat,exteraVar1):\n",
    "\n",
    "        for _ in range(self.num_step):\n",
    "            h = self.conv1(self.g, in_feat)\n",
    "            h = F.relu(h)\n",
    "            h = self.conv2(self.g, h)\n",
    "            h = F.relu(h)\n",
    "            h = self.conv3(self.g, h)\n",
    "            h = F.relu(h)\n",
    "            tuple = (self.conv4(self.g, h),h)\n",
    "            h = torch.cat(tuple,dim=1)\n",
    "            h = F.relu(h)\n",
    "            tuple = (self.conv5(self.g, h),h)\n",
    "            h = torch.cat(tuple,dim=1)\n",
    "            h = F.relu(h)\n",
    "            h = self.conv6(self.g, h)\n",
    "            in_feat=torch.cat((h, torch.squeeze(exteraVar1)), 1).float()\n",
    "        return h\n",
    "        \n",
    "\n",
    "loss = nn.MSELoss()\n",
    "g = g.to(device)\n",
    "model = UnetGraphSAGE(g,7,256, 2,num_step,aggregat).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
    "model.train()\n",
    "\n",
    "all_indices=np.random.permutation(np.arange(start=0, stop=int(TotalSamples/Chuncksize)))\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    \n",
    "    for ss in all_indices:\n",
    "\n",
    "        Z500train=state_training_data[variableList[0]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "        T2mtrain1=state_training_data[variableList[1]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "        T2mtrain2=state_training_data[variableList[2]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "\n",
    "        Z500train=np.swapaxes(Z500train.values, 1, 0)\n",
    "        T2mtrain1=np.swapaxes(T2mtrain1.values, 1, 0)\n",
    "        T2mtrain2=np.swapaxes(T2mtrain2.values, 1, 0)\n",
    "\n",
    "        T2mtrain=T2mtrain1-T2mtrain2\n",
    "\n",
    "        T2mtrain=T2mtrain.reshape(np.size(T2mtrain, 0), np.size(T2mtrain, 1)*np.size(T2mtrain, 2)*np.size(T2mtrain, 3))\n",
    "        Z500train=Z500train.reshape(np.size(Z500train, 0), np.size(Z500train, 1)*np.size(Z500train, 2)*np.size(Z500train, 3))\n",
    "\n",
    "        # Zmean = np.mean(Z500train)\n",
    "        # Zstd = np.std(Z500train)\n",
    "\n",
    "        # Tmean = np.mean(T2mtrain)\n",
    "        # Tstd = np.std(T2mtrain)\n",
    "\n",
    "\n",
    "        T2mtrain=(T2mtrain-Tmean)/Tstd\n",
    "        Z500train=(Z500train-Zmean)/Zstd\n",
    "\n",
    "        \n",
    "\n",
    "        T2mtrain=np.expand_dims(T2mtrain,axis=0)\n",
    "        Z500train=np.expand_dims(Z500train,axis=0)\n",
    "\n",
    "        dataSets=np.concatenate((Z500train,T2mtrain),axis=0)\n",
    "\n",
    "        num_samples=np.size(dataSets,1)\n",
    "        print(f\"Total samples: {num_samples}\")\n",
    "\n",
    "\n",
    "        len_val = round(num_samples * 0.25)\n",
    "        len_train = round(num_samples * 0.75)\n",
    "        train = dataSets[:,: len_train]\n",
    "        val = dataSets[:,len_train+14: len_train + len_val]\n",
    "\n",
    "        x_train=train[:,0:-2*lead,:]\n",
    "        y_train=train[:,lead:-lead,:]\n",
    "        y_train2=train[:,lead::,:]\n",
    "\n",
    "        x_val=val[:,0:-lead,:]\n",
    "        y_val=val[:,lead::,:]\n",
    "\n",
    "        if residual==1:\n",
    "            y_train=y_train-x_train\n",
    "            y_val=y_val-x_val\n",
    "\n",
    "\n",
    "        x_train=np.swapaxes(x_train, 1, 0)\n",
    "        y_train=np.swapaxes(y_train, 1, 0)\n",
    "        y_train2=np.swapaxes(y_train2, 1, 0)\n",
    "\n",
    "        x_train=np.swapaxes(x_train, 2, 1)\n",
    "        y_train=np.swapaxes(y_train, 2, 1)\n",
    "        y_train2=np.swapaxes(y_train2, 2, 1)\n",
    "\n",
    "        x_train=torch.Tensor(x_train).to(device)\n",
    "        y_train=torch.Tensor(y_train).to(device)\n",
    "        y_train2=torch.Tensor(y_train2).to(device)\n",
    "\n",
    "        train_data = torch.utils.data.TensorDataset(x_train, y_train, y_train2)\n",
    "        train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "        x_val=np.swapaxes(x_val, 1, 0)\n",
    "        y_val=np.swapaxes(y_val, 1, 0)\n",
    "        x_val=np.swapaxes(x_val, 2, 1)\n",
    "        y_val=np.swapaxes(y_val, 2, 1)\n",
    "        x_val=torch.Tensor(x_val).to(device)\n",
    "        y_val=torch.Tensor(y_val).to(device)\n",
    "\n",
    "\n",
    "        val_data = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "        val_iter = torch.utils.data.DataLoader(val_data, batch_size)\n",
    "        \n",
    "\n",
    "        if valInde==0:\n",
    "            min_val_loss = np.inf\n",
    "            valInde+=1\n",
    "\n",
    "        l_sum, n = 0.0, 0\n",
    "        for x, y, y2 in train_iter:\n",
    "            print(x-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahimn/miniconda3/envs/dlwp2/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiSAGEUnet\n",
      "weight_layer_MultiSAGEUnet_lead6_epochs_50MP_Block_4aggregat_meancoarsen_1residual_0.pt\n",
      "cpu\n",
      "numebr of grids: 13824\n",
      "loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1890812/1716582478.py:136: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  edg=np.asarray(g.edges())\n",
      "/tmp/ipykernel_1890812/1716582478.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  edg=np.asarray(g.edges())\n"
     ]
    }
   ],
   "source": [
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import dgl.function as fn\n",
    "from dgl.nn import SAGEConv\n",
    "from dgl.nn.pytorch import NNConv\n",
    "import numpy as np\n",
    "import dask.diagnostics\n",
    "import fsspec\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import warnings\n",
    "import time\n",
    "import select as sl\n",
    "import pickle\n",
    "from load_data import *\n",
    "from utilsMPGNNUnet import *\n",
    "import wandb\n",
    "from fv3net.artifacts.resolve_url import resolve_url\n",
    "from vcm import get_fs\n",
    "\n",
    "lead=6\n",
    "multiStep=5\n",
    "\n",
    "coarsenInd=1\n",
    "g = pickle.load(open(\"UpdatedGraph_Neighbour10\", 'rb'))\n",
    "residual=0\n",
    "num_step=4\n",
    "aggregat='mean'\n",
    "\n",
    "control_str='MultiSAGEUnet'#'TNSTTNST' #'TNTSTNTST'\n",
    "\n",
    "print(control_str)\n",
    "\n",
    "epochs=50\n",
    "\n",
    "variableList=['h500','h200','h850']\n",
    "TotalSamples=8500\n",
    "Chuncksize=500\n",
    "\n",
    "\n",
    "\n",
    "lr=0.001\n",
    "disablecuda ='store_true'\n",
    "batch_size=1\n",
    "drop_prob = 0\n",
    "out_feat=2\n",
    "\n",
    "savemodelpath = (\n",
    "    \"weight_layer_\"\n",
    "    + control_str\n",
    "    + \"_lead\"\n",
    "    + str(lead)\n",
    "    + \"_epochs_\"\n",
    "    + str(epochs)\n",
    "    +\"MP_Block_\"\n",
    "    +str(num_step)\n",
    "    + \"aggregat_\"\n",
    "    +aggregat\n",
    "    +\"coarsen_\"\n",
    "    +str(coarsenInd)\n",
    "    +\"residual_\"\n",
    "    +str(residual)\n",
    "    +\".pt\"\n",
    ")\n",
    "\n",
    "print(savemodelpath)\n",
    "\n",
    "BUCKET = \"vcm-ml-experiments\"\n",
    "PROJECT = \"full-model-emulation\"\n",
    "\n",
    "model_out_url = resolve_url(BUCKET, PROJECT, savemodelpath)\n",
    "data_url = \"gs://vcm-ml-scratch/ebrahimn/2022-07-02/experiment-1-y/fv3gfs_run/\"\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_url = \"gs://vcm-ml-scratch/ebrahimn/2022-07-02/experiment-1-y/fv3gfs_run/\" \n",
    "state_training_data = xr.open_zarr(\n",
    "    fsspec.get_mapper(os.path.join(data_url, \"atmos_dt_atmos.zarr\")), consolidated=True\n",
    ")\n",
    "# state_training_data2 = xr.open_zarr(fsspec.get_mapper(os.path.join(data_url, 'sfc_dt_atmos.zarr')))\n",
    "lat_lon_data = xr.open_zarr(\n",
    "    fsspec.get_mapper(os.path.join(data_url, \"state_after_timestep.zarr\"))\n",
    ")\n",
    "\n",
    "landSea = xr.open_zarr(\n",
    "    fsspec.get_mapper(\n",
    "        \"gs://vcm-ml-experiments/default/2022-05-09/baseline-35day-spec-sst/fv3gfs_run/state_after_timestep.zarr\"\n",
    "    )\n",
    ")\n",
    "landSea_Mask = landSea.land_sea_mask[1].load()\n",
    "landSea_Mask = landSea_Mask[:, ::coarsenInd, ::coarsenInd].values.flatten()\n",
    "\n",
    "\n",
    "lat=(lat_lon_data.latitude[1].load())\n",
    "lon=(lat_lon_data.longitude[1].load())\n",
    "lat=lat[:,::coarsenInd,::coarsenInd].values.flatten()\n",
    "lon=lon[:,::coarsenInd,::coarsenInd].values.flatten()\n",
    "# cosLat=np.expand_dims(np.cos(lat),axis=1)\n",
    "# cosLon=np.expand_dims(np.cos(lon),axis=1)\n",
    "# sinLat=np.expand_dims(np.sin(lat),axis=1)\n",
    "# sinLon=np.expand_dims(np.sin(lon),axis=1)\n",
    "cosLat=np.cos(lat)\n",
    "cosLon=np.cos(lon)\n",
    "sinLat=np.sin(lat)\n",
    "sinLon=np.sin(lon)\n",
    "for i in range(2):\n",
    "        if i==0:\n",
    "            sinLon=torch.tensor(sinLon).unsqueeze(0).repeat(1,1)\n",
    "            cosLon=torch.tensor(cosLon).unsqueeze(0).repeat(1,1)\n",
    "            sinLat=torch.tensor(sinLat).unsqueeze(0).repeat(1,1)\n",
    "            cosLat=torch.tensor(cosLat).unsqueeze(0).repeat(1,1)\n",
    "            landSea_Mask=torch.tensor(landSea_Mask).unsqueeze(0).repeat(1,1)\n",
    "        elif i==1:\n",
    "            sinLon=(sinLon).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            cosLon=(cosLon).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            sinLat=(sinLat).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            cosLat=(cosLat).unsqueeze(0).repeat(batch_size,1,1)\n",
    "            landSea_Mask=(landSea_Mask).unsqueeze(0).repeat(batch_size,1,1)\n",
    "\n",
    "exteraVar=torch.cat((sinLon, sinLat,cosLon,cosLat,landSea_Mask), 1).to(device)\n",
    "exteraVar=np.swapaxes(exteraVar,2, 1)\n",
    "print(device)\n",
    "\n",
    "num_nodes=len(lon)\n",
    "print(f\"numebr of grids: {num_nodes}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "edg=np.asarray(g.edges())\n",
    "latInd=lat[edg[1]]\n",
    "lonInd=lon[edg[1]]\n",
    "latlon=[latInd.T,lonInd.T]\n",
    "# latlon=np.swapaxes(latlon, 1, 0)\n",
    "latlon=torch.from_numpy(np.swapaxes(latlon, 1, 0)).float()\n",
    "latlon=latlon.to(device)\n",
    "\n",
    "\n",
    "Zmean=5765.8457   #Z500mean=5765.8457, \n",
    "Zstd=90.79599   #Z500std=90.79599\n",
    "\n",
    "Tmean=10643.382          #Thickmean=10643.382\n",
    "Tstd=162.12427              #Thickstd=162.12427\n",
    "valInde=0\n",
    "\n",
    "print('loading model')\n",
    "\n",
    "class UnetGraphSAGE(nn.Module):\n",
    "    def __init__(self, g, in_feats, h_feats,out_feat,num_step,aggregat):\n",
    "        super(UnetGraphSAGE, self).__init__()\n",
    "        self.conv1 = SAGEConv(in_feats, h_feats,aggregat)\n",
    "        self.conv2 = SAGEConv(h_feats, int(h_feats/2), aggregat)\n",
    "        self.conv3 = SAGEConv(int(h_feats/2), int(h_feats/4), aggregat)\n",
    "        self.conv4 = SAGEConv(int(h_feats/4), int(h_feats/4), aggregat)\n",
    "        self.conv5 = SAGEConv(int(h_feats/2), int(h_feats/2), aggregat)\n",
    "        self.conv6 = SAGEConv(h_feats, out_feat,aggregat)\n",
    "        self.g=g\n",
    "        self.num_step=num_step\n",
    "        \n",
    "    def forward(self, in_feat,exteraVar1):\n",
    "\n",
    "        for _ in range(self.num_step):\n",
    "            h = self.conv1(self.g, in_feat)\n",
    "            h = F.relu(h)\n",
    "            h = self.conv2(self.g, h)\n",
    "            h = F.relu(h)\n",
    "            h = self.conv3(self.g, h)\n",
    "            h = F.relu(h)\n",
    "            tuple = (self.conv4(self.g, h),h)\n",
    "            h = torch.cat(tuple,dim=1)\n",
    "            h = F.relu(h)\n",
    "            tuple = (self.conv5(self.g, h),h)\n",
    "            h = torch.cat(tuple,dim=1)\n",
    "            h = F.relu(h)\n",
    "            h = self.conv6(self.g, h)\n",
    "            in_feat=torch.cat((h, torch.squeeze(exteraVar1)), 1).float()\n",
    "        return h\n",
    "        \n",
    "\n",
    "loss = nn.MSELoss()\n",
    "g = g.to(device)\n",
    "model = UnetGraphSAGE(g,7,256, 2,num_step,aggregat).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.7)\n",
    "model.train()\n",
    "\n",
    "all_indices=np.random.permutation(np.arange(start=0, stop=int(TotalSamples/Chuncksize)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ebrahimn/miniconda3/envs/dlwp2/lib/python3.8/site-packages/numpy/core/_methods.py:179: RuntimeWarning: invalid value encountered in reduce\n",
      "  ret = umr_sum(arr, axis, dtype, out, keepdims, where=where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 500\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (2,339,13824) into shape (2,345,13824)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000004vscode-remote?line=45'>46</a>\u001b[0m y_train\u001b[39m=\u001b[39mx_t\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mzeros([np\u001b[39m.\u001b[39msize(train,\u001b[39m0\u001b[39m),np\u001b[39m.\u001b[39msize(train,\u001b[39m1\u001b[39m)\u001b[39m-\u001b[39mmultiStep\u001b[39m*\u001b[39mlead,np\u001b[39m.\u001b[39msize(train,\u001b[39m2\u001b[39m),multiStep])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000004vscode-remote?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m sm \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,multiStep\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000004vscode-remote?line=47'>48</a>\u001b[0m     y_train[:,:,:,sm]\u001b[39m=\u001b[39mtrain[:,(sm\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mlead:\u001b[39m-\u001b[39m(multiStep\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m*\u001b[39mlead,:]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000004vscode-remote?line=48'>49</a>\u001b[0m \u001b[39mprint\u001b[39m(y_train[:,:,:,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bvm/home/ebrahimn/fv3net/projects/full_model_emulation/test/checkCode.ipynb#ch0000004vscode-remote?line=49'>50</a>\u001b[0m x_val\u001b[39m=\u001b[39mval[:,\u001b[39m0\u001b[39m:\u001b[39m-\u001b[39mlead,:]\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (2,339,13824) into shape (2,345,13824)"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(1,2):\n",
    "    \n",
    "        ss=0\n",
    "\n",
    "        Z500train=state_training_data[variableList[0]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "        T2mtrain1=state_training_data[variableList[1]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "        T2mtrain2=state_training_data[variableList[2]].isel(time=slice((ss*Chuncksize),(ss+1)*Chuncksize)).coarsen(grid_yt=coarsenInd).mean().coarsen(grid_xt=coarsenInd).mean()\n",
    "\n",
    "        Z500train=np.swapaxes(Z500train.values, 1, 0)\n",
    "        T2mtrain1=np.swapaxes(T2mtrain1.values, 1, 0)\n",
    "        T2mtrain2=np.swapaxes(T2mtrain2.values, 1, 0)\n",
    "\n",
    "        T2mtrain=T2mtrain1-T2mtrain2\n",
    "\n",
    "        T2mtrain=T2mtrain.reshape(np.size(T2mtrain, 0), np.size(T2mtrain, 1)*np.size(T2mtrain, 2)*np.size(T2mtrain, 3))\n",
    "        Z500train=Z500train.reshape(np.size(Z500train, 0), np.size(Z500train, 1)*np.size(Z500train, 2)*np.size(Z500train, 3))\n",
    "\n",
    "        # Zmean = np.mean(Z500train)\n",
    "        # Zstd = np.std(Z500train)\n",
    "\n",
    "        # Tmean = np.mean(T2mtrain)\n",
    "        # Tstd = np.std(T2mtrain)\n",
    "\n",
    "\n",
    "        T2mtrain=(T2mtrain-Tmean)/Tstd\n",
    "        Z500train=(Z500train-Zmean)/Zstd\n",
    "\n",
    "        \n",
    "\n",
    "        T2mtrain=np.expand_dims(T2mtrain,axis=0)\n",
    "        Z500train=np.expand_dims(Z500train,axis=0)\n",
    "\n",
    "        dataSets=np.concatenate((Z500train,T2mtrain),axis=0)\n",
    "\n",
    "        num_samples=np.size(dataSets,1)\n",
    "        print(f\"Total samples: {num_samples}\")\n",
    "\n",
    "\n",
    "        len_val = round(num_samples * 0.25)\n",
    "        len_train = round(num_samples * 0.75)\n",
    "        train = dataSets[:,: len_train]\n",
    "        val = dataSets[:,len_train+14: len_train + len_val]\n",
    "\n",
    "        x_train=train[:,0:-multiStep*lead,:]\n",
    "\n",
    "        y_train=x_t=np.zeros([np.size(train,0),np.size(train,1)-multiStep*lead,np.size(train,2),multiStep])\n",
    "        for sm in range(1,multiStep+1):\n",
    "            y_train[:,:,:,sm]=train[:,(sm+1)*lead:-(multiStep-1)*lead,:]\n",
    "        print(y_train[:,:,:,-1])\n",
    "        x_val=val[:,0:-lead,:]\n",
    "        y_val=val[:,lead::,:]\n",
    "\n",
    "\n",
    "\n",
    "        x_train=np.swapaxes(x_train, 1, 0)\n",
    "        y_train=np.swapaxes(y_train, 1, 0)\n",
    "\n",
    "        x_train=np.swapaxes(x_train, 2, 1)\n",
    "        y_train=np.swapaxes(y_train, 2, 1)\n",
    "\n",
    "        x_train=torch.Tensor(x_train)\n",
    "        y_train=torch.Tensor(y_train)\n",
    "        y_train2=torch.Tensor(y_train2)\n",
    "\n",
    "        \n",
    "        train_data = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "        train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for sm in range(1,multiStep+1):\n",
    "    print(multiStep-sm)\n",
    "    if sm==multiStep:\n",
    "        y_train[:,:,:,sm-1]=train[:,(sm)*lead::,:]\n",
    "    else:\n",
    "        y_train[:,:,:,sm-1]=train[:,(sm)*lead:-(multiStep-sm)*lead,:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=np.swapaxes(x_train, 1, 0)\n",
    "y_train=np.swapaxes(y_train, 1, 0)\n",
    "\n",
    "x_train=np.swapaxes(x_train, 2, 1)\n",
    "y_train=np.swapaxes(y_train, 2, 1)\n",
    "\n",
    "x_train=torch.Tensor(x_train)\n",
    "y_train=torch.Tensor(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([345, 13824, 2])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.TensorDataset(x_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n",
      "x:torch.Size([1, 13824, 2])\n",
      "y:torch.Size([1, 13824, 2, 5])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_iter:\n",
    "    print(f\"x:{np.shape(x)}\")\n",
    "    print(f\"y:{np.shape(y)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0677, -0.2603, -0.2732,  ...,  0.4969,  0.4378,  0.3226]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:,0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('fv3net': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f82d70e649e5a240c4428e2ff07edff95e6b0e7e2e406672ec728699ce4b39c5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
